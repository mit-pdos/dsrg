<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[DSRG]]></title>
  <link href="http://pdos.csail.mit.edu/dsrg/atom.xml" rel="self"/>
  <link href="http://pdos.csail.mit.edu/dsrg/"/>
  <updated>2013-08-08T12:47:14-04:00</updated>
  <id>http://pdos.csail.mit.edu/dsrg/</id>
  <author>
    <name><![CDATA[DSRG]]></name>
    <email><![CDATA[narula@mit.edu]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MDCC]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/07/25/mdcc/"/>
    <updated>2013-07-25T12:00:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/07/25/mdcc</id>
    <content type="html"><![CDATA[<h2>Overview</h2>

<h3>Why did we read about MDCC?</h3>

<p>There is a write performance trade off when consistently replicating
across multiple data centers due to the high latency when sending
messages between data centers (often > 100ms).  Existing protocols
use forms of two phase commit which incur 3 blocking round trips
between data centers.</p>

<p><a href="https://amplab.cs.berkeley.edu/wp-content/uploads/2013/03/mdcc-eurosys13.pdf">MDCC</a>
introduces a new cross data center transaction protocol for
transactions over key-value stores (no range scans) that uses many
forms of Paxos to achieve one synchronous cross-data center message
in the common case.</p>

<p>A similar system, Megastore, uses per-shard Paxos groups to replicate
between datacenters.  Unfortunately, heavy conflicts in Megastore
reduce performance to around four transactions per second.</p>

<h3>MDCC Goals</h3>

<p>MDCC aims to have the following properties:</p>

<p><strong>Read-commited isolation</strong>: Transactions never read records
that have not been fully committed.  The paper notes that this is
the form that most commercial and open source databases provide,
so ensuring this level of consistency is sufficient.</p>

<p><strong>No lost updates</strong>: This is when transaction T1 reads a record X,
another transaction T2 commits an update to X, then T1 commits a
write to X which overwrites T2&rsquo;s commit without having ever read
it.</p>

<p><strong>Non-stale reads</strong>: Be able to ensure that reads are always of the
most recent value.</p>

<p><strong>Low latency</strong>: Minimize number of <em>synchronous</em> roundtrips &mdash; in
this case to one.</p>

<p><strong>Costs scale to true transaction overlap</strong>: There should not be
large fixed costs incurred for each transaction commit.  Rather,
the costs should be proportional to the amount of actual contention
between transactions.</p>

<p><strong>Integrity Constraints</strong>: Support some forms of integrity Constraints.
The paper discusses FIELD >/=/&lt; VALUE type constraints.</p>

<h3>Key Ideas</h3>

<ul>
<li>Paxos per record

<ul>
<li>Awesome &mdash; could shuffle record ownership depending on access patterns!</li>
</ul>
</li>
<li>Optimistic concurrency control (keep write sets)</li>
<li>During Paxos, rather than propose the new value, propose an <em>option</em> to
update the new value.  If the option succeeds, then a later write can safely
set the value asynchronously.</li>
<li>Move more logic into quorum nodes than in regular Paxos

<ul>
<li>Decide write-write conflicts</li>
<li>Enforce integrity constraints</li>
</ul>
</li>
<li>Reduce roundtrips via

<ul>
<li>multi-Paxos: select master every N commits</li>
<li>Fast Paxos: clients directly talk to quorum nodes</li>
</ul>
</li>
<li>Reduce conflicts

<ul>
<li>Generalized Paxos: commutative operations</li>
<li>&ldquo;Demarcation protocol&rdquo; conservatively figures out how many conservativy ops are allowed</li>
</ul>
</li>
<li>Failure

<ul>
<li>Send client state to every node on commit (every record&rsquo;s quorom)

<ul>
<li>write set, xact id</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3>Questions</h3>

<ul>
<li>How does a per-record protocol affect/enable fine-grained tuple placement?</li>
<li>What to do to make repeatable reads work?</li>
<li>How does Fast Paxos work?</li>
<li>How powerful/efficient is the demarcation protocol?</li>
<li>How do run range-queries or queries that need indexes?</li>
</ul>


<h2>How MDCC is built up.</h2>

<p>The MDCC paper flows by identifying issues with the best solution
proposed so far, and introducing a new technique to resolve the
issues.  This post will flow in a similar manner.</p>

<h3>Per-record Paxos</h3>

<p>Megastore is slow because it runs Paxos at a per shard granularity,
causing false conflicts.  What if we ran Paxos per record?</p>

<p>Paxos on a record runs as follows:</p>

<ul>
<li>up to 1 roundtrip for client to tell master &ldquo;I want to commit my transaction&rdquo;</li>
<li>1 roundtrip to elect master for record</li>
<li>1 roundtrip to set the value (learn the value)</li>
<li>1 roundtrip to make value visible (can be async)</li>
</ul>


<h3>Multi-record Transactions</h3>

<p>The key idea is to add logic into the acceptor nodes to perform
write-write detection.</p>

<p>If I want to commit records A and B, I run Paxos for each record.<br/>
When running Paxos, instead of setting the values in the second round,
send an <em>option</em> containing the record&rsquo;s read and write versions.
If there&rsquo;s no write conflict, the node returns OK, otherwise NO</p>

<p>If I hear OK for every record, I tell each record&rsquo;s quorums to write
&ldquo;commit&rdquo; into their option log.  Otherwise I write &ldquo;abort&rdquo;.  <strong>This
is different than SQL transaction aborts!</strong>.</p>

<p>Also, quorum nodes are only allowed to accept options if all previous
options have been decided (written &ldquo;commit&rdquo; or &ldquo;abort&rdquo;).  Otherwise
write-write conflicts can&rsquo;t be correctly detected.</p>

<h3>Supporting Failures</h3>

<p>The following sequence of action could block all nodes participating
in a transaction.</p>

<ul>
<li>I send options to acceptors for records A and B.</li>
<li>They accept and respond to me.  The options are undecided, so they can&rsquo;t accept
anything else until I write a &ldquo;commit&rdquo; or &ldquo;abort&rdquo; in the options.</li>
<li>I go on vacation before writing anything.</li>
</ul>


<p>I was the only one that knew what other records were updated in my
transaction!  So the participating nodes can&rsquo;t make anymore progress
until I&rsquo;m back from vacation.</p>

<p>The solution is to send my transaction state (transaction id and
write set) along with the options.  That way if I disappear, each
record&rsquo;s quorum can make a clone of me using the transaction state.</p>

<p><strong>At this point, multi-record transactions using per-record Paxos
&ldquo;works&rdquo;.  Everything after this point is to make Paxos consensus run
faster</strong></p>

<h3>Multi-Paxos</h3>

<p>We stil require 3 roundtrips to commit (send to master, elect master,
pick option).  We can use Paxos to elect a master for a block of
rounds which amortizes the cost of electing a master to 0 (assuming
the masters are stable).</p>

<p>The communication is now</p>

<ul>
<li>up to 1 roundtrip for client to tell master &ldquo;I want to commit my transaction&rdquo;</li>
<li>0 roundtrips (amortized) to elect master for record</li>
<li>1 roundtrip to send option to acceptors</li>
<li>1 asynchronous roundtrip to write &ldquo;commit&rdquo;/&ldquo;abort&rdquo; on options</li>
<li>1 asynchronous roundtrip (can be bundled with other messages) to
make commit visible</li>
</ul>


<h3>Fast-Paxos</h3>

<p>We still need two synchronous roundtrips &mdash; one to contact the master
if it is in another datacenter, and one to send the options.  Can we
avoid the former message?</p>

<p>Yes! Fast Paxos is lets clients bypass the master to directly send
options to the quorum nodes.  Recall that the master&rsquo;s job is to
serialize the log entries.  If we want to de away with the master,
Fast Paxos needs a larger quorum size (a fast quorum).  If a fast
quorum responds OK, then the client can consider the option committed.
Otherwise it falls back to normal Paxos with some details.</p>

<p>None of us had read the fast-Paxos paper so we were ill-equipped
to work through why this section of the paper works. Read below for
how we <em>think</em> Fast Paxos works based on first principles.  Read
the fast-Paxos paper to actually understand how it works.</p>

<h3>Generalized-Paxos</h3>

<p>If my operations are all commutative, I want to avoid the cost of
serialization.</p>

<p>Normal Paxos only allows 1 value update per round.  Generalized Paxos
allows M as long as those values are commutative.  For example, if in
the same round David wants to increment record A by 1 and Jane wants
to increment record A by 1, then the value of A is 2 regardless of the
log order on any of the quorum nodes.</p>

<p>What this means is the acceptors aren&rsquo;t simply writing values
anymore, they also understand &ldquo;delta operations&rdquo;, how they commute,
and how to apply them.</p>

<h3>Integrity constrains</h3>

<p>If I want to support integrity constraints (e.g., A >= 0) the
acceptors need to understand and enforce them.</p>

<p>The paper restricts the domains of the delta operations (e.g., can
only add/subtract 1 or 2) and conservatively computes the maximum
number of operations that are allowed before it is at all possible
(on a single node and any combinations of transactions in a quorum)
that the constraint is violated.</p>

<p>This is useful because many transactions are simply updating counters.</p>

<h3>That&rsquo;s a lot of steps, are there any more tricks?</h3>

<p>We usually want reads to go fast, but if I&rsquo;m reading record A and
the A&rsquo;s replica in my datacenter was not in the quorum, then it
would give me stale data.  One way to get around this is to make
sure the replica in my data center is <em>always</em> in the quorum.  This
works if my data center is the only one reading this record.  For
example, if each data center is collecting the local weather data
and replicating the data, it can be pretty sure that only people
living around the data center will want the local data.</p>

<p>Fast-Paxos is really really bad (in terms of cross datacenter
roundtrips) if a collision happens.  For those cases it makes more
sense to stick with a master and use Multi-Paxos.  Figuring out
when to use each is important.  The paper provides a heuristic that
sticks with multi-Paxos and periodically tries to &ldquo;upgrade&rdquo; to
fast-Paxos.</p>

<p>That&rsquo;s it!  MDCC collects a number of Paxos variants and puts them
together into a nice cross data-center commit protocol.</p>

<h2>A Note on Fast Paxos</h2>

<p>The following was derived from first principles.</p>

<p>The fast-Paxos protocol was described (briefly) in the paper as</p>

<blockquote><p>Any app-server can propose an option directly to the storage
(quorum) nodes, which in turn
promise only to accept the first proposed option.  Simple majority
quorums, however, are
no longer sufficient to leran a value and ensure safeness&hellip;
If a proposer receives an acknowledgment from a fast quorum, the
value is safe and
guaranteed to be committed.</p></blockquote>

<p>A classical Paxos quorum requires the property that for any two
quorums, <code>Qc1</code> and <code>Qc2</code>,</p>

<p>  <code>Qc1 INTERSECT Qc2</code> != Empty set</p>

<p>For Fast Paxos it has the requirement that for a fast quorums,
<code>Qf1</code> and <code>Qf2</code> and regular quorum <Qc>,</p>

<p>  <code>Qf1 INTERSECT Qf2 INTERSECT Qc</code> != empty set</p>

<p>One of our questions was why a simple majority, <code>Qc</code>, is
insufficient and we need a <code>Qf > Qc</code>. The following is an
example of when it fails.</p>

<p>Safety, or correctness, in Paxos means that &ldquo;At most one value can be
learned&rdquo;.  One example is that even with <em>f</em> failures, a committed log
position stays committed.</p>

<p>Lets say <code>Qf = Qc = majority</code>.  Consider the following logical
proposals and their quorums where each column is the state of an
acceptor (1-5), and each row is the state at one Paxos round.  C1, C2,
C3 are clients (proposers).  C1 commits A and manages to communicate
with acceptors 1-3, C2 commits B and so on.</p>

<pre><code>    1 2 3 4 5
C1  A A A
C2      B B B
C3  C C   C
</code></pre>

<p>Say C1 and C2 happen concurrently, and C3 happens later (as a normal
Paxos round, or Fast Paxos).  Then the actual acceptances and what
round each node thinks its in may be</p>

<pre><code> 1 2 3 4 5 Round (as understood by local node)
 A A A B B 1
     B     2
</code></pre>

<p>Now if C3 tries to set C, its quorum could be 1,2,4 which don&rsquo;t
know that A won in round 1 and B &ldquo;won&rdquo; in round 2, because they
have only seen values for round 1, so the nodes happily accept C
in (what they think is) round 2.  This effectively overwrites B,
violating safety.</p>

<pre><code> 1 2 3 4 5 Round (as understood by local node)
 A A A B B 1
 C C B C   2
</code></pre>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CoralCDN]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/07/18/coralcdn/"/>
    <updated>2013-07-18T15:00:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/07/18/coralcdn</id>
    <content type="html"><![CDATA[<h2>Why did we read this paper?</h2>

<p><a href="http://www.coralcdn.org/docs/coral-nsdi04.pdf">CoralCDN</a>
is a system that allows small websites or those with limited resources a method
for remaining available in the face of flash-crowds. The system is interesting
for a number of reasons: it is a live running system that the public can use,
it&rsquo;s peer to peer, self organizing, and also has a <a href="http://www.coralcdn.org/docs/coral-nsdi10.pdf">follow-up paper</a>
that analyzes the system with five years of hindsight.</p>

<h2>What is CoralCDN</h2>

<p>CoralCDN is a content distribution network (CDN) built on top of the Coral
indexing service.  The system can be described in its component parts: Coral
DNS services, Coral HTTP proxy, and the Coral indexing service.</p>

<h2>DNS Functions</h2>

<p>Many of CoralCDN&rsquo;s processes attempt to preserve locality in order to keep
latency low. This starts with the DNS lookup by the end user&rsquo;s browser.
CoralCDN assumes that the resolver for the user&rsquo;s browser is close to the
end user on the network. It then picks a Coral HTTP proxy close to the source
address of the DNS request.</p>

<p>Specifically, CoralCDN keeps track of the latencies between the proxies and the
user and creates levels.  The Coral DNS server can get a round trip time to the
end user and only provide listings to proxies that fall within a given level.</p>

<p>The DNS servers are also responsible for ensuring that the HTTP proxies that
are returned to the end user are available, since a bad HTTP proxy will fail
the user&rsquo;s request. The DNS servers can do this by synchronously checking a
proxy&rsquo;s status with an RPC prior to releasing the DNS response.</p>

<h2>The HTTP Proxy</h2>

<p>The driving goal of the system is to shield origin servers from request
stampedes. The Coral proxies therefore attempt to find the content within
CoralCDN before going to the origin server. Additionally, when a proxy begins
retrieval from the origin server, it inserts a short lived record into the
Coral index so any additional servers that want the object will not
simultaneously contact the origin.</p>

<h2>The Indexing Service</h2>

<p>One of the authors&#8217; stated contributions of this work is the indexing system
including a <em>distributed sloppy hash table</em> (DSHT).  Each node is given a 160 bit ID
and holds key/value pairs with keys that are &lsquo;close&rsquo; to the node&rsquo;s ID.
Additionally, each node maintains a routing table to other nodes. On a put or
get, the request can be routed to successively closer nodes to toward the
closest node.</p>

<p>The service is considered <em>sloppy</em> because a key/value is not always stored at
the closest node. The put process is completed in two phases. The first is a
collection of hops to get to the closest node and forms a stack from the list
of nodes. This phase ends when the storing node reaches the closest node or
when it reaches one that is <em>full</em> and <em>loaded</em>. A full node is one that
already stores <em>l</em> values for that key. A loaded node is one that is receiving
more than <em>b</em> requests per minute.  In the second phase, the storing node
attempts to place the value at the node on top of the stack. If that store
fails for some reason, the storer pops the stack and tries again.</p>

<p>Nodes are arranged in clusters based on their average pairwise latency. If that
latency is below a certain threshold, the cluster is said to be part of a
specific level. A node can be part of multiple clusters but uses
the same ID in each. This clustering allows a sequential search for a target
value, beginning with low latency &lsquo;close&rsquo; nodes and only proceeding to slower
levels if needed.</p>

<h2>Comments</h2>

<p>The idea of levels was interesting mostly because the latency thresholds allow
the clusters to be self-organizing. Clearly, locality is important to a CDN.
It does seem however, that simple static organization of nodes into clusters
would be sufficient to allow latency locality.</p>

<p>Though we didn&rsquo;t technically read the follow-up paper, there is an interesting
section on lessons learned. In the section on usage, the authors found
that a small number of popular URLs account for a large percentage of the
requests but that these requests take up relatively little room on the cache.
This means that more than 70% of requests are served from local cache, with only
7.1% retrieved from other Coral proxies.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Zookeeper]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/07/11/zookeeper/"/>
    <updated>2013-07-11T15:00:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/07/11/zookeeper</id>
    <content type="html"><![CDATA[<h2>Why did we read this paper?</h2>

<p><a href="http://static.usenix.org/event/usenix10/tech/full_papers/Hunt.pdf">Zookeeper</a>
is a practical system with replicated storage used by Yahoo!.
We are interested in understanding what replication protocol Zookeeper uses, why they
needed a <em>new</em> replication protocol, what applications/services people build upon it,
and what features are required to make such a replicated storage system
practical.</p>

<p>Also, Zookeeper has pretty good performance for a replication
protocol. It is interesting to see how it works.</p>

<h2>What is Zookeeper</h2>

<p>The Zookeeper design consists of three components &ndash; replicated storage,
relaxed consistent caching at clients, and detection of client failures.</p>

<h2>Replication</h2>

<p>Zookeeper provides a key/value data model, where keys are named like the paths of
a file system and values are arbitrary blobs. They call each key/value pair a
<em>Znode</em>. Each Znode must be accessed with a full path so that Zookeeper doesn&rsquo;t
have to implement open/close. Each value has a version number and an internal
sequence number, which they use a lot when building applications/services presented
in the paper. Each Znode can have its own value, and a collection of children.</p>

<p>Data can be accessed with get/set/create/delete methods. Zookeeper supports
conditional versions of these operations too. For example, a client can say
set the value of &ldquo;/ds-reading/schedule&rdquo; to &ldquo;3pm,thursday&rdquo; only if the Znode&rsquo;s
current version is 100. This feature is used widely in the the presented
applications/services.</p>

<p>Znodes are replicated via what they called Zab, an atomic broadcast protocol.
Zab is their own replication protocol. It seems quite similar to viewstamped
replication (VR). The only difference I can tell is that Zab is special case
of viewstamped based replication. Zab requires clients to send requests
in order (thus they use TCP), but VR does not. Zab also requires the requests
to be idempotent, so that a new leader can re-propose the most recent request
without detecting duplicated requests.</p>

<p>Despite Zab&rsquo;s restriction for replication of only idempotent operations, Zookeeper does
support non-idempotent operations. The trick is that for each potentially
non-idempotent request, the leader converts it to idempotent requests by
executing it locally.</p>

<h2>Relaxed consisent caching</h2>

<p>Another component of Zookeeper is relaxed consistent caching between clients
and Zookeeper. When clients get data from Zookeeper, clients can cache it, and
optionally register at Zookeeper to receive notifications if the accessed Znode
changes.  Zookeeper will send notifications to caching clients ASYNCHRONOUSLY
once the data is changed. The benefit of this is that updates don&rsquo;t have to wait for
invalidations to complete, thus they don&rsquo;t suffer from the impact of client failure;
the downside is that now the client&rsquo;s cache is not consistent with Zookeeper.</p>

<p>The notification doesn&rsquo;t contain the actual update, and each registration is
triggered only once (the server deletes it once the notification is delivered).</p>

<h2>Detection of client failures</h2>

<p>Zookeeper supports a special Znode type called &ldquo;ephemeral Znode&rdquo; to detect the
failure of clients.  If a session terminates, all ephemeral ZNodes created
within that session are deleted. Services/applications can use it to detect
failure. For example, Katta uses it to detect master failure.</p>

<h2>Other design choices</h2>

<p>Zookeeper allows applications and services to choose their own level of consistency.  Zookeeper
linearizes all writes. Reads are served from a local Zookeeper server, but a
client can linearize reads using the sync() API.</p>

<h2>Comments</h2>

<p>In all, we feel that ZooKeeper is cool. It provides building blocks that people
can use to construct their own services with different consistency/performance
requirements. It also simplifies the building of other services, as demonstrated
in the paper.</p>

<p>One thing we didn&rsquo;t understand is why the paper makes the claim that
ZooKeeper is not intended for general storage.  It seems like that
would work.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Thialfi]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/07/03/thialfi/"/>
    <updated>2013-07-03T16:00:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/07/03/thialfi</id>
    <content type="html"><![CDATA[<h2>Why did we read this paper?</h2>

<p>Several of the papers we&rsquo;ve read recently have focused on
sophisticated, generic fault tolerance abstractions based on complex
protocols.  <a href="http://www.cs.columbia.edu/~lierranli/coms6998-11Fall2012/papers/thia_sosp2011.pdf">Thialfi</a>
 offers a contrast: its approach to fault tolerance
is intentionally simple, while at the same time being resilient to
arbitrary (halting) failure, including entire data centers.  Thialfi&rsquo;s
approach to fault tolerance permeates the design of its abstraction,
unlike Raft and VR, which provide general-purpose state machine
replication.</p>

<p>Thialfi is also a real, massively deployed system, but is simple
enough to explain in more depth within the constraints of a conference
paper than most production systems.</p>

<h2>What is Thialfi?</h2>

<p>The paper calls Thialfi a &ldquo;notification service&rdquo;, but really it&rsquo;s an
object update signaling service.  When an application server updates
an object, it notifies Thialfi, and Thialfi notifies end-user clients
on the Internet that have registered for the object.  Critically,
these &ldquo;notifications&rdquo; contain no information about how the object
changed&mdash;-the client has to query the application server to get its
updated state&mdash;-which means Thialfi is free to combine notifications
and to generate spurious notifications, as long as clients interested
in an object eventually get at least one notification that the object
has changed if the client&rsquo;s last seen version is not the object&rsquo;s
current version.</p>

<p>Under normal operation, Thialfi delivers timely update notifications
to connected clients, without any polling (clients must send periodic
heartbeats to maintain the connection, but these are infrequent,
small, and efficient to process).  When a client goes offline and
later returns, Thialfi remembers its previous object registrations and
sends only notifications for objects that changed while the client was
offline.  Similarly, the client only needs to resend its registrations
if they have changed since it was last online or if it was offline for
over two weeks (after which Thialfi garbage collects its state).</p>

<h2>Designing for fault tolerance</h2>

<p>There are a few core ideas in Thialfi&rsquo;s design that help it achieve
fault tolerance.  These are not necessarily unique to Thialfi, but
they work well in concert.</p>

<p>Thialfi&rsquo;s abstraction is carefully chosen to enable simple fault
tolerance.  Thialfi is <em>always</em> allowed to respond to clients with &ldquo;I
don&rsquo;t know&rdquo;; in the worst case, the client will fall back to polling
the application server.  This is a key choice because it means Thialfi
is free to drop any and all state, as long as it never incorrectly
claims to know the version of an object.  In fact, the initial design
presented by the paper (4.1) is entirely in-memory, yet can survive
data center failures.  At first, this may seem like an undesirable
abstraction to build a client application atop, but, in fact, clients
already have to deal with this when they first run.</p>

<p>Since the only thing Thialfi can tell a client is &ldquo;object X might have
changed&rdquo;, it&rsquo;s free to coalesce, repeat, and generate spurious
notifications.  The only thing it&rsquo;s not allowed to do is drop a
notification entirely.  Since, faced with arbitrary faults, there&rsquo;s no
way to know whether or not a notification was dropped, Thialfi
conservatively generates spurious notifications whenever a failure
<em>might</em> have caused a notification to be dropped.</p>

<p>Responsibility for hard state is colocated with the nodes that care
about that hard state.  A client is responsible for its object
registrations, because if the client dies, its registrations don&rsquo;t
matter.  Likewise, an application server is responsible for
application data, because it has to persist that anyway (and if it
dies, there&rsquo;s nothing to send notifications about).</p>

<p>Regular paths and error handling paths are the same wherever possible.
This means they don&rsquo;t have to distinguish errors from regular
operation and that the code is more likely to be correct (error
handling code is notoriously buggy largely because it doesn&rsquo;t get
exercised).  For example, initial registration, modifications to
registrations, handling server loss of registration state, and
re-registering after migration are all handled in the same way: the
client and server exchange digests of what they think the registration
state is in every heartbeat; if these are out of sync, the client
simply resends its entire set of registrations.  This extends to the
user of the client API as well: as mentioned above, client cold-start
and the loss of version state in Thialfi are handled identically at an
API level.</p>

<p>Ultimately, Thialfi makes recovering from failure the responsibility
of application servers and clients, keeping itself off the critical
path for anything.  This seems simple, but achieving this without
burdening application servers and clients requires careful and
conscious design.</p>

<h2>Discussion</h2>

<p>We felt there was one dark corner of Thialfi&rsquo;s design that makes it
difficult to completely understand its fault tolerance properties.
Application servers post version changes to Thialfi via Google&rsquo;s
reliable pub-sub system, about which the paper is devoid of details.
It&rsquo;s difficult to tell how the pub-sub system could fail and how this
would affect Thialfi.  Furthermore, if Thialfi bootstraps off a
reliable pub-sub system, what would have happened if Google had simply
exposed a client API to subscribe to the pub-sub system?  Our best
guess is that it wouldn&rsquo;t have scaled to millions of clients like
Thialfi does, but we can only guess.</p>

<p>Thialfi&rsquo;s approach of signaling rather than notification reprises the
long debate between &ldquo;level-triggered&rdquo; and &ldquo;edge-triggered&rdquo; interfaces
in OS and hardware design.  Level-triggered interfaces like <code>poll()</code>
and PCI interrupts have very similar properties to Thialfi&rsquo;s API
(e.g., like a Thialfi notification, <code>poll()</code> only tells the caller
that data is available on an FD, not what the data is, or how many
times the FD was written to).  On the other hand, edge-triggered
interfaces more closely resemble the reliable pub-sub interface that
Thialfi explicitly rejected.  Historically, level-triggered interfaces
have generally scaled better, and we found it interesting to see this
revisited and reinforced from a very different perspective.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[VR Revisited]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/06/20/vr-revisited/"/>
    <updated>2013-06-20T18:00:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/06/20/vr-revisited</id>
    <content type="html"><![CDATA[<h2>Why are we reading Viewstamped Replication Revisited?</h2>

<p><a href="http://pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication</a>
 is a mechanism for providing replication through a
Primary / Backup scheme. This paper provides a distilled view of this
technique along with several optimizations that can be applied. In particular,
this paper focuses solely on the Viewstamped Replication protocol
without looking at any specific implementation or uses.</p>

<p>While a general Primary / Backup replication scheme may seem easy to
get right, considering how to handle view changes and the
many optimizations that others have come up with over the years, this
paper provides a go-to source for building such a system.</p>

<p>Previously we have looked at the Paxos protocol for
consensus as well as Spanner which is an externally consistent distributed storage system.
This paper sits in-between these two extremes in that it is a technique
used for replication, thus being more complete than one-time consensus, while
eliding the details of a full storage system like Spanner.</p>

<h2>What is Viewstamped Replication?</h2>

<p>Viewstamped Replication (referred to as VR in the remainder of this post) is a
replication protocol that uses consensus to support a replicated state machine.
The replication state machine allows clients using this service to run operations
that either view or modify the state, upon which other services can be built
such as a distributed key-value store.</p>

<p>One goal of VR is to support <em>f</em> failures using 2<em>f</em> + 1 nodes, so it
should be used in a distributed system where failures may
occur. Beyond normal operation, the system handles two scenarios:
changing the primary between the current list of members in a <em>view
change</em> as well as changing the set of participating members in a
<em>reconfiguration</em>.  This paper assumes that state replicated on many
machiens can be used for durability, thus avoiding a potential latency
of writing to persistent storage.  Unfortunately if VR is run in one
datacenter all the machines may be on the same power source and thus
in the same failure domain, so this might not be practical without a UPS.</p>

<h2>Architecture</h2>

<p>Some number of clients will be interacting directly with a service such as a
key-value store. The clients use a VR library or proxy that will abstract away
the details of the replication so that client code will use the abstraction of
&ldquo;read&rdquo; and &ldquo;write&rdquo; like operations on client defined state. Clients will use a
monotonically increasing request number which will allow the system to detect
duplicate requests.</p>

<p>Some number of servers 2<em>f</em> + 1 when trying to support
<em>f</em> failures) will run the VR code as well as the service code. The
VR code on the server will determine when to apply operations on the service
data and push these operations up to the service code. Note that these servers
can return after a failure (or network partition) and will only result in a
view change which would require getting the failed / partitioned node(s) up to
date.</p>

<p>Note that in VR as presented in this paper, the operations are performed on
several different replicas (instead of shipping the data around after the
operation has been performed). As a result, the operations  must be
deterministic.  It is mentioned in the
paper that particular techniques can be used to ensure this property.</p>

<p>As this is a Primary / Backup based system, the ordering is decided by the
primary, however <em>f</em> + 1 replicas must know about a request before
executing it to ensure durability despite failures and that ordering is guaranteed.</p>

<h2>System Operation</h2>

<p>The system is in one of three states. These states are normal
operation, view changes when the system needs a new primary, and reconfiguration
when membership is changing. The primary node is deterministically chosen based
on the configuration (list of servers) and the view numbers. As a result
this system does not need to rely on voting or consensus (e.g. longest
log) to determine the next leader to take over.</p>

<h3>Normal Operation</h3>

<p>Replicas use a view number to determine if they are in the correct state. If
the sender is behind the receiver will drop the message, on the other hand if a
sender is ahead the receiver must update itself first and then process the
message.</p>

<p>A client sends a request to perform an operation at the server. The server then
sends <code>Prepare</code> messages to each of the backups and waits for
<em>f</em> <code>PrepareOk</code> responses. Once it has received these responses it can
assume that the message will persist and it applies the operation by making an
upcall to the service code and finally replies to the client. A backup will
perform the same operation but does not reply to the client.</p>

<h3>View Changes</h3>

<p>View changes occcur when the system needs to elect a new leader. A key correctness
requirement for the protocol is that every operation executed by an up-call to
the service code must make it into the new view in the same order as the original
execution. To achieve this requirement, <em>f</em> + 1 logs are obtained and merged
using the view number to break conflicts in op number ordering.</p>

<p>Protocol:</p>

<ul>
<li>Replica sends <code>StartViewChange</code> to all other replicas</li>
<li>Receives f responses, sends <code>DoViewChange</code> to new primary</li>
<li>New primary waits for <em>f</em> + 1 <code>DoViewChange</code> messages before assuming new view</li>
</ul>


<p>Note that sending a suffix of the log (e.g. 1-2 entries) in the
<code>DoViewChange</code> message will likely bring the new server up to date
without requiring any additional state transfer from the replicas.</p>

<h3>Recovery</h3>

<p>Server recovery has the correctness requirement that a replicamust be as up to
date as it was when it crashed, otherwise it may forget about ops that it
prepared. This is achieved by receiving state from other replicas using the
following recovery protocol (note that nodes do not participate in request
processing or view changes during the recovery phase):</p>

<ul>
<li>Recovering node send “Recovery” message to all</li>
<li>All reply with “RecoveryResponse”, view number and nonce (and log, etc if primary)</li>
<li>Replica waits for <em>f</em> + 1 responses (and primary), applies log and begins normal processing</li>
</ul>


<p>Note that in the theoretical solution, logs may be prohibitively big, however
optimizations exist to trim the log (e.g. snapshots).</p>

<p>Client recovery is simply achieved by starting any new request with the old
request number (obtained from replicas) + 2.</p>

<h3>Reconfiguration</h3>

<p>Though reconfiguration is discussed later in the paper, it fits the flow here
in that it is essentially the last mode of operation. Beyond that several
optimizations are considered to speed up various parts of the system.</p>

<p>Reconfiguration is used to add/remove nodes to the system (thus changing the
<em>f</em> failures that the system can handle) or to upgrade or relocate
machines (for long running systems). A reconfiguration is instantiated by an
administrator of the system. In this paper, the term &ldquo;epoch&rdquo; refers to a configuration
number and the &ldquo;transitioning&rdquo; state refers to a node that is currently changing
configurations. The reconfiguration is started similarly to other operations by
sending the operation to the leader, however included in this operation is the
new configuration (list of participating machines). The primary will then
send the <code>StartEpoch</code> message and wait for <em>f</em> responses.</p>

<p>Any new replicas will be brough up to date before the epoch change (by sending
them a list of operations or a snapshot + diff). Once a new replica is up to date
it will send an <code>EpochStarted</code> message to old replicas. Thus, once
an old replica (that is not in the new configuration) has received <em>f</em> + 1
<code>EpochStarted</code> messages, it is free to shut down. Note that one
particular optimization is to bring new machines up to date (e.g. warm-up)
before performing the reconfiguration to minimize the down time during
transition as nodes will not respond to messages for earlier epochs or while
transitioning.</p>

<p>The administrator can determine status of old replicas by sending out
<code>CheckEpoch</code> messages and then know when it is safe to shut down old
machine(s).</p>

<p>One issue for this system is rendesvous, however the solution provided in the
paper is to simply publish it somewhere out-of-band.</p>

<h4>Efficient Recovery</h4>

<p>One concern is achieving efficient recovery of failed server machines. As
presented previously, sending the missing log could result in the transfer of a
substantial amount of data. One way to solve this is to store application state
as a &ldquo;checkpoint&rdquo; that represents a log prefix, thus allowing the transfer of a
potentially much more compressed state + some log diff.</p>

<p>After a server creates a checkpoint, it can mark any modification as &ldquo;dirty&rdquo;
and provide those as the diff over the last created checkpoint. A perhaps
generic way of accomplishing this diff is to use merkle trees to efficiently
determine which pages are dirty to avoid sending the entire checkpoint.
Finally, if the state is too large to transfer then the paper suggests an
out-of-band mechanism (e.g. sneakernet).</p>

<p>Note that checkpoints allow garbage collection of the log, but the log may
still be required to bring back a recovering node, so it is beneficial to keep
some of it, else the system will suffer the cost of state transfer between
servers.</p>

<h3>State Transfer</h3>

<p>State transfer between two replicas can exist in essentially two cases. One
being that there are missing operations in the current view. To solve this the
replica will simply obtain said operations from another replica. The second and
more difficult situation is that there was a view change in which case the
replica will set it&rsquo;s op-number to the latest commit-number and obtain updates
from another replica. If there is a gap in this replica&rsquo;s log, it will need to
fast-forward using application state (such as a checkpoint).</p>

<h3>Other Optimizations</h3>

<p>There are a handful of optimizations that others have come up with since the
original VR procool was presented. One example is using Witnesses that aren&rsquo;t
performing the operations. This is a simple extension to the system by having
<em>f</em> replicas act as log keepers that are only used for view changes and
recovery.</p>

<p>Another optimization is to batch operations which simply implies that if the
system is busy then piggy-back several operations in a single message.</p>

<p>Finally, <em>fast reads</em> is a technique used in several replication systems.
This essentially allows the primary to respond to a read request without going
through the full protocol. These can be performed at the Primary, though the
use of leases and loosely synchronized clocks are required to maintain
consistency. Additionally, if the user of the system is ok with stale data,
then reads can be served at backups where a backup will reply to a client if it
has seens commits up to that request.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cheriton and Skeen]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/06/13/cheriton-and-skeen/"/>
    <updated>2013-06-13T16:21:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/06/13/cheriton-and-skeen</id>
    <content type="html"><![CDATA[<h2>Why did we read this paper?</h2>

<p>Though from 1993, in its time <a href="http://cs3.ist.unomaha.edu/~stanw/papers/93-catocs.pdf">this paper</a>
 sparked some controversy,
provoking an impassioned <a href="http://www.csie.fju.edu.tw/~yeh/research/papers/os-reading-list/birman93response-to-cheriton.pdf">response</a>.
  We wanted to understand the debate
about the question of providing ordering guarantees as part of the network.</p>

<h2>What is CATOCS?</h2>

<p>CATOCS stands for &ldquo;causally and totally ordered communication.&rdquo;  It means that
messages are delivered in the order they are sent, as specified by a
<em>happens-before</em> relationship.  A synonym for happens-before is
<em>causally-precedes</em>.  The following is the definition of happens-before:</p>

<p><em>Happens-before</em>: m1 happens before m2 if there exists a P such that
m1 is sent or received at P before P sends m2.</p>

<p>Under causal ordering, concurrent writes may be seen in different
orders at different participants.</p>

<p><em>Total ordering</em> is a stronger property; it ensures that messages are
delivered to all participants in the same order.</p>

<h2>What is wrong with CATOCS?</h2>

<p>Cheriton and Skeen make the point that ensuring CATOCS in the network
is prohibitively expensive, and since most applications need something
stronger than CATOCS anyway (such as transactional consistency), there
is no point in doing so.  They claim that CATOCS violates the
<a href="http://en.wikipedia.org/wiki/End-to-end_principle">End To End principle</a>,
which states that application-specific functionality should reside at
the end nodes of a network, instead of the intermediary nodes.  It is
worth noting that this principle is frequently misapplied.</p>

<p>They identify the following limitations in CATOCS systems:</p>

<ul>
<li><p>Can&rsquo;t say &ldquo;for sure&rdquo;</p>

<p>There are almost always hidden channels in a group of nodes, or
methods of communication not captured by the network.  For example,
processes might all write to a shared database, and writes seen at
that database might not preserve CATOCS.  Similarly, threads on a
single machine might share memory.</p>

<p>They use a contrived example of an independent &ldquo;FIRE&rdquo; message
appearing before an unrelated &ldquo;FIRE OUT&rdquo; message, and thus the
system might appear to not be in a &ldquo;FIRE&rdquo; state, because it
misapplied the unrelated &ldquo;FIRE OUT&rdquo;.</p></li>
<li><p>Can&rsquo;t say &ldquo;together&rdquo;</p>

<p>As stated above, applications often require transactional semantics.
CATOCS does not help with the serialization or atomicity between
<em>groups</em> of messages.  A system with this property obviates the need
for CATOCS.</p></li>
<li><p>Can&rsquo;t say &ldquo;whole story&rdquo;</p>

<p>Happens-before might not be enough.  Applications might require
linearizability or sequential consistency.</p></li>
<li><p>Can&rsquo;t say &ldquo;efficiently&rdquo;</p>

<p>They claim CATOCS protocols don&rsquo;t show any efficiency gains over
state-level techniques, and in fact are very inefficient.
Unfortunately the paper does not provide actuala measurements.</p>

<p>False causality could be an issue; happens-before enforces ordering
that the application might not care about.</p></li>
</ul>


<p>Cheriton and Skeen would prefer to see state-level and
application-specific ordering techniques.</p>

<h2>A Response</h2>

<p>Birman sees this paper as a critique of Isis, and claims that Cheriton
and Skeen misrepresented the true debate.  CATOCS should not be
considered in isolation, but when transactional semantics are
required, techniques like <em>virtual synchrony</em> should be used in
conjunction with CATOCS.</p>

<p>Birman makes the point that application developers should not even
need to consider their semantic ordering needs, instead the network
should provide guarantees for them, reducing user-visible design
complexity.</p>

<p>He also claims that their assumptions about overhead are completely off.</p>

<h2>Conclusion</h2>

<p>This seems founded in a more general debate &mdash; should systems developers aim
for efficiency and performance first, giving application developers
total control but leaving them to layer safety accordingly, or should they
apply an unknown cost to all users, making strong semantics an
indelible part of the system?</p>

<p>In the space of datastores, the former argument seems to have &ldquo;won&rdquo;.
Most application developers do not run their databases with
serializability or even other forms of slightly weaker consistency.
There is a move towards general key/value stores which do not provide
transactions or any ordering guarantees and might not necessarily pay
the penalty of writing to disk for durability.  It seems as though
application developers have chosen performance over safety, and
developed techniques to accomodate inconsistencies on their own (one
of which might be simply ignoring them).</p>

<p>We found it extremely difficult to reason about these two papers
without looking at a real system with a concrete design.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PacificA]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/06/06/pacifica/"/>
    <updated>2013-06-06T18:00:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/06/06/pacifica</id>
    <content type="html"><![CDATA[<h2>Why are we reading PacificA?</h2>

<p>In the <a href="http://research.microsoft.com/apps/mobile/Publication.aspx?id=66814">PacificA paper</a>,
the authors describe very clearly how to
properly implement a primary/backup replicated storage layer with
strong consistency.</p>

<p>At RICON East 2013, Kyle Kingsbury presented a performance evaluation
of several commercial datastores. This evaluation is specifically
geared toward assessing the correctness of distributed systems in the
face of network partitions. Kyle has turned his talk into a <a
href="http://aphyr.com/tags/jepsen">series of blog posts</a>
describing the not-so-encouraging results.</p>

<p>In previous weeks we have discussed consensus protocols like Paxos and
Raft that provide strong consistency to clients and are provably safe
in the event of a network partition. However, many commercial
datastores use some variation of primary/backup replication, and
according to the above mentioned evaluation, do not behave as one
would hope when the network misbehaves. So is there a (provably)
correct way to implement primary/backup replication? PacificA proposes
such an algorithm and presents an evaluation of a distributed log
built atop this protocol.</p>

<h2>The Protocol</h2>

<h3>Configuration and Data Management</h3>

<p>The PacificA protocol separates the responsibility for data storage
from the task of configuration management. From the perspective of the
servers participating in the primary/backup protocol, the
configuration of the storage cluster is handled by an authoritative
3rd party (a separate Paxos cluster in their implementation).</p>

<h3>Replicating Data</h3>

<p>The server designated as the current primary is responsible for
interacting with clients. It handles all reads (from its committed
state) and writes. When a write is requested, the primary assigns a
sequence number to the operation and sends a <code>prepare</code>
message all replicas. When a replica receives a <code>prepare</code>
message, it adds the operation to its log and acknowledges the
primary. When the primary receives an acknowledgement from all
replicas it commits the operation and responds to the client. The
commit point on the replicas is advanced in a subsequent message from
the primary. This protocol preserves the Commit Invariant, which
guarantees the following properties of the logs on the primary
<code>P</code> and replicas <code>R</code>:</p>

<p><code>commited<sub>R</sub> &sube; committed<sub>P</sub> &sube; prepared<sub>P,R</sub></code></p>

<p>Because of this invariant, during failover recovery it is possible for
a new primary (which is always a previous replica) to bring its commit
point up to (and possibly past) the commit point of the previous
primary while guaranteeing that the operations in its log are
consistent with the previous primary&rsquo;s state.</p>

<h3>Detecting Failures</h3>

<p>The PacificA protocol uses a lease mechanism to detect failures in the
system. The primary and replicas all track how long it has been since
communicating with the others (using normal message traffic or with
heartbeat messages injected during idle periods) and assume a failure
has occurred if no communication occurs within a given timeout
period. When a server realizes a failure, it stops processing messages
and requests a configuration change from the 3rd party configuration
manager. To prevent a situation where two servers believe they are the
primary, the timeout periods differ depending on the server&rsquo;s current
role. The primary will timeout if it has not heard from one of its
replicas in a &ldquo;lease period&rdquo; of <code>T<sub>L</sub></code> seconds,
whereas a replica will timeout if it has not heard from the primary in
a &ldquo;grace period&rdquo; of <code>T<sub>G</sub></code> seconds. If
<code>T<sub>L</sub> &lt; T<sub>G</sub></code> then the current primary
will always realize the failure first (if it is still alive) and stop
processing messages. It will also propose a new configuration first,
allowing it to continue as the primary, which may mitigate disruptions
to the service. Should the primary fail or become partitioned, the
grace period will expire and one of the replicas will request to be
the new primary.</p>

<p>The appeal of this approach is its implementation simplicity.
However, for a short period of time during our discussion we were
convinced that this scheme could result in a situation where two
servers believed they were the primary and could serve (possibly
stale) reads, violating the strong consistency guarantee. The scenario
involved a heartbeat <code>ACK</code> that was delayed by the network,
extending the primary&rsquo;s lease but not the replica&rsquo;s grace period. It
turns out we were wrong. A careful reading of the paper specifies that
the lease period is measured from the <em>sending time</em> of the last
acknowledged heartbeat. We had mistakenly used the reception time in
our example, and had convinced ourselves that the protocol was
flawed. While this is clearly our fault, it is a caution that even
simple schemes can be implemented incorrectly, and would probably work
under normal conditions before eventually causing big problems.</p>

<h3>Changing Configurations</h3>

<p>When a new primary is selected it must reconcile the state of the
system using its log. It does so by sending new prepare messages for
any operations that were prepared by the previous primary but not (to
its knowledge) committed. This will bring its commit point up to or
past that of the previous primary before responding to clients,
preserving strong consistency. In addition, the other replicas must
truncate their logs to purge any prepared operations that extend
beyond the log of the new primary.</p>

<p>When a new replica is brought online it must obtain the current state
from another server before it can participate in replication. If
starting cold, this could halt progress in the system for a long time,
so replicas are allowed to join as candidates (their acks are not
required for committing at the primary) while they catch up.</p>

<p>As far as configuration changes go, this strategy appears to be fairly
straightforward (especially after working through some of the
subtleties in Raft).</p>

<h2>Implementing a Distributed Log</h2>

<p>The paper shifts focus and includes an explanation of how to implement
three variations of a distributed log with features similar to
Bigtable. We find this portion of the paper to be somewhat superfluous
in the context of the primary/backup protocol.</p>

<h2>Evaluation</h2>

<p>The most interesting part of the evaluation is Figure 5. There are two
portions of this graph that sparked discussion, from time 60-90 and
160-300. The first occurs after the primary is killed and the second
occurs after a replica is added.</p>

<p>We were a bit shocked by the amount of time that the system was
unavailable during failover (time 60-90). Part of this downtime is due
to the reconciliation process of the new primary, but we suspect this
is relatively short. Certainly, part of the problem is failure
detection, which relies on timeouts. In this case, the lease period
was 10 seconds and the grace period was 15 seconds. This means a good
portion of that downtime is likely due to the replicas waiting to
timeout and propose a new configuration. It prompted one reviewer to
exclaim &ldquo;wow, timeouts are a real bummer&rdquo;.</p>

<p>While the timeout scheme is simple to implement, it also must be
tuned. You want the timeout to be long enough so that temporary
hiccups in the network do not cause undue reconfigurations, but short
enough so that a real failure is detected in a reasonable amount of
time. It seems like the type of tuning parameter that may be hard to
get right. It would have been nice to see an evaluation in which
various lease/grace periods were tried in a failure scenario.</p>

<p>During time 160-300 the server that was previously the primary (and
was killed) rejoins the group as a secondary. What surprised us was
the drop in client-perceived throughput by the addition of a replica,
and how long it took for the system to recover. This re-emphasizes the
need to set the timeouts correctly so that there is not a lot of churn
in the configuration. Even when there is no change of primary, a
reconfiguration causes a significant performance penalty (though it is
still available).</p>

<p>We were also curious about the read throughput in Figure 4. The total
throughput seems shockingly low, but we reason this is due to disk
throughput. The experiment performs random reads on a large dataset,
which is partially contained in memory, several checkpoint files, and
an on-disk image.</p>

<h2>Discussion</h2>

<p>We spent some time discussing when a primary/backup scheme might be
used instead of a consensus algorithm like Paxos. The authors provide
some pros and cons in the paper, most of which seem to be a wash
(serving reads from a single server, external configuration) or in
favor of Paxos (not bottlenecked by slowest server). The main reasons
for preferring primary backup are simplicity and availability.</p>

<p>The simplicity argument is easy to understand (Paxos can be confusing)
but only to a certain extent. Paxos has become more accessible over
time, with simpler explanations, available implementations, and
variations that make it more practical. In addition, if the
primary/backup protocol in this paper is implemented as suggested by
the authors, then Paxos should be used to implement the configuration
manager, requiring the need to implement Paxos anyway.</p>

<p>The authors make an availability argument (versus consensus) in favor
of their scheme. They claim, rightfully so, that their system can
survive <em>n-1</em> failures within a replica group and still make progress
(the remaining server is the primary with no backups). On the other
hand, a consensus algorithm cannot make progress when a majority of
the servers are unavailable. This is a valid argument if the service
values availability over durability; that is, it prefers to make
progress even if writes are only persisted to one server. If, however,
the service is deployed for durability (e.g. with a QoS that requires
writes are persisted to 3 servers) then a replica group of 5 servers
would have the same availability as a Paxos group of the same size.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[spanner]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/05/30/spanner/"/>
    <updated>2013-05-30T18:05:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/05/30/spanner</id>
    <content type="html"><![CDATA[<h2>What is Spanner?</h2>

<p><a href="https://www.usenix.org/system/files/conference/osdi12/osdi12-final-16.pdf">Spanner</a>
 is a highly distributed, externally consistent database
developed by Google.  It provides replication and transactions over a
geographically distributed set of servers.  Spanner uses time bounds,
Paxos, and two-phase commit to ensure external consistency.</p>

<h2>Interesting Ideas</h2>

<p>Spanner uses clocks with bounded uncertainty to provide synchrony between
servers. It also shards an application&rsquo;s data to provide fine-grained load
distribution.</p>

<p>Spanner uses the TrueTime API (TT) to synchronize time between
servers. A call to <code>TT.now()</code> gives a range guaranteed to
contain the actual time.</p>

<p>Spanner has two kind of reads.  The first reads the most recent value
of a key or set of keys, called a read-only transaction.  The second
is a snapshot read which is executed at a specific timestamp in the
past.  Using a combination of Paxos leases and TrueTime guarantees to
agree on a timestamp, Spanner can execute snapshot reads and read-only
transactions without locks or two-phase commit.</p>

<p>Spanner peforms schema changes atomically without blocking, by picking
a future time for the change to occur. Other read and write operations
choose a timestamp so that at each replica, the operation performs
either before or after the schema change.</p>

<h2>Subtleties</h2>

<p>Since Spanner commits only when <code>TT.now.after(timestamp)</code>
is true, we are guaranteed that from now on <code>TT.now.latest()</code> will
always be larger than the commited timestamp on all servers.</p>

<p>Spanner very carefully chooses timestamps for RW transactions to ensure when
they are safely visible.  They call this the <em>commit-wait rule</em>.</p>

<h2>Questions</h2>

<ul>
<li><p><strong>Why is the write throughput so low?</strong> 4K ops/sec for 50 paxos
servers of one replica each (so not running Paxos), not waiting for
any other commit times, seems very low.</p></li>
<li><p><strong>Why is the throughput experiment in 5.1 CPU bound?</strong></p></li>
<li><p><strong>What happens if we use logical time(which preserves causality)
rather than the true time?</strong> Maybe external consistency breaks, but
the system is still sequentially consistent.</p></li>
</ul>


<p><a href="http://pdos.csail.mit.edu/6.824/notes/l07.txt">6.824 notes</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[raft]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/05/23/raft/"/>
    <updated>2013-05-23T18:00:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/05/23/raft</id>
    <content type="html"><![CDATA[<p>Disclaimer:  This paper is still under submission.</p>

<h2>Why did we read about Raft?</h2>

<p><a href="ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf">Raft</a>
 is a new consensus algorithm that is optimized for “ease of
implementation”.  Its main purpose is to present a protocol that is
more understandable than Paxos, which, for many practitioners, is
difficult to implement correctly.  Viewstamped Replication is more
similar to Raft, however it is far less popular than Paxos, so it is
unfortunately not focused on in the paper.</p>

<p>All of these consensus algorithms operate as long as a majority of
servers are functioning.  So you would require 2f+1 servers to
tolerate f failures.  Servers are assumed to fail by stopping, though
they might recover with state from stable storage (such as disk).  We
read this paper because there are very few consensus algorithms, and
something that is easier to understand than Paxos sounds great!  Raft
pros and cons</p>

<p>The paper designs and ensures invariants around the data structure
that Paxos, in practice, is used for &mdash; a replicated state machine
log.  This abstraction is nice, because it is easier to think about
operating on a sequential log and ensuring a small number of
properties rather than running multiple independent instances of
Paxos.  Raft assigns leaders on a per-term basis (i.e., epochs), and
terms are used as an implicit coordination mechanism.  It also
supports configuration changes (removing or adding nodes from the
system) while still serving requests.</p>

<p>The bulk of the paper is fairly digestible and it was nice that the
mechanisms to ensure safety were concentrated in a small handful of
subsections.  However distributed consensus is hard, so verifying
Raft’s safety still took effort.  Many members of the reading group
have a basic understanding of Paxos, so we didn’t necessarily feel
that understanding Raft’s correctness was easier than understanding
Paxos.</p>

<p>One of the group members noted that he wrote an implementation of Raft
from the paper and some basic tests “just worked”, which was
surprising.  However, note that a production implementation of Raft
would require many of the same things required to properly implement
and run Paxos (see Paxos Made Live).  Namely, you’d have to consider
and handle disk corruptions, safely implement snapshots, and use
something like leases so that reads don’t always require running a
round of the consensus algorithm.  These added features are necessary
in a practical system, and would complicate a Raft implementation as
much as a Paxos one.</p>

<p>In terms of performance, Raft can add log entries with f+1 roundtrips
total, whereas Paxos requires 2(f+1).  Paxos can be further optimized
by using leaders, and both systems can benefit from batching log
entries.  The space used for “understandability” experiments could
have been better used to show more performance numbers or more
explanation about correctness.</p>

<h2>Main Subtleties</h2>

<p>One of the most interesting parts of the algorithm is the commit point
for a log entry, or the point at which you can determine that even if
f servers fail, the log entry will be present.  It turns out that
because nodes perform leader election by consulting and sending only
their log length and term number, a log entry that is simply on the
majority of the nodes can still be overwritten when a new leader is
elected.  The paper bolds the actual requirement:</p>

<p>A log entry may only be considered committed if the entry is stored on
a majority of the servers; in addition, at least one entry from the
leader’s current term must also be stored on a majority of servers.</p>

<p>Figure 7 shows how this can happen.  Consider this case with five
servers, S1, S2, S3, S4, S5.  We use the notation Value:Term in each
slot.  S1 is leader in term 2, and replicates an entry X for slot 2,
it goes to 2 servers (S1, S2).</p>

<pre>     
slot    1     2
S1     1:1  X:2
S2     1:1  X:2
S3     1:1
S4     1:1
S5     1:1
</pre>


<p>S1 crashes before it can finish replicating X:2, S5 is elected leader
for term 3 (with votes from S3, S4, and S5) and stores Y in slot 2</p>

<pre>
slot    1     2
S1     1:1  X:2
S2     1:1  X:2
S3     1:1
S4     1:1
S5     1:1  Y:3
</pre>


<p>S5 crashes before it can replicate Y, and S1 is elected leader again,
now for term 4.  S1 finishes replicating X:2, and adds a new entry, Z
in slot 3, but crashes before it can completely replicate it</p>

<pre>
slot    1     2     3
S1     1:1  X:2   Z:4
S2     1:1  X:2  
S3     1:1  X:2
S4     1:1
S5     1:1  Y:3
</pre>


<p>S5 becomes leader for term 5 because it has the most recent term out
of the remaining nodes.  S5 will force a Y into slot 2 (Note that S5
has Y in slot 2 and S1,S2,S3 had an X in slot 2.)</p>

<pre>
slot    1     2     3
S1     1:1  X:2   Z:4
S2     1:1  Y:3 
S3     1:1  Y:3
S4     1:1
S5     1:1  Y:3
</pre>


<p>Note that if S1 had gotten Z into two additional logs (two out of S2,
S3, S4), then S5 could never have been elected leader, because its log
was not long enough.</p>

<p>We found this surprising.</p>

<h2>Conclusions</h2>

<p>There are some nice aspects of Raft &mdash; the log as a first order
primitive, the method of configuration changes, and only two RPCs
(though one could argue AppendEntry is heavily overloaded).
Unfortunately, the authors’ main point falls flat for two reasons:
First, they primarily compare their system to Paxos while their system
is much closer to Viewstamped Replication. They claim this is
reasonable because Paxos is much more widely used. Second, they claim
Raft is much simpler and easier to implement (compared to Paxos).
However, in our reading, Raft didn’t appear to be any less subtle (it
took effort to convince ourselves of the correctness of the example
explained here) and several things that make Paxos difficult to
implement are still evidenced in Raft.</p>

<p>We don’t see many new consensus algorithms, so this was still very interesting to read and discuss.</p>

<h2>Open Questions</h2>

<p>Did they make any comparisons between Raft and Paxos that couldn’t have been made between Paxos and VR?</p>
]]></content>
  </entry>
  
</feed>
