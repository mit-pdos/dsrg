<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[DSRG]]></title>
  <link href="http://pdos.csail.mit.edu/dsrg/atom.xml" rel="self"/>
  <link href="http://pdos.csail.mit.edu/dsrg/"/>
  <updated>2013-06-05T13:13:49-04:00</updated>
  <id>http://pdos.csail.mit.edu/dsrg/</id>
  <author>
    <name><![CDATA[PDOS]]></name>
    <email><![CDATA[narula@mit.edu]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[spanner]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/05/30/spanner/"/>
    <updated>2013-05-30T18:05:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/05/30/spanner</id>
    <content type="html"><![CDATA[<h2>What is Spanner?</h2>

<p>Spanner is a highly distributed, externally consistent database
developed by Google.  It provides replication and transactions over a
geographically distributed set of servers.  Spanner uses time bounds,
Paxos, and two-phase commit to ensure external consistency.</p>

<h2>Interesting Ideas</h2>

<p>Spanner uses clocks with bounded uncertainty to provide synchrony between
servers. It also shards an application&rsquo;s data to provide fine-grained load
distribution.</p>

<p>Spanner uses the TrueTime API (TT) to synchronize time between
servers. A call to <code>TT.now()</code> gives a range guaranteed to
contain the actual time.</p>

<p>Spanner has two kind of reads.  The first reads the most recent value
of a key or set of keys, called a read-only transaction.  The second
is a snapshot read which is executed at a specific timestamp in the
past.  Using a combination of Paxos leases and TrueTime guarantees to
agree on a timestamp, Spanner can execute snapshot reads and read-only
transactions without locks or two-phase commit.</p>

<p>Spanner peforms schema changes atomically without blocking, by picking
a future time for the change to occur. Other read and write operations
choose a timestamp so that at each replica, the operation performs
either before or after the schema change.</p>

<h2>Subtleties</h2>

<p>Since Spanner commits only when <code>TT.now.after(timestamp)</code>
is true, we are guaranteed that from now on <code>TT.now.latest()</code> will
always be larger than the commited timestamp on all servers.</p>

<p>Spanner very carefully chooses timestamps for RW transactions to ensure when
they are safely visible.  They call this the <em>commit-wait rule</em>.</p>

<h2>Questions</h2>

<ul>
<li><p><strong>Why is the write throughput so low?</strong> 4K ops/sec for 50 paxos
servers of one replica each (so not running Paxos), not waiting for
any other commit times, seems very low.</p></li>
<li><p><strong>Why is the throughput experiment in 5.1 CPU bound?</strong></p></li>
<li><p><strong>What happens if we use logical time(which preserves causality)
rather than the true time?</strong> Maybe external consistency breaks, but
the system is still sequentially consistent.</p></li>
</ul>


<p><a href="http://pdos.csail.mit.edu/6.824/notes/l07.txt">6.824 notes</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[raft]]></title>
    <link href="http://pdos.csail.mit.edu/dsrg/blog/2013/05/23/raft/"/>
    <updated>2013-05-23T18:00:00-04:00</updated>
    <id>http://pdos.csail.mit.edu/dsrg/blog/2013/05/23/raft</id>
    <content type="html"><![CDATA[<p>Disclaimer:  This paper is still under submission.</p>

<h2>What is Raft and why was it written and why did we read it?</h2>

<p>Raft is a new consensus algorithm that is optimized for “ease of
implementation”.  Its main purpose is to present a protocol that is
more understandable than Paxos, which, for many practitioners, is
difficult to implement correctly.  Viewstamped Replication is more
similar to Raft, however it is far less popular than Paxos, so it is
unfortunately not focused on in the paper.</p>

<p>All of these consensus algorithms operate as long as a majority of
servers are functioning.  So you would require 2f+1 servers to
tolerate f failures.  Servers are assumed to fail by stopping, though
they might recover with state from stable storage (such as disk).  We
read this paper because there are very few consensus algorithms, and
something that is easier to understand than Paxos sounds great!  Raft
pros and cons</p>

<p>The paper designs and ensures invariants around the data structure
that Paxos, in practice, is used for &mdash; a replicated state machine
log.  This abstraction is nice, because it is easier to think about
operating on a sequential log and ensuring a small number of
properties rather than running multiple independent instances of
Paxos.  Raft assigns leaders on a per-term basis (i.e., epochs), and
terms are used as an implicit coordination mechanism.  It also
supports configuration changes (removing or adding nodes from the
system) while still serving requests.</p>

<p>The bulk of the paper is fairly digestible and it was nice that the
mechanisms to ensure safety were concentrated in a small handful of
subsections.  However distributed consensus is hard, so verifying
Raft’s safety still took effort.  Many members of the reading group
have a basic understanding of Paxos, so we didn’t necessarily feel
that understanding Raft’s correctness was easier than understanding
Paxos.</p>

<p>One of the group members noted that he wrote an implementation of Raft
from the paper and some basic tests “just worked”, which was
surprising.  However, note that a production implementation of Raft
would require many of the same things required to properly implement
and run Paxos (see Paxos Made Live).  Namely, you’d have to consider
and handle disk corruptions, safely implement snapshots, and use
something like leases so that reads don’t always require running a
round of the consensus algorithm.  These added features are necessary
in a practical system, and would complicate a Raft implementation as
much as a Paxos one.</p>

<p>In terms of performance, Raft can add log entries with f+1 roundtrips
total, whereas Paxos requires 2(f+1).  Paxos can be further optimized
by using leaders, and both systems can benefit from batching log
entries.  The space used for “understandability” experiments could
have been better used to show more performance numbers or more
explanation about correctness.</p>

<h2>Main Subtleties</h2>

<p>One of the most interesting parts of the algorithm is the commit point
for a log entry, or the point at which you can determine that even if
f servers fail, the log entry will be present.  It turns out that
because nodes perform leader election by consulting and sending only
their log length and term number, a log entry that is simply on the
majority of the nodes can still be overwritten when a new leader is
elected.  The paper bolds the actual requirement:</p>

<p>A log entry may only be considered committed if the entry is stored on
a majority of the servers; in addition, at least one entry from the
leader’s current term must also be stored on a majority of servers.</p>

<p>Figure 7 shows how this can happen.  Consider this case with five
servers, S1, S2, S3, S4, S5.  We use the notation Value:Term in each
slot.  S1 is leader in term 2, and replicates an entry X for slot 2,
it goes to 2 servers (S1, S2).</p>

<pre>     
slot    1     2
S1     1:1  X:2
S2     1:1  X:2
S3     1:1
S4     1:1
S5     1:1
</pre>


<p>S1 crashes before it can finish replicating X:2, S5 is elected leader
for term 3 (with votes from S3, S4, and S5) and stores Y in slot 2</p>

<pre>
slot    1     2
S1     1:1  X:2
S2     1:1  X:2
S3     1:1
S4     1:1
S5     1:1  Y:3
</pre>


<p>S5 crashes before it can replicate Y, and S1 is elected leader again,
now for term 4.  S1 finishes replicating X:2, and adds a new entry, Z
in slot 3, but crashes before it can completely replicate it</p>

<pre>
slot    1     2     3
S1     1:1  X:2   Z:4
S2     1:1  X:2  
S3     1:1  X:2
S4     1:1
S5     1:1  Y:3
</pre>


<p>S5 becomes leader for term 5 because it has the most recent term out
of the remaining nodes.  S5 will force a Y into slot 2 (Note that S5
has Y in slot 2 and S1,S2,S3 had an X in slot 2.)</p>

<pre>
slot    1     2     3
S1     1:1  X:2   Z:4
S2     1:1  Y:3 
S3     1:1  Y:3
S4     1:1
S5     1:1  Y:3
</pre>


<p>Note that if S1 had gotten Z into two additional logs (two out of S2,
S3, S4), then S5 could never have been elected leader, because its log
was not long enough.</p>

<p>We found this surprising.</p>

<h2>Conclusions</h2>

<p>There are some nice aspects of Raft &mdash; the log as a first order
primitive, the method of configuration changes, and only two RPCs
(though one could argue AppendEntry is heavily overloaded).
Unfortunately, the authors’ main point falls flat for two reasons:
First, they primarily compare their system to Paxos while their system
is much closer to Viewstamped Replication. They claim this is
reasonable because Paxos is much more widely used. Second, they claim
Raft is much simpler and easier to implement (compared to Paxos).
However, in our reading, Raft didn’t appear to be any less subtle (it
took effort to convince ourselves of the correctness of the example
explained here) and several things that make Paxos difficult to
implement are still evidenced in Raft.</p>

<p>We don’t see many new consensus algorithms, so this was still very interesting to read and discuss.</p>

<h2>Open Questions</h2>

<p>Did they make any comparisons between Raft and Paxos that couldn’t have been made between Paxos and VR?</p>
]]></content>
  </entry>
  
</feed>
