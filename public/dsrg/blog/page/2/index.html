
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>DSRG</title>
  <meta name="author" content="DSRG">

  
  <meta name="description" content="Why did we read this paper? Though from 1993, in its time this paper sparked some controversy,
provoking an impassioned response. We wanted to &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://pdos.csail.mit.edu/dsrg/blog/page/2">
  <link href="/dsrg/favicon.png" rel="icon">
  <link href="/dsrg/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/dsrg/atom.xml" rel="alternate" title="DSRG" type="application/atom+xml">
  <script src="/dsrg/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/dsrg/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-968607-14']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/dsrg/">DSRG</a></h1>
  
    <h2>Distributed systems reading group at MIT</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/dsrg/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:pdos.csail.mit.edu/dsrg" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/dsrg/about">About</a></li>
  <li><a href="/dsrg/">Blog</a></li>
  <li><a href="/dsrg/schedule">Schedule</a></li>
  <li><a href="/dsrg/papers">Papers</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/dsrg/blog/2013/06/13/cheriton-and-skeen/">Cheriton and Skeen</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-13T16:21:00-04:00" pubdate data-updated="true">Jun 13<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Why did we read this paper?</h2>

<p>Though from 1993, in its time <a href="http://cs3.ist.unomaha.edu/~stanw/papers/93-catocs.pdf">this paper</a>
 sparked some controversy,
provoking an impassioned <a href="http://www.csie.fju.edu.tw/~yeh/research/papers/os-reading-list/birman93response-to-cheriton.pdf">response</a>.
  We wanted to understand the debate
about the question of providing ordering guarantees as part of the network.</p>

<h2>What is CATOCS?</h2>

<p>CATOCS stands for &ldquo;causally and totally ordered communication.&rdquo;  It means that
messages are delivered in the order they are sent, as specified by a
<em>happens-before</em> relationship.  A synonym for happens-before is
<em>causally-precedes</em>.  The following is the definition of happens-before:</p>

<p><em>Happens-before</em>: m1 happens before m2 if there exists a P such that
m1 is sent or received at P before P sends m2.</p>

<p>Under causal ordering, concurrent writes may be seen in different
orders at different participants.</p>

<p><em>Total ordering</em> is a stronger property; it ensures that messages are
delivered to all participants in the same order.</p>

<h2>What is wrong with CATOCS?</h2>

<p>Cheriton and Skeen make the point that ensuring CATOCS in the network
is prohibitively expensive, and since most applications need something
stronger than CATOCS anyway (such as transactional consistency), there
is no point in doing so.  They claim that CATOCS violates the
<a href="http://en.wikipedia.org/wiki/End-to-end_principle">End To End principle</a>,
which states that application-specific functionality should reside at
the end nodes of a network, instead of the intermediary nodes.  It is
worth noting that this principle is frequently misapplied.</p>

<p>They identify the following limitations in CATOCS systems:</p>

<ul>
<li><p>Can&rsquo;t say &ldquo;for sure&rdquo;</p>

<p>There are almost always hidden channels in a group of nodes, or
methods of communication not captured by the network.  For example,
processes might all write to a shared database, and writes seen at
that database might not preserve CATOCS.  Similarly, threads on a
single machine might share memory.</p>

<p>They use a contrived example of an independent &ldquo;FIRE&rdquo; message
appearing before an unrelated &ldquo;FIRE OUT&rdquo; message, and thus the
system might appear to not be in a &ldquo;FIRE&rdquo; state, because it
misapplied the unrelated &ldquo;FIRE OUT&rdquo;.</p></li>
<li><p>Can&rsquo;t say &ldquo;together&rdquo;</p>

<p>As stated above, applications often require transactional semantics.
CATOCS does not help with the serialization or atomicity between
<em>groups</em> of messages.  A system with this property obviates the need
for CATOCS.</p></li>
<li><p>Can&rsquo;t say &ldquo;whole story&rdquo;</p>

<p>Happens-before might not be enough.  Applications might require
linearizability or sequential consistency.</p></li>
<li><p>Can&rsquo;t say &ldquo;efficiently&rdquo;</p>

<p>They claim CATOCS protocols don&rsquo;t show any efficiency gains over
state-level techniques, and in fact are very inefficient.
Unfortunately the paper does not provide actuala measurements.</p>

<p>False causality could be an issue; happens-before enforces ordering
that the application might not care about.</p></li>
</ul>


<p>Cheriton and Skeen would prefer to see state-level and
application-specific ordering techniques.</p>

<h2>A Response</h2>

<p>Birman sees this paper as a critique of Isis, and claims that Cheriton
and Skeen misrepresented the true debate.  CATOCS should not be
considered in isolation, but when transactional semantics are
required, techniques like <em>virtual synchrony</em> should be used in
conjunction with CATOCS.</p>

<p>Birman makes the point that application developers should not even
need to consider their semantic ordering needs, instead the network
should provide guarantees for them, reducing user-visible design
complexity.</p>

<p>He also claims that their assumptions about overhead are completely off.</p>

<h2>Conclusion</h2>

<p>This seems founded in a more general debate &mdash; should systems developers aim
for efficiency and performance first, giving application developers
total control but leaving them to layer safety accordingly, or should they
apply an unknown cost to all users, making strong semantics an
indelible part of the system?</p>

<p>In the space of datastores, the former argument seems to have &ldquo;won&rdquo;.
Most application developers do not run their databases with
serializability or even other forms of slightly weaker consistency.
There is a move towards general key/value stores which do not provide
transactions or any ordering guarantees and might not necessarily pay
the penalty of writing to disk for durability.  It seems as though
application developers have chosen performance over safety, and
developed techniques to accomodate inconsistencies on their own (one
of which might be simply ignoring them).</p>

<p>We found it extremely difficult to reason about these two papers
without looking at a real system with a concrete design.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/dsrg/blog/2013/06/06/pacifica/">PacificA</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-06-06T18:00:00-04:00" pubdate data-updated="true">Jun 6<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Why are we reading PacificA?</h2>

<p>In the <a href="http://research.microsoft.com/apps/mobile/Publication.aspx?id=66814">PacificA paper</a>,
the authors describe very clearly how to
properly implement a primary/backup replicated storage layer with
strong consistency.</p>

<p>At RICON East 2013, Kyle Kingsbury presented a performance evaluation
of several commercial datastores. This evaluation is specifically
geared toward assessing the correctness of distributed systems in the
face of network partitions. Kyle has turned his talk into a <a
href="http://aphyr.com/tags/jepsen">series of blog posts</a>
describing the not-so-encouraging results.</p>

<p>In previous weeks we have discussed consensus protocols like Paxos and
Raft that provide strong consistency to clients and are provably safe
in the event of a network partition. However, many commercial
datastores use some variation of primary/backup replication, and
according to the above mentioned evaluation, do not behave as one
would hope when the network misbehaves. So is there a (provably)
correct way to implement primary/backup replication? PacificA proposes
such an algorithm and presents an evaluation of a distributed log
built atop this protocol.</p>

<h2>The Protocol</h2>

<h3>Configuration and Data Management</h3>

<p>The PacificA protocol separates the responsibility for data storage
from the task of configuration management. From the perspective of the
servers participating in the primary/backup protocol, the
configuration of the storage cluster is handled by an authoritative
3rd party (a separate Paxos cluster in their implementation).</p>

<h3>Replicating Data</h3>

<p>The server designated as the current primary is responsible for
interacting with clients. It handles all reads (from its committed
state) and writes. When a write is requested, the primary assigns a
sequence number to the operation and sends a <code>prepare</code>
message all replicas. When a replica receives a <code>prepare</code>
message, it adds the operation to its log and acknowledges the
primary. When the primary receives an acknowledgement from all
replicas it commits the operation and responds to the client. The
commit point on the replicas is advanced in a subsequent message from
the primary. This protocol preserves the Commit Invariant, which
guarantees the following properties of the logs on the primary
<code>P</code> and replicas <code>R</code>:</p>

<p><code>commited<sub>R</sub> &sube; committed<sub>P</sub> &sube; prepared<sub>P,R</sub></code></p>

<p>Because of this invariant, during failover recovery it is possible for
a new primary (which is always a previous replica) to bring its commit
point up to (and possibly past) the commit point of the previous
primary while guaranteeing that the operations in its log are
consistent with the previous primary&rsquo;s state.</p>

<h3>Detecting Failures</h3>

<p>The PacificA protocol uses a lease mechanism to detect failures in the
system. The primary and replicas all track how long it has been since
communicating with the others (using normal message traffic or with
heartbeat messages injected during idle periods) and assume a failure
has occurred if no communication occurs within a given timeout
period. When a server realizes a failure, it stops processing messages
and requests a configuration change from the 3rd party configuration
manager. To prevent a situation where two servers believe they are the
primary, the timeout periods differ depending on the server&rsquo;s current
role. The primary will timeout if it has not heard from one of its
replicas in a &ldquo;lease period&rdquo; of <code>T<sub>L</sub></code> seconds,
whereas a replica will timeout if it has not heard from the primary in
a &ldquo;grace period&rdquo; of <code>T<sub>G</sub></code> seconds. If
<code>T<sub>L</sub> &lt; T<sub>G</sub></code> then the current primary
will always realize the failure first (if it is still alive) and stop
processing messages. It will also propose a new configuration first,
allowing it to continue as the primary, which may mitigate disruptions
to the service. Should the primary fail or become partitioned, the
grace period will expire and one of the replicas will request to be
the new primary.</p>

<p>The appeal of this approach is its implementation simplicity.
However, for a short period of time during our discussion we were
convinced that this scheme could result in a situation where two
servers believed they were the primary and could serve (possibly
stale) reads, violating the strong consistency guarantee. The scenario
involved a heartbeat <code>ACK</code> that was delayed by the network,
extending the primary&rsquo;s lease but not the replica&rsquo;s grace period. It
turns out we were wrong. A careful reading of the paper specifies that
the lease period is measured from the <em>sending time</em> of the last
acknowledged heartbeat. We had mistakenly used the reception time in
our example, and had convinced ourselves that the protocol was
flawed. While this is clearly our fault, it is a caution that even
simple schemes can be implemented incorrectly, and would probably work
under normal conditions before eventually causing big problems.</p>

<h3>Changing Configurations</h3>

<p>When a new primary is selected it must reconcile the state of the
system using its log. It does so by sending new prepare messages for
any operations that were prepared by the previous primary but not (to
its knowledge) committed. This will bring its commit point up to or
past that of the previous primary before responding to clients,
preserving strong consistency. In addition, the other replicas must
truncate their logs to purge any prepared operations that extend
beyond the log of the new primary.</p>

<p>When a new replica is brought online it must obtain the current state
from another server before it can participate in replication. If
starting cold, this could halt progress in the system for a long time,
so replicas are allowed to join as candidates (their acks are not
required for committing at the primary) while they catch up.</p>

<p>As far as configuration changes go, this strategy appears to be fairly
straightforward (especially after working through some of the
subtleties in Raft).</p>

<h2>Implementing a Distributed Log</h2>

<p>The paper shifts focus and includes an explanation of how to implement
three variations of a distributed log with features similar to
Bigtable. We find this portion of the paper to be somewhat superfluous
in the context of the primary/backup protocol.</p>

<h2>Evaluation</h2>

<p>The most interesting part of the evaluation is Figure 5. There are two
portions of this graph that sparked discussion, from time 60-90 and
160-300. The first occurs after the primary is killed and the second
occurs after a replica is added.</p>

<p>We were a bit shocked by the amount of time that the system was
unavailable during failover (time 60-90). Part of this downtime is due
to the reconciliation process of the new primary, but we suspect this
is relatively short. Certainly, part of the problem is failure
detection, which relies on timeouts. In this case, the lease period
was 10 seconds and the grace period was 15 seconds. This means a good
portion of that downtime is likely due to the replicas waiting to
timeout and propose a new configuration. It prompted one reviewer to
exclaim &ldquo;wow, timeouts are a real bummer&rdquo;.</p>

<p>While the timeout scheme is simple to implement, it also must be
tuned. You want the timeout to be long enough so that temporary
hiccups in the network do not cause undue reconfigurations, but short
enough so that a real failure is detected in a reasonable amount of
time. It seems like the type of tuning parameter that may be hard to
get right. It would have been nice to see an evaluation in which
various lease/grace periods were tried in a failure scenario.</p>

<p>During time 160-300 the server that was previously the primary (and
was killed) rejoins the group as a secondary. What surprised us was
the drop in client-perceived throughput by the addition of a replica,
and how long it took for the system to recover. This re-emphasizes the
need to set the timeouts correctly so that there is not a lot of churn
in the configuration. Even when there is no change of primary, a
reconfiguration causes a significant performance penalty (though it is
still available).</p>

<p>We were also curious about the read throughput in Figure 4. The total
throughput seems shockingly low, but we reason this is due to disk
throughput. The experiment performs random reads on a large dataset,
which is partially contained in memory, several checkpoint files, and
an on-disk image.</p>

<h2>Discussion</h2>

<p>We spent some time discussing when a primary/backup scheme might be
used instead of a consensus algorithm like Paxos. The authors provide
some pros and cons in the paper, most of which seem to be a wash
(serving reads from a single server, external configuration) or in
favor of Paxos (not bottlenecked by slowest server). The main reasons
for preferring primary backup are simplicity and availability.</p>

<p>The simplicity argument is easy to understand (Paxos can be confusing)
but only to a certain extent. Paxos has become more accessible over
time, with simpler explanations, available implementations, and
variations that make it more practical. In addition, if the
primary/backup protocol in this paper is implemented as suggested by
the authors, then Paxos should be used to implement the configuration
manager, requiring the need to implement Paxos anyway.</p>

<p>The authors make an availability argument (versus consensus) in favor
of their scheme. They claim, rightfully so, that their system can
survive <em>n-1</em> failures within a replica group and still make progress
(the remaining server is the primary with no backups). On the other
hand, a consensus algorithm cannot make progress when a majority of
the servers are unavailable. This is a valid argument if the service
values availability over durability; that is, it prefers to make
progress even if writes are only persisted to one server. If, however,
the service is deployed for durability (e.g. with a QoS that requires
writes are persisted to 3 servers) then a replica group of 5 servers
would have the same availability as a Paxos group of the same size.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/dsrg/blog/2013/05/30/spanner/">Spanner</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-30T18:05:00-04:00" pubdate data-updated="true">May 30<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>What is Spanner?</h2>

<p><a href="https://www.usenix.org/system/files/conference/osdi12/osdi12-final-16.pdf">Spanner</a>
 is a highly distributed, externally consistent database
developed by Google.  It provides replication and transactions over a
geographically distributed set of servers.  Spanner uses time bounds,
Paxos, and two-phase commit to ensure external consistency.</p>

<h2>Interesting Ideas</h2>

<p>Spanner uses clocks with bounded uncertainty to provide synchrony between
servers. It also shards an application&rsquo;s data to provide fine-grained load
distribution.</p>

<p>Spanner uses the TrueTime API (TT) to synchronize time between
servers. A call to <code>TT.now()</code> gives a range guaranteed to
contain the actual time.</p>

<p>Spanner has two kind of reads.  The first reads the most recent value
of a key or set of keys, called a read-only transaction.  The second
is a snapshot read which is executed at a specific timestamp in the
past.  Using a combination of Paxos leases and TrueTime guarantees to
agree on a timestamp, Spanner can execute snapshot reads and read-only
transactions without locks or two-phase commit.</p>

<p>Spanner peforms schema changes atomically without blocking, by picking
a future time for the change to occur. Other read and write operations
choose a timestamp so that at each replica, the operation performs
either before or after the schema change.</p>

<h2>Subtleties</h2>

<p>Since Spanner commits only when <code>TT.now.after(timestamp)</code>
is true, we are guaranteed that from now on <code>TT.now.latest()</code> will
always be larger than the commited timestamp on all servers.</p>

<p>Spanner very carefully chooses timestamps for RW transactions to ensure when
they are safely visible.  They call this the <em>commit-wait rule</em>.</p>

<h2>Questions</h2>

<ul>
<li><p><strong>Why is the write throughput so low?</strong> 4K ops/sec for 50 paxos
servers of one replica each (so not running Paxos), not waiting for
any other commit times, seems very low.</p></li>
<li><p><strong>Why is the throughput experiment in 5.1 CPU bound?</strong></p></li>
<li><p><strong>What happens if we use logical time(which preserves causality)
rather than the true time?</strong> Maybe external consistency breaks, but
the system is still sequentially consistent.</p></li>
</ul>


<p><a href="http://pdos.csail.mit.edu/6.824/notes/l07.txt">6.824 notes</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/dsrg/blog/2013/05/23/raft/">Raft</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-05-23T18:00:00-04:00" pubdate data-updated="true">May 23<span>rd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Disclaimer:  This paper is still under submission.</p>

<h2>Why did we read about Raft?</h2>

<p><a href="ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf">Raft</a>
 is a new consensus algorithm that is optimized for “ease of
implementation”.  Its main purpose is to present a protocol that is
more understandable than Paxos, which, for many practitioners, is
difficult to implement correctly.  Viewstamped Replication is more
similar to Raft, however it is far less popular than Paxos, so it is
unfortunately not focused on in the paper.</p>

<p>All of these consensus algorithms operate as long as a majority of
servers are functioning.  So you would require 2f+1 servers to
tolerate f failures.  Servers are assumed to fail by stopping, though
they might recover with state from stable storage (such as disk).  We
read this paper because there are very few consensus algorithms, and
something that is easier to understand than Paxos sounds great!  Raft
pros and cons</p>

<p>The paper designs and ensures invariants around the data structure
that Paxos, in practice, is used for &mdash; a replicated state machine
log.  This abstraction is nice, because it is easier to think about
operating on a sequential log and ensuring a small number of
properties rather than running multiple independent instances of
Paxos.  Raft assigns leaders on a per-term basis (i.e., epochs), and
terms are used as an implicit coordination mechanism.  It also
supports configuration changes (removing or adding nodes from the
system) while still serving requests.</p>

<p>The bulk of the paper is fairly digestible and it was nice that the
mechanisms to ensure safety were concentrated in a small handful of
subsections.  However distributed consensus is hard, so verifying
Raft’s safety still took effort.  Many members of the reading group
have a basic understanding of Paxos, so we didn’t necessarily feel
that understanding Raft’s correctness was easier than understanding
Paxos.</p>

<p>One of the group members noted that he wrote an implementation of Raft
from the paper and some basic tests “just worked”, which was
surprising.  However, note that a production implementation of Raft
would require many of the same things required to properly implement
and run Paxos (see Paxos Made Live).  Namely, you’d have to consider
and handle disk corruptions, safely implement snapshots, and use
something like leases so that reads don’t always require running a
round of the consensus algorithm.  These added features are necessary
in a practical system, and would complicate a Raft implementation as
much as a Paxos one.</p>

<p>In terms of performance, Raft can add log entries with f+1 roundtrips
total, whereas Paxos requires 2(f+1).  Paxos can be further optimized
by using leaders, and both systems can benefit from batching log
entries.  The space used for “understandability” experiments could
have been better used to show more performance numbers or more
explanation about correctness.</p>

<h2>Main Subtleties</h2>

<p>One of the most interesting parts of the algorithm is the commit point
for a log entry, or the point at which you can determine that even if
f servers fail, the log entry will be present.  It turns out that
because nodes perform leader election by consulting and sending only
their log length and term number, a log entry that is simply on the
majority of the nodes can still be overwritten when a new leader is
elected.  The paper bolds the actual requirement:</p>

<p>A log entry may only be considered committed if the entry is stored on
a majority of the servers; in addition, at least one entry from the
leader’s current term must also be stored on a majority of servers.</p>

<p>Figure 7 shows how this can happen.  Consider this case with five
servers, S1, S2, S3, S4, S5.  We use the notation Value:Term in each
slot.  S1 is leader in term 2, and replicates an entry X for slot 2,
it goes to 2 servers (S1, S2).</p>

<pre>     
slot    1     2
S1     1:1  X:2
S2     1:1  X:2
S3     1:1
S4     1:1
S5     1:1
</pre>


<p>S1 crashes before it can finish replicating X:2, S5 is elected leader
for term 3 (with votes from S3, S4, and S5) and stores Y in slot 2</p>

<pre>
slot    1     2
S1     1:1  X:2
S2     1:1  X:2
S3     1:1
S4     1:1
S5     1:1  Y:3
</pre>


<p>S5 crashes before it can replicate Y, and S1 is elected leader again,
now for term 4.  S1 finishes replicating X:2, and adds a new entry, Z
in slot 3, but crashes before it can completely replicate it</p>

<pre>
slot    1     2     3
S1     1:1  X:2   Z:4
S2     1:1  X:2  
S3     1:1  X:2
S4     1:1
S5     1:1  Y:3
</pre>


<p>S5 becomes leader for term 5 because it has the most recent term out
of the remaining nodes.  S5 will force a Y into slot 2 (Note that S5
has Y in slot 2 and S1,S2,S3 had an X in slot 2.)</p>

<pre>
slot    1     2     3
S1     1:1  X:2   Z:4
S2     1:1  Y:3 
S3     1:1  Y:3
S4     1:1
S5     1:1  Y:3
</pre>


<p>Note that if S1 had gotten Z into two additional logs (two out of S2,
S3, S4), then S5 could never have been elected leader, because its log
was not long enough.</p>

<p>We found this surprising.</p>

<h2>Conclusions</h2>

<p>There are some nice aspects of Raft &mdash; the log as a first order
primitive, the method of configuration changes, and only two RPCs
(though one could argue AppendEntry is heavily overloaded).
Unfortunately, the authors’ main point falls flat for two reasons:
First, they primarily compare their system to Paxos while their system
is much closer to Viewstamped Replication. They claim this is
reasonable because Paxos is much more widely used. Second, they claim
Raft is much simpler and easier to implement (compared to Paxos).
However, in our reading, Raft didn’t appear to be any less subtle (it
took effort to convince ourselves of the correctness of the example
explained here) and several things that make Paxos difficult to
implement are still evidenced in Raft.</p>

<p>We don’t see many new consensus algorithms, so this was still very interesting to read and discuss.</p>

<h2>Open Questions</h2>

<p>Did they make any comparisons between Raft and Paxos that couldn’t have been made between Paxos and VR?</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/dsrg/blog/archives">Blog Archives</a>
    
    <a class="next" href="/dsrg/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/dsrg/blog/2014/01/10/epaxos/">EPaxos</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/12/16/sinfonia/">Sinfonia</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/09/30/spark-streaming/">Discretized Streams</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/09/30/spanstore/">SPANStore</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/08/08/chain-replication/">Chain Replication</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/07/25/mdcc/">MDCC</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/07/18/coralcdn/">CoralCDN</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/07/11/zookeeper/">Zookeeper</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/07/03/thialfi/">Thialfi</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/06/20/vr-revisited/">VR Revisited</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/06/13/cheriton-and-skeen/">Cheriton and Skeen</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/06/06/pacifica/">PacificA</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/05/30/spanner/">Spanner</a>
      </li>
    
      <li class="post">
        <a href="/dsrg/blog/2013/05/23/raft/">Raft</a>
      </li>
    
  </ul>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - DSRG -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'dsrg';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
