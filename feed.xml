<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distributed Systems Reading Group</title>
    <description>DSRG is a Distributed Systems Reading Group at MIT. We meet once a week on the 9th floor of Stata to discuss distributed systems research papers, and cover papers from conferences like SOSP, OSDI, PODC, VLDB, and SIGMOD. We try to have a healthy mix of current systems papers and older seminal papers.
</description>
    <link>http://dsrg.pdos.csail.mit.edu/</link>
    <atom:link href="http://dsrg.pdos.csail.mit.edu/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 26 Jun 2016 12:15:01 -0400</pubDate>
    <lastBuildDate>Sun, 26 Jun 2016 12:15:01 -0400</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Scalability! But what is the COST?</title>
        <description>&lt;p&gt;After a long break, we made our first paper &lt;a href=&quot;http://www.frankmcsherry.org/assets/COST.pdf&quot;&gt;Scalability! But at what
COST?&lt;/a&gt; by Frank McSherry,
Michael Isard, and Derek Murray. It is fairly short paper published in
HotOS’15 which raises some interesting questions about distributed
systems research, and the focus on scalability as the holy grail of
performance.&lt;/p&gt;

&lt;p&gt;In this post, I’ll go over some of the paper’s main arguments, and then
give a summary of the reading group’s thoughts and questions about the
paper. We are hoping to get around to another paper in two weeks’ time,
but after that we might take a summer break and return in September.&lt;/p&gt;

&lt;h2 id=&quot;paper-summary&quot;&gt;Paper summary&lt;/h2&gt;

&lt;p&gt;The paper is, at its heart, a criticism of how the performance of
current research systems are evaluated. The authors focus on the field
of graph processing, but their arguments extend to most distributed
computation research where performance is a key factor. They observe
that most systems are currently evaluated in terms of their
&lt;em&gt;scalability&lt;/em&gt;, how their performance changes as more compute resources
are used, but that this metric is often both useless and misleading.&lt;/p&gt;

&lt;p&gt;Without going into too much detail (you can find that in the paper), the
crux of their argument is that if a system has significant, but easily
&lt;em&gt;parallelizeable overhead&lt;/em&gt;, the system will appear to scale well, even
if its absolute performance is quite poor. To demonstrate this point,
the authors write single-threaded implementations for algorithms that
are commonly used to evaluate state-of-the-art graph processing systems.
They then run these implementations on the same datasets as those used
to evaluate several well-known systems in the field (Spark, GraphLab,
GraphX, etc.), and compare the total runtime against the numbers
published for those other systems.&lt;/p&gt;

&lt;p&gt;In almost all cases, the single-threaded implementation outperforms all
the others, sometimes by an order of magnitude, despite the distributed
systems using 16-128 cores. Several hundred cores were generally needed
for state-of-the-art systems to rival the performance of their
single-threaded program. This threshold is what the authors refer to as
a system’s COST. Some systems &lt;em&gt;never&lt;/em&gt; got better than the authors’
implementations, giving them effectively infinite COST.&lt;/p&gt;

&lt;p&gt;It is clear that the authors are not trying to pick on these systems in
particular. Instead, they seek to highlight a shortcoming in how current
research into distributed systems is evaluated. It is not merely enough
for a system to be better than its predecessors — it needs to be
faster or “better” compared to some &lt;em&gt;sensible baseline&lt;/em&gt; that represents
what someone skilled in the field would come up with. For example,
Hilbert curves and Union-Find are examples of tricks that it seems
reasonable for a well-versed author to employ. Crucially, this is not
just about researchers reporting these kinds of results, but also about
reviewers demanding them. A system that is twice as fast as some
previous system, but slower than a much simpler system, should probably
not be accepted.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;This paper spawned a fair bit of discussion at our meeting, though we
were mostly in agreement with each other, and with the authors. Broadly
speaking, we talked about three main aspects of the paper: how did we
get here, will this trigger any changes, and what other fields are
affected? I’ll summarize each one below:&lt;/p&gt;

&lt;h3 id=&quot;how-did-we-get-here&quot;&gt;How did we get here?&lt;/h3&gt;

&lt;p&gt;As far as we understand it, this paper came about following a sense of
annoyance with state-of-the-art graph research — the graph most people
operate on just aren’t that large. As long as the data fits on a couple
of SSDs, single-machine, or even single-thread programs can be
sufficient. Crucially, the data doesn’t need to fit in RAM to outperform
a distributed system (which has to pay network communication cost
despite keeping all the data in memory).&lt;/p&gt;

&lt;p&gt;The results also follow from Amdahl’s law. The law states that, given
&lt;code&gt;s&lt;/code&gt; compute resources, and &lt;code&gt;p&lt;/code&gt; as the fraction of the computation that
is parallelizeable, the expected system speedup is:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;S_\text{latency}(s) = \frac{1}{(1 - p) + \frac{p}{s}}&lt;/script&gt; &lt;!-- __$_ --&gt;&lt;/p&gt;

&lt;p&gt;Given this, it should be clear that the way to achieve a higher speedup
as the number of compute resources increases is to increase the fraction
of the program that is parallelizeable. However, there are two ways of
doing this. You can either &lt;em&gt;change&lt;/em&gt; the program such that more of it is
parallelizeable, or you can &lt;em&gt;slow down&lt;/em&gt; or blow up the amount of
parallelizeable code, so that it accounts for a larger fraction. This
latter technique would “improve” the correlation between &lt;code&gt;s&lt;/code&gt; and
speed-up, but does not actually make the program any faster than the
original.&lt;/p&gt;

&lt;p&gt;Usually, it is pretty sensible to compare research systems to prior
systems. The transitive argument that “we’re good because they’re good
and we’re better” is generally sufficient. However, this is only true if
one of those prior systems have been shown to be good for the use-case
you are trying to solve. In graph processing, the stage was set by
Google using Map/Reduce to compute PageRank over the web graph (which is
arguably one of the few real, large graphs out there). Due to the
graph’s sheer size, distribution was necessary, but this also meant the
system was fairly slow if you didn’t have access to the same amount of
compute resources as Google does.&lt;/p&gt;

&lt;p&gt;Unfortunately, the systems that then followed all compared themselves to
PageRank on M/R, &lt;em&gt;even for smaller problems&lt;/em&gt;. The Twitter graph, or the
&lt;code&gt;uk-2007-05&lt;/code&gt; graph referred to in the paper, which have been used to
evaluate countless graph processing systems, simply do not require
distribution anywhere near the scale of the web graph. By sacrificing
scalability (i.e., distribution), &lt;em&gt;better algorithms&lt;/em&gt; can be used,
which can speed up the computation significantly, but the follow-up
systems did not compare themselves against that. And neither did the
systems that followed on from those.&lt;/p&gt;

&lt;h3 id=&quot;are-things-going-to-change&quot;&gt;Are things going to change?&lt;/h3&gt;

&lt;p&gt;One might argue that the reviewers of these papers should have caught
the “lie” — that they should have demanded to see &lt;em&gt;why&lt;/em&gt; the massive
scalability was necessary, and why a smaller, but “smarter” solution
wasn’t the right choice instead. We suspect that part of the reason they
didn’t is that it can be quite hard to judge just how large a problem
is. If a graph has 100 million edges, is that big? Does it fit in RAM?
Does it fit on a single disk? If it doesn’t fit in RAM, is the
computation going to be excruciatingly slow? The reviewers assumed that
the researchers were right about the problem necessitating the use of
many machines, possibly because they knew it was true for PageRank on
the web graph. And then the transitive argument was applied from there.&lt;/p&gt;

&lt;p&gt;The authors are trying to argue, among other things, that researchers
should think more carefully about the algorithm they use. In many cases,
that is much more important than whether you can use a given graph
processing framework. Restricting yourself to “think like a vertex” can
make you lose out on significant performance gains, which could in turn
mean you can make do with a single machine, rather than a
hundred-machine cluster.&lt;/p&gt;

&lt;p&gt;Despite this, the paper is not really targeted at end-users. It is
targeted at researchers and reviewers, and trying to make them apply
more rigorous standards to their work. Given that this might introduce
substantial work for researchers, we suspect that the process will need
to be reviewer driven. A researcher might be hesitant to invest lots of
time into writing a single-threaded implementation just for baseline
comparison, but if the reviewers start demanding it, they will be forced
to. At the very least, reviewers should require that the paper argues
why a simple, relatively naïve implementation is not good enough for the
problem at hand. One should not add a requirement to have access to a
huge compute cluster lightly, just because a few companies have access
to them!&lt;/p&gt;

&lt;p&gt;One good compromise might be for researchers to look at the algorithms
that exist, and visit the papers that introduced them. They will
generally give the &lt;em&gt;real&lt;/em&gt; complexity of the algorithm (i.e., not just
the big-O upper bound), which could be used as an interesting comparison
for the system’s scaling properties.&lt;/p&gt;

&lt;!--
 - It&#39;s probably hard to write a framework that has both high
   performance *and* is super scalable.
 - We&#39;d like a single number for performance + scalability to evaluate.
--&gt;

&lt;h3 id=&quot;where-else-does-this-problem-appear&quot;&gt;Where else does this problem appear?&lt;/h3&gt;

&lt;p&gt;The authors make their point in the context of graph processing systems,
but their COST metric also applies to a variety of other distributed
systems. One that immediately came to mind is distributed storage
systems such as GFS. It would be interesting to see what performance a
system that targeted GFS-like features (concurrent writers with
append-record semantics) on a single machine with many disks could
achieve, and how that translates into overhead incurred by the
distribution in, say, GFS. Many users, though obviously not the likes of
Google or Facebook, could probably get away with using such a system,
and might be able to reap significant benefits.&lt;/p&gt;

&lt;p&gt;There are also contexts in which we believe the argument does &lt;em&gt;not&lt;/em&gt;
apply. For example, for long-running, incremental-computation systems
(such as databases) that have request-side scalability (e.g., support
many concurrent readers), it is not clear that comparing to a system
that only supports one-request-at-the-time is an apples-to-apples
comparison.&lt;/p&gt;

&lt;p&gt;In systems with strict requirements for latency, it might also be
necessary to pay the COST in order to hit the latency target. It could
be that a single-threaded implementation simply cannot achieve the
required latency, whereas a scalable system can reach it with hundreds
or thousands of cores. The price may be steep (and knowing what it is is
important), but it might be one that you are required to pay.&lt;/p&gt;

&lt;!--
 - What about pre-processing? Can be expensive? Is it fair to only
   distribute the pre-processing? Probably not.
--&gt;
</description>
        <pubDate>Sun, 26 Jun 2016 12:14:00 -0400</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2016/06/26/scalability-cost/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2016/06/26/scalability-cost/</guid>
        
        
      </item>
    
      <item>
        <title>EPaxos</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://github.com/efficient/epaxos&quot;&gt;EPaxos&lt;/a&gt; is a leaderless Paxos
variant which tries to reduce latencies for a geo-distributed replica
group by enabling the client to use the replica with the lowest
round-trip latency as the operation leader, and optimistically skipping
a round of replica communication by inter-operation conflict detection&lt;/p&gt;

&lt;p&gt;Instead of explaining EPaxos in this post, I will go over an example of
how the recovery protocol works. This example confused me for a while
since it is not actually addressed in the SOSP’13 paper — the full
recovery protocol is in the &lt;a href=&quot;http://www.pdl.cmu.edu/PDL-FTP/associated/CMU-PDL-13-111.pdf&quot;&gt;tech
report&lt;/a&gt;.
Therefore, I recommend you read the recovery protocol in the tech report
before you start playing EPaxos with toy examples.&lt;/p&gt;

&lt;h2 id=&quot;failure-and-recovery-scenario&quot;&gt;Failure and recovery scenario&lt;/h2&gt;

&lt;p&gt;Consider a replica group of five nodes. The fast-path quorum size for
five replicas is &lt;script type=&quot;math/tex&quot;&gt;3 \left(F + \lfloor\frac{F + 1}{2}\rfloor\right)&lt;/script&gt;,
making the EPaxos fast-path quorum the same size as a simple majority.
Let us name the replicas like so:&lt;/p&gt;

&lt;figure class=&quot;code-highlight-figure&quot;&gt;&lt;div class=&quot;code-highlight&quot;&gt;&lt;pre class=&quot;code-highlight-pre&quot;&gt;&lt;div data-line=&quot;1&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;  A   B
&lt;/div&gt;&lt;/div&gt;&lt;div data-line=&quot;2&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;C   D   E&lt;/div&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;One situation that could be problematic occurs when a client
successfully completes one operation (which has side-effects i.e.
writes), then issues another one, but then some failure occurs before
the second operation completes and the recovery protocol incorrectly
orders the second operation before the first.&lt;/p&gt;

&lt;p&gt;Suppose the first operation uses A as leader with C and D making up the
other fast-path quorum members while the second operation uses B as
leader with D and E as quorum members. Furthermore, let’s say that A
fast-path commits operation 1, responding success to the client but
crashing right before it sends “commit” messages to C and D. Then, B
sends pre-accepts to D and E (without operation 1 in the deps because B
doesn’t know about operation 1) and also crashes. The system has
suffered two failures but must recover and continue operation since five
replicas can tolerate two failures in EPaxos. However, the remaining
replicas look like this (with subscripts representing which operation
the replica has pre-accepted in its log):&lt;/p&gt;

&lt;figure class=&quot;code-highlight-figure&quot;&gt;&lt;div class=&quot;code-highlight&quot;&gt;&lt;pre class=&quot;code-highlight-pre&quot;&gt;&lt;div data-line=&quot;1&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;  X   X
&lt;/div&gt;&lt;/div&gt;&lt;div data-line=&quot;2&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;C_1 D_1 E_2&lt;/div&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;At this point, each replica knows about only one of the two operations
but not whether it is committed, and both operations conflict with each
other (see the tech report section 6.2 number 6 for exactly what
“conflicts” means). But when C, D, or E initiates recovery for
operations 1 and 2, how does it know which operation, if any, was
fast-path committed and in which order?&lt;/p&gt;

&lt;p&gt;Turns out we can figure out this mess if we know the leaders for both
operations — this is exactly what EPaxos does in this situation. If
the leader of an operation, alpha, is in the fast-path quorum of a
different operation, beta, then beta could not have been fast-path
committed if the pre-accept messages for alpha do not contain beta in
deps.&lt;/p&gt;

&lt;p&gt;The recovery would proceed as follows. Suppose C initiates recovery for
operation 1. First, it asks the fast-path quorum what their logs
contain. C observes that at least &lt;script type=&quot;math/tex&quot;&gt;\lfloor\frac{F+1}{2}\rfloor&lt;/script&gt;
replicas (C and D) have pre-accepted an operation with the identical
default attributes, and then tries to convince other replicas to accept
the operation until at least &lt;script type=&quot;math/tex&quot;&gt;F + 1&lt;/script&gt; have accepted it.  When C asks E
to accept operation 1, E refuses since operation 1 conflicts with
operation 2 and E has already pre-accepted operation 2. Therefore, C
will defer the recovery of operation 1 (section 6.2, step 7-e) and
instead try to recover the operation it conflicts with, operation 2.&lt;/p&gt;

&lt;p&gt;For the recovery of operation 2, replicas C and D will not accept
operation 2 and will respond with the conflicting operation and the
identity of the leader for the conflicting operation. The recovering
replica will now observe that the leader for operation 1, replica A, is
in the fast-path quorum for operation 2, but that A clearly didn’t know
about operation 2; otherwise it would have listed operation 2 as a
dependency for operation 1 in the pre-accept messages.  Thus operation 2
could not have been fast-path accepted. Operation 2 would then be filled
with a no-op and the recovery of operation 1 would be resolved,
concluding that operation 1 was fast-path committed (note that it is
always safe to conclude that an operation was fast-path committed if
there are no conflicts, even if the operation in question wasn’t).&lt;/p&gt;
</description>
        <pubDate>Fri, 10 Jan 2014 13:04:00 -0500</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2014/01/10/epaxos/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2014/01/10/epaxos/</guid>
        
        
      </item>
    
      <item>
        <title>Sinfonia</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://www.cs.princeton.edu/courses/archive/fall08/cos597B/papers/sinfonia.pdf&quot;&gt;Sinfonia&lt;/a&gt;
is a service that allows hosts to share application data in a
fault-tolerant, scalable, and consistent manner using a novel
mini-transaction primitive. We read this paper because it provides an
interesting alternative to message passing for building distributed
systems.&lt;/p&gt;

&lt;h2 id=&quot;data-layout&quot;&gt;Data Layout&lt;/h2&gt;

&lt;p&gt;At a high level, Sinfonia provides multiple independent linear address
spaces that live on &lt;em&gt;memory nodes&lt;/em&gt;. No structure is imposed on these
address spaces, and they can contain arbitrary bytes of data. Data in
Sinfonia is referenced using a pair: &lt;code&gt;(memory-node-id, offset)&lt;/code&gt;.
Sinfonia does not perform automatic load balancing, and data placement
is left to the application.&lt;/p&gt;

&lt;h2 id=&quot;mini-transactions&quot;&gt;Mini-transactions&lt;/h2&gt;

&lt;p&gt;Operations on data take the form of &lt;em&gt;mini-transactions&lt;/em&gt;, which are
basically distributed compare and swap/read operations. A
mini-transaction consists of a triple &lt;code&gt;(compare-set, write-set,
read-set)&lt;/code&gt;. Elements in the compare-set are tuples: &lt;code&gt;(memory-node-id,
offset, length, data)&lt;/code&gt; where the first three fields describe the
location and size of the data to compare, and the last field is the
value expected at that location. The write-set is the same, except that
the last field is the value to write to the location. Elements in the
read-set are similar, but they omit the data field.&lt;/p&gt;

&lt;p&gt;The mini-transaction performs the following operation atomically:&lt;/p&gt;

&lt;figure class=&quot;code-highlight-figure&quot;&gt;&lt;div class=&quot;code-highlight&quot;&gt;&lt;pre class=&quot;code-highlight-pre&quot;&gt;&lt;div data-line=&quot;1&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compare&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comparisons&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expected&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;&amp;#x7b;&lt;/span&gt;
&lt;/div&gt;&lt;/div&gt;&lt;div data-line=&quot;2&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;  &lt;span class=&quot;n&quot;&gt;Perform&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;
&lt;/div&gt;&lt;/div&gt;&lt;div data-line=&quot;3&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;  &lt;span class=&quot;n&quot;&gt;Return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;
&lt;/div&gt;&lt;/div&gt;&lt;div data-line=&quot;4&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;&lt;span class=&quot;p&quot;&gt;&amp;#x7d;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;For example, an atomic compare and swap operation on the first byte at
memory node 0 (where the expected value is “5” and the new value is “6”)
would be:&lt;/p&gt;

&lt;figure class=&quot;code-highlight-figure&quot;&gt;&lt;div class=&quot;code-highlight&quot;&gt;&lt;pre class=&quot;code-highlight-pre&quot;&gt;&lt;div data-line=&quot;1&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
&lt;/div&gt;&lt;/div&gt;&lt;div data-line=&quot;2&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;    &lt;span class=&quot;p&quot;&gt;&amp;#x7b;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;&amp;#x7d;,&lt;/span&gt;
&lt;/div&gt;&lt;/div&gt;&lt;div data-line=&quot;3&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;    &lt;span class=&quot;p&quot;&gt;&amp;#x7b;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;&amp;#x7d;,&lt;/span&gt;
&lt;/div&gt;&lt;/div&gt;&lt;div data-line=&quot;4&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;    &lt;span class=&quot;p&quot;&gt;&amp;#x7b;&amp;#x7d;&lt;/span&gt;
&lt;/div&gt;&lt;/div&gt;&lt;div data-line=&quot;5&quot; class=&quot;code-highlight-row numbered&quot;&gt;&lt;div class=&quot;code-highlight-line&quot;&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;Mini-transactions are committed using a 2-phase commit protocol where
application nodes are coordinators and memory nodes are participants,
but mini-transactions operating on a single participant use only a
single phase. This allows for operations that perform writes on one
memory node as a result of a compare on another memory node. The paper
gives several examples of powerful operations that can be implemented
using mini-transactions, including: atomic reads across multiple memory
nodes, compare and swap, acquiring multiple leases atomically, and
changing data if a lease is held.&lt;/p&gt;

&lt;p&gt;Mini-transactions were designed so that the operation itself can
piggyback on the commit protocol for added performance.  This does not
work for arbitrary transactions, but the restricted operations available
for mini-transactions allows this optimization.  For example, a
participant can vote “no” to commit a transaction if it knows that the
coordinator will abort the transaction as a result of a value it is
reading (this occurs when a comparison in the compare-set fails).
Additionally, the results for reads in mini-transactions are included
with the vote for committing in the first phase.&lt;/p&gt;

&lt;p&gt;The design of mini-transactions permits this kind of optimization, but
it makes certain operations impossible to fit in a single
mini-transaction. For example, reading and copying data between memory
nodes requires two mini-transactions (one to read the data and another
that atomically checks that the data is still valid and writes the data
to another memory node).&lt;/p&gt;

&lt;h2 id=&quot;design&quot;&gt;Design&lt;/h2&gt;

&lt;p&gt;Overall, Sinfonia provides an alternative to message passing for
implementing distributed systems. Rather than explicitly sending
messages, applications describe operations on shared data in terms of
mini-transactions that are executed atomically using 2-phase commit.&lt;/p&gt;

&lt;p&gt;Sinfonia uses logging and replication to provide fault tolerance and
reduce downtime. The design of Sinfonia, however, does not support
automatic load-balancing or caching. Load-balancing and caching are left
to the application developer who is given some load information by the
system.&lt;/p&gt;

&lt;p&gt;Coordinator crashes are handled using a recovery coordinator, which is
triggered when a transaction has not been committed or aborted after
some timeout. The recovery coordinator asks each participant how it
voted on that transaction, and the participant replies with its original
vote if it had already voted or &lt;code&gt;ABORT&lt;/code&gt; if it had not (remembering that
it must abort this transaction in the future if the original coordinator
reappears).&lt;/p&gt;

&lt;p&gt;Sinfonia uses a write-ahead redo-log for performance and fault-tolerance
for the participants.  When a participant votes to commit, the
transaction data is added to the redo-log which is replayed if the
participant crashes. Participants also keep track of decided
transactions so they know if they can commit the changes or not.  Other
participants must be contacted for their votes if the recovered
participant does not know how the transaction was decided.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;Sinfonia provides an interesting alternative to message passing in
distributed systems, but requires application developer intervention to
provide data locality, caching, and data placement. The paper details
several optimizations, which are especially important for
mini-transaction performance. The paper also includes interesting
applications built on top of Sinfonia: a file system and a group
communication service, which show how Sinfonia can be used to implement
real-world applications.&lt;/p&gt;

&lt;p&gt;Mini-transactions are somewhat limited in what operations they can
perform, but the operations they do support can be efficiently executed.&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Dec 2013 15:52:00 -0500</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2013/12/16/sinfonia/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2013/12/16/sinfonia/</guid>
        
        
      </item>
    
      <item>
        <title>Discretized Streams</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://sigops.org/sosp/sosp13/papers/p423-zaharia.pdf&quot;&gt;Discretized Streams: Fault Tolerant Computing at
Scale&lt;/a&gt; describes
additions to the
&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;Spark&lt;/a&gt;
system to handle streaming data. Compared to other streaming systems,
Spark Streaming offers a more robust fault recovery and straggler
handling strategies using the Resilient Distributed Dataset (RDD) memory
abstraction. In addition to allowing parallel recovery, Spark Streaming
is one of the first systems which can incorporate batch and interactive
query models all within the same system.&lt;/p&gt;

&lt;h2 id=&quot;what-are-rdds&quot;&gt;What are RDDs?&lt;/h2&gt;

&lt;p&gt;RDDs are a memory abstraction model described in the original
&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;Spark&lt;/a&gt;
paper.  They are immutable and only allow a specified set of
functional-like transformations to be operated on them. However, these
seemingly constraining properties allow the computation done on RDDs to
be completely deterministic, and RDDs can be computed in parallel
without having to worry about synchronization. Another really
interesting aspect of RDDs are their resilience to faults. By tracking
the lineage of transformations done on RDDs, we can reconstruct any lost
RDDs by simply retracing the lineage and recomputing from the source
RDDs.&lt;/p&gt;

&lt;h2 id=&quot;stream-discretization&quot;&gt;Stream Discretization&lt;/h2&gt;

&lt;p&gt;Spark Streaming is different from other streaming systems in that it
discretizes streaming input using a sliding window. The discretized
input are turned into RDDs and Spark Streaming can then perform small
batch operations on them as it would have in general batched mode. By
converting the input streaming data into batched RDDs, Spark Streaming
can easily intermix between the streaming model and the general batched
model because it operates over the same RDD abstraction.&lt;/p&gt;

&lt;h2 id=&quot;tracking&quot;&gt;Tracking&lt;/h2&gt;

&lt;p&gt;Typically, streaming systems employ a constant operator model in which
several constantly running workers wait for streaming data, operate on
them, and output the data. Spark Streaming differs from this model in
that it expresses all the operations done on the data through the RDDs.
The worker nodes maintain no state of their own. If state is needed to
operate on the data, Spark Streaming employs a special track operation,
which provides access to a key-value store (structured as an RDD). The
state for any data can be stored as a value in the key-value store and
can be accessed again using the same key.&lt;/p&gt;

&lt;h2 id=&quot;parallel-recovery&quot;&gt;Parallel Recovery&lt;/h2&gt;

&lt;p&gt;When a fault occurs, the Spark Streaming model simply recalculates the
RDDs that have been destroyed by retracing through the lineage of the
graph. However, because the RDDs are immutable and deterministic,
recomputation can be performed in parallel both in time and partition.
This allows Spark Streaming model to have fault-recovery times on the
order of seconds and tens of seconds, while existing systems take
minutes.&lt;/p&gt;

&lt;h2 id=&quot;stragglers&quot;&gt;Stragglers&lt;/h2&gt;

&lt;p&gt;Once again because RDDs are immutable and deterministic, Spark Streaming
can perform speculative replication to handle any stragglers. It is not
a problem if an RDD is computed twice because transformations on an RDD
is deterministic, so no synchronization needs to take place when
actually storing the RDD.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;Overall, using RDDs to operate on streaming data provides a nice clean
solution for fault-recovery and stragglers. Also, this idea of
discretizing streaming data could lead to interesting future work in the
area.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Sep 2013 16:00:00 -0400</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2013/09/30/spark-streaming/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2013/09/30/spark-streaming/</guid>
        
        
      </item>
    
      <item>
        <title>SPANStore</title>
        <description>&lt;p&gt;Several cloud providers provide storage in many data centers globally,
and customers can use simple PUTs and GETs to store and retrieve data
without dealing with the complexities of the storage infrastructure.
However, in reality, every storage system leaves replication across data
centers to the application, and although replication across all data
centers provides low latency, it is expensive.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://doi.acm.org/10.1145/2517349.2522730&quot;&gt;SPANStore: Cost-Effective Geo-Replicated Storage Spanning Multiple
Cloud Services&lt;/a&gt; is the first
system that tries to solve this automatically, by minimizing the cost
incurred by latency-sensitive application providers.&lt;/p&gt;

&lt;h2 id=&quot;what-is-spanstore&quot;&gt;What is SPANStore?&lt;/h2&gt;

&lt;p&gt;SPANStore is a key-value store that provides a unified view of storage
services present in several geographically distributed data centers. It
spans data centers across multiple cloud providers and determines where
to replicate every object and how to perform this replication. Finally,
it reduces cost by minimizing the computing resources necessary to offer
a global view of storage.&lt;/p&gt;

&lt;h3 id=&quot;multi-cloud&quot;&gt;Multi-Cloud&lt;/h3&gt;

&lt;p&gt;SPANStore uses multiple cloud providers to offer lower GET/PUT
latencies.  Also, this allows for lower cost by exploiting price
discrepancies across providers to meet latency SLOs.&lt;/p&gt;

&lt;h3 id=&quot;replication-policy&quot;&gt;Replication Policy&lt;/h3&gt;

&lt;p&gt;PMan determines the replication policies in SPANStore. It takes a
three-part specification as input:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;a characterization of SPANStore’s deployment&lt;/li&gt;
  &lt;li&gt;the application’s latency, fault tolerance, and consistency
requirements&lt;/li&gt;
  &lt;li&gt;a specification of the application’s workload as inputs&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;and in return, it provides the set of data centers that maintain copies
of all objects with that access set, and, at each data center in the
access set, which of these copies SPANStore should read from and write
to when an application VM issues a GET or PUT.&lt;/p&gt;

&lt;h3 id=&quot;eventual-consistency&quot;&gt;Eventual consistency&lt;/h3&gt;

&lt;p&gt;SPANStore can trade-off costs for storage, PUT/GET requests, and network
transfers if the application requires only eventual consistency.
SPANStore replicates objects at fewer data centers to reduce storage
costs and PUT request costs. PMan address this trade-off between
storage, networking, and PUT/GET request costs using a replication
policy as a mixed integer program.&lt;/p&gt;

&lt;h3 id=&quot;strong-consistency&quot;&gt;Strong consistency&lt;/h3&gt;

&lt;p&gt;They rely on quorum consistency for strong consistency. They use
asymmetric quorum sets and require an intersection of at least &lt;script type=&quot;math/tex&quot;&gt;2f +
1&lt;/script&gt; data centers with the PUT replica set of every other data center in
the access set.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;The main goal of this paper seems to be to minimize costs of deploying
an application by trading off replication, network costs, storage costs,
and latency. It seems that there is still a huge burden on developer to
provide the correct inputs to PMan so that PMan can provide the best
replication policy. This doesn’t seem to reduce the complexity involved.
A lot of the paper relies on the objective function, and there are not
many new distributed system concepts.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Sep 2013 16:00:00 -0400</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2013/09/30/spanstore/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2013/09/30/spanstore/</guid>
        
        
      </item>
    
      <item>
        <title>Chain Replication</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://db2.usenix.org/events/osdi04/tech/full_papers/renesse/renesse.pdf&quot;&gt;Chain
Replication&lt;/a&gt;
is a paper from ‘04 by Renesse and Schneider. The system is interesting,
because it is a primary-backup system with an unconventional
architecture that aimed to achieve high throughput and availability
while maintaining strong consistency.&lt;/p&gt;

&lt;h2 id=&quot;what-is-chain-replication&quot;&gt;What is Chain Replication&lt;/h2&gt;

&lt;p&gt;Chain replication is a special primary backup system, in which the
servers are linearly ordered to form a chain with the primary at the
end. In “classic” primary backup systems, the topology resembles a star
with the primary at the center.  The goal of this design is, in the
authors’ own words, “to achieve high throughput and availability without
sacrificing strong consistency”.&lt;/p&gt;

&lt;h2 id=&quot;the-chain-replication-protocol&quot;&gt;The Chain replication protocol&lt;/h2&gt;

&lt;p&gt;The chain replication protocol is quite simple: All read requests are
sent to the tail (primary) of the chain as in normal primary-backup
systems, all write requests are sent to the head (a backup), which then
passes the update along the chain. To avoid unnecessary work, only the
result of the write is passed down the chain. Strong consistency
naturally follows, because all requests are ordered by the primary at
the tail of the chain.&lt;/p&gt;

&lt;p&gt;The protocol considers only three failure cases, all of which are
fail-stop failures:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fail-stop of the head&lt;/li&gt;
  &lt;li&gt;Fail-stop of the tail (primary)&lt;/li&gt;
  &lt;li&gt;Fail-stop of a middle server&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Failures are detected by a Paxos-service. The simplest failure is
fail-stop of the head, where the next server in the chain takes over as
head. Failure of the tail is equally simple.&lt;/p&gt;

&lt;p&gt;To cope with a middle server failure, servers need to keep a history of
requests that they have processed. If a middle server fails, its two
neighbors bypass it and connect. The later server sends the other one
the list of requests it has seen, so that the other server can forward
any requests that were dropped when the middle server failed. To keep
the size of this history short, the tail of the chain acks requests and
the ack is passed up the chain.&lt;/p&gt;

&lt;p&gt;The protocol also allows for chains to be extended. This is done by
copying the state of the tail to a new server and then making the new
server the tail.&lt;/p&gt;

&lt;h2 id=&quot;performance-evaluation&quot;&gt;Performance evaluation&lt;/h2&gt;

&lt;p&gt;Renesse and Schneider compare their protocol with ordinary primary
backup. While they admit that detecting a failed server will take much
longer than fixing the failure once it’s detected, they discuss at
length how many message round-trips the recovery will take in both
systems. Which system performs better actually depends on the mix of
reads vs. writes, because while primary failure is easier to fix in
chain replication, backup failure is easier to fix under a classic
primary-backup scheme.&lt;/p&gt;

&lt;p&gt;A big part of the paper consists of evaluating the performance of chain
replication with different setups under multiple load scenarios. The
interesting take-away for systems with a single chain is that weak chain
replication (where one can read (possibly stale) data from any server in
the chain) may actually perform worse than strong chain replication with
a ratio of updates to writes as small as &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{10}&lt;/script&gt;. They explain
this with the fact that processing reads uses resources at the head that
could otherwise be used for updates.&lt;/p&gt;

&lt;p&gt;Renesse and Schneider also evaluate the performance of their system when
the data is spread in &lt;em&gt;volumes&lt;/em&gt; across multiple chains. The interesting
thing to note there is that read throughput is permanently lowered after
a failure, even when a new server is added to the system, because that
new server will become the tail of all the chains that the failed server
was involved in. Also interesting to note is that work throughput
increases temporarily after a failure, because the shorter length of
chains means that less work has to be done for each write until the
failed server is replaced.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;It was interesting to read a paper about a different kind of
primary-backup system, and the simplicity of chain-replication was quite
surprising. While the system is interesting from a conceptual point of
view, it doesn’t seem very suitable for any real-world scenarios.
Firstly, the system is designed to operate with equal latency between
all nodes and thus doesn’t scale beyond a LAN. Secondly, the authors
assume uniform popularity of stored items. The current system would
perform very poorly if a few items were the target of the majority of
the requests, because replication is not used to spread the load.&lt;/p&gt;

&lt;p&gt;The system still has some potential for improvement. As the experiments
clearly show, performance decreases with every failure. This could
easily be fixed by allowing for servers to be added not only as tail,
but also as head or middle server.&lt;/p&gt;
</description>
        <pubDate>Thu, 08 Aug 2013 15:00:00 -0400</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2013/08/08/chain-replication/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2013/08/08/chain-replication/</guid>
        
        
      </item>
    
      <item>
        <title>MDCC</title>
        <description>&lt;p&gt;There is a write performance trade off when consistently replicating
across multiple data centers due to the high latency when sending
messages between data centers (often &amp;gt; 100ms).  Existing protocols use
forms of two phase commit which incur 3 blocking round trips between
data centers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amplab.cs.berkeley.edu/wp-content/uploads/2013/03/mdcc-eurosys13.pdf&quot;&gt;MDCC&lt;/a&gt;
introduces a new cross data center transaction protocol for transactions
over key-value stores (no range scans) that uses many forms of Paxos to
achieve one synchronous cross-data center message in the common case.&lt;/p&gt;

&lt;p&gt;A similar system, Megastore, uses per-shard Paxos groups to replicate
between datacenters.  Unfortunately, heavy conflicts in Megastore reduce
performance to around four transactions per second.&lt;/p&gt;

&lt;h2 id=&quot;mdcc-goals&quot;&gt;MDCC Goals&lt;/h2&gt;

&lt;p&gt;MDCC aims to have the following properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Read-commited isolation&lt;/strong&gt;: Transactions never read records that
have not been fully committed.  The paper notes that this is the form
that most commercial and open source databases provide, so ensuring
this level of consistency is sufficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;No lost updates&lt;/strong&gt;: This is when transaction &lt;script type=&quot;math/tex&quot;&gt;T_1&lt;/script&gt; reads a record
&lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, another transaction &lt;script type=&quot;math/tex&quot;&gt;T_2&lt;/script&gt; commits an update to &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;, then
&lt;script type=&quot;math/tex&quot;&gt;T_1&lt;/script&gt; commits a write to &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; which overwrites &lt;script type=&quot;math/tex&quot;&gt;T_2&lt;/script&gt;’s commit
without having ever read it.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-stale reads&lt;/strong&gt;: Be able to ensure that reads are always of the
most recent value.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Low latency&lt;/strong&gt;: Minimize number of &lt;em&gt;synchronous&lt;/em&gt; round-trips — in
this case to one.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Costs scale to true transaction overlap&lt;/strong&gt;: There should not be
large, fixed costs incurred for each transaction commit.  Rather, the
costs should be proportional to the amount of actual contention
between transactions.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Integrity Constraints&lt;/strong&gt;: Support some forms of integrity
Constraints.  The paper discusses &lt;code&gt;FIELD &amp;gt;/=/&amp;lt; VALUE&lt;/code&gt; type
constraints.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;key-ideas&quot;&gt;Key Ideas&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Paxos per record
    &lt;ul&gt;
      &lt;li&gt;Awesome — could shuffle record ownership depending on access
patterns!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimistic concurrency control (keep write sets)&lt;/li&gt;
  &lt;li&gt;During Paxos, rather than propose the new value, propose an &lt;em&gt;option&lt;/em&gt;
to update the new value.  If the option succeeds, then a later write
can safely set the value asynchronously.&lt;/li&gt;
  &lt;li&gt;Move more logic into quorum nodes than in regular Paxos
    &lt;ul&gt;
      &lt;li&gt;Decide write-write conflicts&lt;/li&gt;
      &lt;li&gt;Enforce integrity constraints&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reduce round-trips via
    &lt;ul&gt;
      &lt;li&gt;Multi-Paxos: select master every &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; commits&lt;/li&gt;
      &lt;li&gt;Fast-Paxos: clients directly talk to quorum nodes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reduce conflicts
    &lt;ul&gt;
      &lt;li&gt;Generalized Paxos: commutative operations&lt;/li&gt;
      &lt;li&gt;“Demarcation protocol” conservatively figures out how many
conservative ops are allowed&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Failure
    &lt;ul&gt;
      &lt;li&gt;Send client state to every node on commit (every record’s quorum)
        &lt;ul&gt;
          &lt;li&gt;write set + xact id&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;questions&quot;&gt;Questions&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;How does a per-record protocol affect/enable fine-grained tuple
placement?&lt;/li&gt;
  &lt;li&gt;What to do to make repeatable reads work?&lt;/li&gt;
  &lt;li&gt;How does Fast-Paxos work?&lt;/li&gt;
  &lt;li&gt;How powerful/efficient is the demarcation protocol?&lt;/li&gt;
  &lt;li&gt;How do you run range-queries or queries that need indexes?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overview-of-mdcc&quot;&gt;Overview of MDCC.&lt;/h2&gt;

&lt;p&gt;The MDCC paper flows by identifying issues with the best solution
proposed so far, and introducing a new technique to resolve the issues.
This post will flow in a similar manner.&lt;/p&gt;

&lt;h3 id=&quot;per-record-paxos&quot;&gt;Per-record Paxos&lt;/h3&gt;

&lt;p&gt;Megastore is slow because it runs Paxos at a per shard granularity,
causing false conflicts.  What if we ran Paxos per record?&lt;/p&gt;

&lt;p&gt;Paxos on a record runs as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;up to 1 round-trip for client to tell master “I want to commit my
transaction”&lt;/li&gt;
  &lt;li&gt;1 round-trip to elect master for record&lt;/li&gt;
  &lt;li&gt;1 round-trip to set the value (learn the value)&lt;/li&gt;
  &lt;li&gt;1 round-trip to make value visible (can be asynchronous)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multi-record-transactions&quot;&gt;Multi-record Transactions&lt;/h3&gt;

&lt;p&gt;The key idea is to add logic into the acceptor nodes to perform
write-write detection.&lt;/p&gt;

&lt;p&gt;If I want to commit records A and B, I run Paxos for each record.  When
running Paxos, instead of setting the values in the second round, send
an &lt;em&gt;option&lt;/em&gt; containing the record’s read and write versions.  If there’s
no write conflict, the node returns OK, otherwise NO&lt;/p&gt;

&lt;p&gt;If I hear OK for every record, I tell each record’s quorums to write
“commit” into their option log.  Otherwise I write “abort”.  &lt;strong&gt;This is
different than SQL transaction aborts!&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Also, quorum nodes are only allowed to accept options if all previous
options have been decided (written “commit” or “abort”).  Otherwise
write-write conflicts can’t be correctly detected.&lt;/p&gt;

&lt;h3 id=&quot;supporting-failures&quot;&gt;Supporting Failures&lt;/h3&gt;

&lt;p&gt;The following sequence of actions could block all nodes participating in
a transaction.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I send options to acceptors for records A and B.&lt;/li&gt;
  &lt;li&gt;They accept and respond to me.  The options are undecided, so they
can’t accept anything else until I write a “commit” or “abort” in the
options.&lt;/li&gt;
  &lt;li&gt;I go on vacation before writing anything.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I was the only one that knew what other records were updated in my
transaction!  So the participating nodes can’t make anymore progress
until I’m back from vacation.&lt;/p&gt;

&lt;p&gt;The solution is to send my transaction state (transaction id and write
set) along with the options.  That way if I disappear, each record’s
quorum can make a clone of me using the transaction state.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;At this point, multi-record transactions using per-record Paxos
“works”.  Everything after this point is to make Paxos consensus run
faster&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;multi-paxos&quot;&gt;Multi-Paxos&lt;/h3&gt;

&lt;p&gt;We still require 3 round-trips to commit (send to master, elect master,
pick option).  We can use Paxos to elect a master for a block of rounds
which amortizes the cost of electing a master to 0 (assuming the masters
are stable).&lt;/p&gt;

&lt;p&gt;The communication is now&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;up to 1 round-trip for client to tell master “I want to commit my
transaction”&lt;/li&gt;
  &lt;li&gt;0 round-trips (amortized) to elect master for record&lt;/li&gt;
  &lt;li&gt;1 round-trip to send option to acceptors&lt;/li&gt;
  &lt;li&gt;1 asynchronous round-trip to write “commit”/”abort” on options&lt;/li&gt;
  &lt;li&gt;1 asynchronous round-trip (can be bundled with other messages) to make
commit visible&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-paxos&quot;&gt;Fast-Paxos&lt;/h3&gt;

&lt;p&gt;We still need two synchronous round-trips — one to contact the master
if it is in another datacenter, and one to send the options.  Can we
avoid the former message?&lt;/p&gt;

&lt;p&gt;Yes! Fast-Paxos lets clients bypass the master to directly send options
to the quorum nodes.  Recall that the master’s job is to serialize the
log entries.  If we want to de away with the master, Fast-Paxos needs a
larger quorum size (a fast quorum).  If a fast quorum responds OK, then
the client can consider the option committed.  Otherwise it falls back
to normal Paxos with some details.&lt;/p&gt;

&lt;p&gt;None of us had read the &lt;a href=&quot;http://research.microsoft.com/pubs/64624/tr-2005-112.pdf&quot;&gt;Fast-Paxos
paper&lt;/a&gt; so we
were ill-equipped to work through why this section of the paper works.
Read below for how we &lt;em&gt;think&lt;/em&gt; Fast-Paxos works based on first
principles.  Read the Fast-Paxos paper to actually understand how it
works.&lt;/p&gt;

&lt;h3 id=&quot;generalized-paxos&quot;&gt;Generalized-Paxos&lt;/h3&gt;

&lt;p&gt;If my operations are all commutative, I want to avoid the cost of
serialization.&lt;/p&gt;

&lt;p&gt;Normal Paxos only allows 1 value update per round.  Generalized Paxos
allows M as long as those values are commutative.  For example, if in
the same round David wants to increment record A by 1 and Jane wants to
increment record A by 1, then the value of A is 2 regardless of the log
order on any of the quorum nodes.&lt;/p&gt;

&lt;p&gt;What this means is the acceptors aren’t simply writing values anymore,
they also understand “delta operations”, how they commute, and how to
apply them.&lt;/p&gt;

&lt;h3 id=&quot;integrity-constrains&quot;&gt;Integrity constrains&lt;/h3&gt;

&lt;p&gt;If I want to support integrity constraints (e.g., &lt;code&gt;A &amp;gt;= 0&lt;/code&gt;), the
acceptors need to understand and enforce them.&lt;/p&gt;

&lt;p&gt;The paper restricts the domains of the delta operations (e.g., can only
add/subtract 1 or 2) and conservatively computes the maximum number of
operations that are allowed before it is at all possible (on a single
node and any combinations of transactions in a quorum) that the
constraint is violated.&lt;/p&gt;

&lt;p&gt;This is useful because many transactions are simply updating counters.&lt;/p&gt;

&lt;h3 id=&quot;thats-a-lot-of-steps-are-there-any-more-tricks&quot;&gt;That’s a lot of steps, are there any more tricks?&lt;/h3&gt;

&lt;p&gt;We usually want reads to go fast, but if I’m reading record A and A’s
replica in my datacenter was not in the quorum, then it would give me
stale data.  One way to get around this is to make sure the replica in
my data center is &lt;em&gt;always&lt;/em&gt; in the quorum.  This works if my data center
is the only one reading this record.  For example, if each data center
is collecting the local weather data and replicating the data, it can be
pretty sure that only people living around the data center will want the
local data.&lt;/p&gt;

&lt;p&gt;Fast-Paxos is really really bad (in terms of cross datacenter
round-trips) if a collision happens.  For those cases it makes more
sense to stick with a master and use Multi-Paxos.  Figuring out when to
use each is important.  The paper provides a heuristic that sticks with
Multi-Paxos and periodically tries to “upgrade” to Fast-Paxos.&lt;/p&gt;

&lt;p&gt;That’s it!  MDCC collects a number of Paxos variants and puts them
together into a nice cross data-center commit protocol.&lt;/p&gt;

&lt;h2 id=&quot;a-note-on-fast-paxos&quot;&gt;A Note on Fast-Paxos&lt;/h2&gt;

&lt;p&gt;The following was derived from first principles.&lt;/p&gt;

&lt;p&gt;The Fast-Paxos protocol was described (briefly) in the paper as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Any app-server can propose an option directly to the storage (quorum)
nodes, which in turn promise only to accept the first proposed option.
Simple majority quorums, however, are no longer sufficient to learn a
value and ensure safeness… If a proposer receives an acknowledgment
from a fast quorum, the value is safe and guaranteed to be committed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A classical Paxos quorum requires the property that for any two quorums,
&lt;script type=&quot;math/tex&quot;&gt;Qc_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Qc_2&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Qc_1 \cap Qc_2 \neq \emptyset&lt;/script&gt;

&lt;p&gt;For Fast-Paxos it has the requirement that for any two fast quorums,
&lt;script type=&quot;math/tex&quot;&gt;Qf_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Qf_2&lt;/script&gt;, and regular quorum &lt;script type=&quot;math/tex&quot;&gt;Qc&lt;/script&gt;,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Qf_1 \cap QF_2 \cap Qc \neq \emptyset&lt;/script&gt;

&lt;p&gt;One of our questions was why a simple majority, &lt;script type=&quot;math/tex&quot;&gt;Qc&lt;/script&gt;, is insufficient
and we need a &lt;script type=&quot;math/tex&quot;&gt;Qf \supset Qc&lt;/script&gt;. The following is an example of when it
fails.&lt;/p&gt;

&lt;p&gt;Safety, or correctness, in Paxos means that “At most one value can be
learned”.  One example is that even with &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; failures, a committed log
position stays committed.&lt;/p&gt;

&lt;p&gt;Lets say &lt;script type=&quot;math/tex&quot;&gt;Qf = Qc&lt;/script&gt; is a majority.  Consider the following logical
proposals and their quorums where each column is the state of an
acceptor (1-5), and each row is the state at one Paxos round.  C1, C2,
C3 are clients (proposers).  C1 commits A and manages to communicate
with acceptors 1-3, C2 commits B and so on.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    1 2 3 4 5
C1  A A A
C2      B B B
C3  C C   C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Say C1 and C2 happen concurrently, and C3 happens later (as a normal
Paxos round, or Fast-Paxos).  Then the actual acceptances and what round
each node thinks its in may be&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1 2 3 4 5 Round (as understood by local node)
 A A A B B 1
     B     2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now if C3 tries to set C, its quorum could be 1,2,4 which don’t know
that A won in round 1 and B “won” in round 2, because they have only
seen values for round 1, so the nodes happily accept C in (what they
think is) round 2.  This effectively overwrites B, violating safety.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1 2 3 4 5 Round (as understood by local node)
 A A A B B 1
 C C B C   2
&lt;/code&gt;&lt;/pre&gt;
</description>
        <pubDate>Thu, 25 Jul 2013 12:00:00 -0400</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2013/07/25/mdcc/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2013/07/25/mdcc/</guid>
        
        
      </item>
    
      <item>
        <title>CoralCDN</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://www.coralcdn.org/docs/coral-nsdi04.pdf&quot;&gt;CoralCDN&lt;/a&gt; is a system
that allows small websites or those with limited resources a method for
remaining available in the face of flash-crowds. The system is
interesting for a number of reasons: it is a live running system that
the public can use, it’s peer to peer, self organizing, and also has a
&lt;a href=&quot;http://www.coralcdn.org/docs/coral-nsdi10.pdf&quot;&gt;follow-up paper&lt;/a&gt; that
analyzes the system with five years of hindsight.&lt;/p&gt;

&lt;h2 id=&quot;what-is-coralcdn&quot;&gt;What is CoralCDN&lt;/h2&gt;

&lt;p&gt;CoralCDN is a content distribution network (CDN) built on top of the
Coral indexing service.  The system can be described in its component
parts: Coral DNS services, Coral HTTP proxy, and the Coral indexing
service.&lt;/p&gt;

&lt;h3 id=&quot;dns-functions&quot;&gt;DNS Functions&lt;/h3&gt;

&lt;p&gt;Many of CoralCDN’s processes attempt to preserve locality in order to
keep latency low. This starts with the DNS lookup by the end user’s
browser.  CoralCDN assumes that the resolver for the user’s browser is
close to the end user on the network. It then picks a Coral HTTP proxy
close to the source address of the DNS request.&lt;/p&gt;

&lt;p&gt;Specifically, CoralCDN keeps track of the latencies between the proxies
and the user and creates levels.  The Coral DNS server can get a round
trip time to the end user and only provide listings to proxies that fall
within a given level.&lt;/p&gt;

&lt;p&gt;The DNS servers are also responsible for ensuring that the HTTP proxies
that are returned to the end user are available, since a bad HTTP proxy
will fail the user’s request. The DNS servers can do this by
synchronously checking a proxy’s status with an RPC prior to releasing
the DNS response.&lt;/p&gt;

&lt;h3 id=&quot;the-http-proxy&quot;&gt;The HTTP Proxy&lt;/h3&gt;

&lt;p&gt;The driving goal of the system is to shield origin servers from request
stampedes. The Coral proxies therefore attempt to find the content
within CoralCDN before going to the origin server. Additionally, when a
proxy begins retrieval from the origin server, it inserts a short lived
record into the Coral index so any additional servers that want the
object will not simultaneously contact the origin.&lt;/p&gt;

&lt;h3 id=&quot;the-indexing-service&quot;&gt;The Indexing Service&lt;/h3&gt;

&lt;p&gt;One of the authors’ stated contributions of this work is the indexing
system, including a &lt;em&gt;distributed sloppy hash table&lt;/em&gt; (DSHT).  Each node
is given a 160 bit ID and holds key/value pairs with keys that are
“close” to the node’s ID.  Additionally, each node maintains a routing
table to other nodes. On a put or get, the request can be routed to
successively closer nodes to toward the closest node.&lt;/p&gt;

&lt;p&gt;The service is considered &lt;em&gt;sloppy&lt;/em&gt; because a key/value is not always
stored at the closest node. The put process is completed in two phases.
The first is a collection of hops to get to the closest node and forms a
stack from the list of nodes. This phase ends when the storing node
reaches the closest node or when it reaches one that is &lt;em&gt;full&lt;/em&gt; and
&lt;em&gt;loaded&lt;/em&gt;. A full node is one that already stores &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; values for that
key. A loaded node is one that is receiving more than &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; requests per
minute.  In the second phase, the storing node attempts to place the
value at the node on top of the stack. If that store fails for some
reason, the storer pops the stack and tries again.&lt;/p&gt;

&lt;p&gt;Nodes are arranged in clusters based on their average pairwise latency.
If that latency is below a certain threshold, the cluster is said to be
part of a specific level. A node can be part of multiple clusters but
uses the same ID in each. This clustering allows a sequential search for
a target value, beginning with low latency “close” nodes and only
proceeding to slower levels if needed.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;The idea of levels was interesting mostly because the latency thresholds
allow the clusters to be self-organizing. Clearly, locality is important
to a CDN.  It does seem however, that simple static organization of
nodes into clusters would be sufficient to allow latency locality.&lt;/p&gt;

&lt;p&gt;Though we didn’t technically read the follow-up paper, there is an
interesting section on lessons learned. In the section on usage, the
authors found that a small number of popular URLs account for a large
percentage of the requests but that these requests take up relatively
little room on the cache.  This means that more than 70% of requests are
served from local cache, with only 7.1% retrieved from other Coral
proxies.&lt;/p&gt;
</description>
        <pubDate>Thu, 18 Jul 2013 15:00:00 -0400</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2013/07/18/coralcdn/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2013/07/18/coralcdn/</guid>
        
        
      </item>
    
      <item>
        <title>Zookeeper</title>
        <description>&lt;p&gt;&lt;a href=&quot;http://static.usenix.org/event/usenix10/tech/full_papers/Hunt.pdf&quot;&gt;Zookeeper&lt;/a&gt;
is a practical system with replicated storage used by Yahoo!.  We are
interested in understanding what replication protocol Zookeeper uses,
why they needed a &lt;em&gt;new&lt;/em&gt; replication protocol, what applications/services
people build upon it, and what features are required to make such a
replicated storage system practical.&lt;/p&gt;

&lt;p&gt;Also, Zookeeper has pretty good performance for a replication protocol.
It is interesting to see how it works.&lt;/p&gt;

&lt;h2 id=&quot;what-is-zookeeper&quot;&gt;What is Zookeeper&lt;/h2&gt;

&lt;p&gt;The Zookeeper design consists of three components - replicated storage,
relaxed consistent caching at clients, and detection of client failures.&lt;/p&gt;

&lt;h3 id=&quot;replication&quot;&gt;Replication&lt;/h3&gt;

&lt;p&gt;Zookeeper provides a key/value data model, where keys are named like the
paths of a file system and values are arbitrary blobs. They call each
key/value pair a &lt;em&gt;Znode&lt;/em&gt;. Each Znode must be accessed with a full path
so that Zookeeper doesn’t have to implement open/close. Each value has a
version number and an internal sequence number, which they use a lot
when building applications/services presented in the paper. Each Znode
can have its own value, and a collection of children.&lt;/p&gt;

&lt;p&gt;Data can be accessed with get/set/create/delete methods. Zookeeper
supports conditional versions of these operations too. For example, a
client can say set the value of &lt;code&gt;/ds-reading/schedule&lt;/code&gt; to &lt;code&gt;3pm,thursday&lt;/code&gt;
only if the Znode’s current version is 100. This feature is used widely
in the presented applications/services.&lt;/p&gt;

&lt;p&gt;Znodes are replicated via what they called Zab, an atomic broadcast
protocol.  Zab is their own replication protocol. It seems quite similar
to viewstamped replication (VR). The only difference I can tell is that
Zab is special case of viewstamp-based replication. Zab requires clients
to send requests in order (thus they use TCP), but VR does not. Zab also
requires the requests to be idempotent, so that a new leader can
re-propose the most recent request without detecting duplicated
requests.&lt;/p&gt;

&lt;p&gt;Despite Zab’s restriction for replication of only idempotent operations,
Zookeeper does support non-idempotent operations. The trick is that for
each potentially non-idempotent request, the leader converts it to
idempotent requests by executing it locally.&lt;/p&gt;

&lt;h3 id=&quot;relaxed-consistent-caching&quot;&gt;Relaxed consistent caching&lt;/h3&gt;

&lt;p&gt;Another component of Zookeeper is relaxed consistent caching between
clients and Zookeeper. When clients get data from Zookeeper, clients can
cache it, and optionally register at Zookeeper to receive notifications
if the accessed Znode changes.  Zookeeper will send notifications to
caching clients &lt;em&gt;asynchronously&lt;/em&gt; once the data is changed. The benefit
of this is that updates don’t have to wait for invalidations to
complete, thus they don’t suffer from the impact of client failure; the
downside is that now the client’s cache is not consistent with
Zookeeper.&lt;/p&gt;

&lt;p&gt;The notification doesn’t contain the actual update, and each
registration is triggered only once (the server deletes it once the
notification is delivered).&lt;/p&gt;

&lt;h3 id=&quot;detection-of-client-failures&quot;&gt;Detection of client failures&lt;/h3&gt;

&lt;p&gt;Zookeeper supports a special Znode type called an “ephemeral Znode” to
detect the failure of clients.  If a session terminates, all ephemeral
ZNodes created within that session are deleted. Services/applications
can use it to detect failure. For example, Katta uses it to detect
master failure.&lt;/p&gt;

&lt;h2 id=&quot;other-design-choices&quot;&gt;Other design choices&lt;/h2&gt;

&lt;p&gt;Zookeeper allows applications and services to choose their own level of
consistency.  Zookeeper linearizes all writes. Reads are served from a
local Zookeeper server, but a client can linearize reads using the
&lt;code&gt;sync()&lt;/code&gt; API.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;In all, we feel that ZooKeeper is cool. It provides building blocks that
people can use to construct their own services with different
consistency/performance requirements. It also simplifies the building of
other services, as demonstrated in the paper.&lt;/p&gt;

&lt;p&gt;One thing we didn’t understand is why the paper makes the claim that
ZooKeeper is not intended for general storage.  It seems like that would
work.&lt;/p&gt;
</description>
        <pubDate>Thu, 11 Jul 2013 15:00:00 -0400</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2013/07/11/zookeeper/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2013/07/11/zookeeper/</guid>
        
        
      </item>
    
      <item>
        <title>Thialfi</title>
        <description>&lt;p&gt;Several of the papers we’ve read recently have focused on sophisticated,
generic fault tolerance abstractions based on complex protocols.
&lt;a href=&quot;http://www.cs.columbia.edu/~lierranli/coms6998-11Fall2012/papers/thia_sosp2011.pdf&quot;&gt;Thialfi&lt;/a&gt;
offers a contrast: its approach to fault tolerance is intentionally
simple, while at the same time being resilient to arbitrary (halting)
failure, including entire data centers.  Thialfi’s approach to fault
tolerance permeates the design of its abstraction, unlike Raft and VR,
which provide general-purpose state machine replication.&lt;/p&gt;

&lt;p&gt;Thialfi is also a real, massively deployed system, but is simple enough
to explain in more depth within the constraints of a conference paper
than most production systems.&lt;/p&gt;

&lt;h2 id=&quot;what-is-thialfi&quot;&gt;What is Thialfi?&lt;/h2&gt;

&lt;p&gt;The paper calls Thialfi a “notification service”, but really it’s an
object update signaling service.  When an application server updates an
object, it notifies Thialfi, and Thialfi notifies end-user clients on
the Internet that have registered for the object.  Critically, these
“notifications” contain no information about how the object
changed—the client has to query the application server to get its
updated state—which means Thialfi is free to combine notifications and
to generate spurious notifications, as long as clients interested in an
object eventually get at least one notification that the object has
changed if the client’s last seen version is not the object’s current
version.&lt;/p&gt;

&lt;p&gt;Under normal operation, Thialfi delivers timely update notifications to
connected clients, without any polling (clients must send periodic
heartbeats to maintain the connection, but these are infrequent, small,
and efficient to process).  When a client goes offline and later
returns, Thialfi remembers its previous object registrations and sends
only notifications for objects that changed while the client was
offline.  Similarly, the client only needs to resend its registrations
if they have changed since it was last online or if it was offline for
over two weeks (after which Thialfi garbage collects its state).&lt;/p&gt;

&lt;h2 id=&quot;designing-for-fault-tolerance&quot;&gt;Designing for fault tolerance&lt;/h2&gt;

&lt;p&gt;There are a few core ideas in Thialfi’s design that help it achieve
fault tolerance.  These are not necessarily unique to Thialfi, but they
work well in concert.&lt;/p&gt;

&lt;p&gt;Thialfi’s abstraction is carefully chosen to enable simple fault
tolerance.  Thialfi is &lt;em&gt;always&lt;/em&gt; allowed to respond to clients with “I
don’t know”; in the worst case, the client will fall back to polling the
application server.  This is a key choice because it means Thialfi is
free to drop any and all state, as long as it never incorrectly claims
to know the version of an object.  In fact, the initial design presented
by the paper (4.1) is entirely in-memory, yet can survive data center
failures.  At first, this may seem like an undesirable abstraction to
build a client application atop, but, in fact, clients already have to
deal with this when they first run.&lt;/p&gt;

&lt;p&gt;Since the only thing Thialfi can tell a client is “object X might have
changed”, it’s free to coalesce, repeat, and generate spurious
notifications.  The only thing it’s not allowed to do is drop a
notification entirely.  Since, faced with arbitrary faults, there’s no
way to know whether or not a notification was dropped, Thialfi
conservatively generates spurious notifications whenever a failure
&lt;em&gt;might&lt;/em&gt; have caused a notification to be dropped.&lt;/p&gt;

&lt;p&gt;Responsibility for hard state is colocated with the nodes that care
about that hard state.  A client is responsible for its object
registrations, because if the client dies, its registrations don’t
matter.  Likewise, an application server is responsible for application
data, because it has to persist that anyway (and if it dies, there’s
nothing to send notifications about).&lt;/p&gt;

&lt;p&gt;Regular paths and error handling paths are the same wherever possible.
This means they don’t have to distinguish errors from regular operation
and that the code is more likely to be correct (error handling code is
notoriously buggy largely because it doesn’t get exercised).  For
example, initial registration, modifications to registrations, handling
server loss of registration state, and re-registering after migration
are all handled in the same way: the client and server exchange digests
of what they think the registration state is in every heartbeat; if
these are out of sync, the client simply resends its entire set of
registrations.  This extends to the user of the client API as well: as
mentioned above, client cold-start and the loss of version state in
Thialfi are handled identically at an API level.&lt;/p&gt;

&lt;p&gt;Ultimately, Thialfi makes recovering from failure the responsibility of
application servers and clients, keeping itself off the critical path
for anything.  This seems simple, but achieving this without burdening
application servers and clients requires careful and conscious design.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;We felt there was one dark corner of Thialfi’s design that makes it
difficult to completely understand its fault tolerance properties.
Application servers post version changes to Thialfi via Google’s
reliable pub-sub system, about which the paper is devoid of details.
It’s difficult to tell how the pub-sub system could fail and how this
would affect Thialfi.  Furthermore, if Thialfi bootstraps off a reliable
pub-sub system, what would have happened if Google had simply exposed a
client API to subscribe to the pub-sub system?  Our best guess is that
it wouldn’t have scaled to millions of clients like Thialfi does, but we
can only guess.&lt;/p&gt;

&lt;p&gt;Thialfi’s approach of signaling rather than notification reprises the
long debate between “level-triggered” and “edge-triggered” interfaces in
OS and hardware design.  Level-triggered interfaces like &lt;code&gt;poll()&lt;/code&gt; and
PCI interrupts have very similar properties to Thialfi’s API (e.g., like
a Thialfi notification, &lt;code&gt;poll()&lt;/code&gt; only tells the caller that data is
available on an FD, not what the data is, or how many times the FD was
written to).  On the other hand, edge-triggered interfaces more closely
resemble the reliable pub-sub interface that Thialfi explicitly
rejected.  Historically, level-triggered interfaces have generally
scaled better, and we found it interesting to see this revisited and
reinforced from a very different perspective.&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jul 2013 16:00:00 -0400</pubDate>
        <link>http://dsrg.pdos.csail.mit.edu/2013/07/03/thialfi/</link>
        <guid isPermaLink="true">http://dsrg.pdos.csail.mit.edu/2013/07/03/thialfi/</guid>
        
        
      </item>
    
  </channel>
</rss>
