<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distributed Systems Reading Group</title>
    <description>DSRG is a Distributed Systems Reading Group at MIT. We meet once a week on the 9th floor of Stata to discuss distributed systems research papers, and cover papers from conferences like SOSP, OSDI, PODC, VLDB, and SIGMOD. We try to have a healthy mix of current systems papers and older seminal papers.
</description>
    <link>http://mit-pdos.github.io/dsrg/</link>
    <atom:link href="http://mit-pdos.github.io/dsrg/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 08 Jul 2015 11:48:44 -0400</pubDate>
    <lastBuildDate>Wed, 08 Jul 2015 11:48:44 -0400</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>EPaxos</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://github.com/efficient/epaxos&quot;&gt;EPaxos&lt;/a&gt;
is a leaderless Paxos variant which tries to reduce latencies for
a geo-distributed replica group by:
1. enabling the client to use the replica with the lowest round-trip latency as the operation leader
2. optimistically skipping a round of replica communication by inter-operation conflict detection&lt;/p&gt;

&lt;p&gt;Instead of explaining EPaxos in this post, I will go over an example of how the
recovery protocol works. This example confused me for a while since it is not
actually addressed in the SOSP’13 paper – the full recovery protocol is in the
&lt;a href=&quot;http://www.pdl.cmu.edu/PDL-FTP/associated/CMU-PDL-13-111.pdf&quot;&gt;tech report&lt;/a&gt;.
Therefore, I recommend you read the recovery protocol in the tech report before
you start playing EPaxos with toy examples.&lt;/p&gt;

&lt;h2 id=&quot;failure-and-recovery-scenario&quot;&gt;Failure and recovery scenario&lt;/h2&gt;
&lt;p&gt;Consider a replica group of five nodes. The fast-path quorum size for five
replicas is 3 (F + floor((F + 1)/2)), making the EPaxos fast-path quorum the
same size as a simple majority. Let us name the replicas like so:&lt;/p&gt;

&lt;pre&gt;
        A   B
      C   D   E
&lt;/pre&gt;

&lt;p&gt;One situation that could be problematic occurs when a client successfully
completes one operation (which has side-effects i.e. writes), then issues
another one, but then some failure occurs before the second operation completes
and the recovery protocol incorrectly orders the second operation before the
first.&lt;/p&gt;

&lt;p&gt;Suppose the first operation uses A as leader with C and D making up the other
fast-path quorum members while the second operation uses B as leader with D and
E as quorum members. Furthermore, let’s say that A fast-path commits operation
1, responding success to the client but crashing right before it sends “commit”
messages to C and D. Then, B sends pre-accepts to D and E (without operation 1
in the deps because B doesn’t know about operation 1) and also crashes. The
system has suffered two failures but must recover and continue operation since
five replicas can tolerate two failures in EPaxos. However, the remaining
replicas look like this (with subscripts representing which operation the
replica has pre-accepted in its log):&lt;/p&gt;

&lt;pre&gt;
        X   X
      C_1 D_1 E_2
&lt;/pre&gt;

&lt;p&gt;At this point, each replica knows about only one of the two operations but not
whether it is committed, and both operations conflict with each other (see the
tech report section 6.2 number 6 for exactly what “conflicts” means). But when
C, D, or E initiates recovery for operations 1 and 2, how does it know which
operation, if any, was fast-path committed and in which order?&lt;/p&gt;

&lt;p&gt;Turns out we can figure out this mess if we know the leaders for both
operations – this is exactly what EPaxos does in this situation. If the leader
of an operation, alpha, is in the fast-path quorum of a different operation,
beta, then beta could not have been fast-path committed if the pre-accept
messages for alpha do not contain beta in deps.&lt;/p&gt;

&lt;p&gt;The recovery would proceed as follows. Suppose C initiates recovery for
operation 1. First, it asks the fast-path quorum what their logs contain. C
observes that at least floor((F + 1)/2) replicas (C and D) have pre-accepted an
operation with the identical default attributes, and then tries to convince
other replicas to accept the operation until at least F + 1 have accepted it.
When C asks E to accept operation 1, E refuses since operation 1 conflicts with
operation 2 and E has already pre-accepted operation 2. Therefore, C will defer
the recovery of operation 1 (section 6.2, step 7-e) and instead try to recover
the operation it conflicts with, operation 2.&lt;/p&gt;

&lt;p&gt;For the recovery of operation 2, replicas C and D will not accept operation 2
and will respond with the conflicting operation and the identity of the leader
for the conflicting operation. The recovering replica will now observe that the
leader for operation 1, replica A, is in the fast-path quorum for operation 2,
but that A clearly didn’t know about operation 2; otherwise it would have
listed operation 2 as a dependency for operation 1 in the pre-accept messages.
Thus operation 2 could not have been fast-path accepted. Operation 2 would then
be filled with a no-op and the recovery of operation 1 would be resolved,
concluding that operation 1 was fast-path committed (note that it is always
safe to conclude that an operation was fast-path committed if there are no
conflicts, even if the operation in question wasn’t).&lt;/p&gt;
</description>
        <pubDate>Fri, 10 Jan 2014 13:04:00 -0500</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2014/01/10/epaxos.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2014/01/10/epaxos.html</guid>
        
        
      </item>
    
      <item>
        <title>Sinfonia</title>
        <description>&lt;h2 id=&quot;why-did-we-read-this-paper&quot;&gt;Why did we read this paper?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.princeton.edu/courses/archive/fall08/cos597B/papers/sinfonia.pdf&quot;&gt;Sinfonia&lt;/a&gt;
is a service that allows hosts to share application data in a
fault-tolerent, scalable, and consistent manner using a novel
minitransaction primitive. We read this paper because it provides
an interesting alternative to message passing for building distributed
systems.&lt;/p&gt;

&lt;h2 id=&quot;data-layout&quot;&gt;Data Layout&lt;/h2&gt;

&lt;p&gt;At a high level, Sinfonia provides multiple independent linear address spaces that live on “memory nodes.” No structure is imposed on these address spaces, and they can contain arbitrary bytes of data. Data in Sinfonia is referenced using a pair: (memory-node-id, offset). Sinfonia does not perform automatic load balancing, and data placement is left to the application.&lt;/p&gt;

&lt;h2 id=&quot;minitransactions&quot;&gt;Minitransactions&lt;/h2&gt;

&lt;p&gt;Operations on data take the form of “minitransactions,” which are
basically distributed compare and swap/read operations. A minitransaction
consists of a triple (compare-set, write-set, read-set). Elements in the compare-set are tuples: (memory-node-id, offset, length, data) where the first three fields describe the location and size of the data to compare, and the last field is the value expected at that location. The write-set is the same, except that the last field is the value to write to the location. Elements in the read-set are similar, but they omit the data field.&lt;/p&gt;

&lt;p&gt;The minitransaction performs the following operation atomically:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compare&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;comparisons&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;match&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expected&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Perform&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For example, an atomic compare and swap operation on the first byte at memory node 0 (where the expected value is “5” and the new value is “6”) would be:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Minitransactions are committed using a 2-phase commit protocol where
application nodes are coordinators and memory nodes are participants,
but minitransactions operating on a single participant use only a
single phase. This allows for operations that perform writes on one
memory node as a result of a compare on another memory node. The
paper gives several examples of powerful operations that can be
implemented using minitransactions, including: atomic reads across
multiple memory nodes, compare and swap, acquiring multiple leases
atomically, and changing data if a lease is held.&lt;/p&gt;

&lt;p&gt;Minitransactions were designed so that the operation itself can
piggyback on the commit protocol for added performance.  This does
not work for arbitrary transactions, but the restricted operations
available for minitransactions allows this optimization.  For
example, a participant can vote “no” to commit a transaction if it
knows that the coordinator will abort the transaction as a result
of a value it is reading (this occurs when a comparison in the
compare-set fails). Additionally, the results for reads in
minitransactions are included with the vote for committing in the
first phase.&lt;/p&gt;

&lt;p&gt;The design of minitransactions permits this kind of optimization,
but it makes certain operations impossible to fit in a single
minitransaction. For example, reading and copying data between
memory nodes requires two minitransactions (one to read the data
and another that atomically checks that the data is still valid and
writes the data to another memory node).&lt;/p&gt;

&lt;h2 id=&quot;design&quot;&gt;Design&lt;/h2&gt;

&lt;p&gt;Overall, Sinfonia provides an alternative to message passing for
implementing distributed systems. Rather than explicitly sending
messages, applications describe operations on shared data in terms
of minitransactions that are executed atomically using 2-phase
commit.&lt;/p&gt;

&lt;p&gt;Sinfonia uses logging and replication to provide fault tolerance
and reduce downtime. The design of Sinfonia, however, does not
support automatic load-balancing or caching. Load-balancing and
caching are left to the application developer who is given some
load information by the system.&lt;/p&gt;

&lt;p&gt;Coordinator crashes are handled using a recovery coordinator, which
is triggered when a transaction has not been committed or aborted
after some timeout. The recovery coordinator asks each participant
how it voted on that transaction, and the participant replies with
its original vote if it had already voted or “ABORT” if it had not
(remembering that it must abort this transaction in the future if
the original coordinator reappears).&lt;/p&gt;

&lt;p&gt;Sinfonia uses a write-ahead redo-log for performance and fault-tolerance
for the participants.  When a participant votes to commit, the
transaction data is added to the redo-log which is replayed if the
participant crashes. Participants also keep track of decided
transactions so they know if they can commit the changes or not.
Other participants must be contacted for their votes if the
recovered participant does not know how the transaction was
decided.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;Sinfonia provides an interesting alternative to message passing in
distributed systems, but requires application developer intervention
to provide data locality, caching, and data placement. The paper
details several optimizations, which are especially important for
minitransaction performance. The paper also includes interesting
applications built on top of Sinfonia: a file system and a group
communication service, which show how Sinfonia can be used to
implement real-world applications.&lt;/p&gt;

&lt;p&gt;Minitransactions are somewhat limited in what operations they can perform, but the 
operations they do support can be efficiently executed.&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Dec 2013 15:52:00 -0500</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2013/12/16/sinfonia.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2013/12/16/sinfonia.html</guid>
        
        
      </item>
    
      <item>
        <title>Discretized Streams</title>
        <description>&lt;h2 id=&quot;why-did-we-read-this-paper&quot;&gt;Why did we read this paper?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://sigops.org/sosp/sosp13/papers/p423-zaharia.pdf&quot;&gt;Discretized Streams: Fault Tolerant Computing at Scale&lt;/a&gt; 
describes additions to the
&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;Spark&lt;/a&gt; system to
handle streaming data. Compared to other streaming systems, Spark Streaming
offers a more robust fault recovery and straggler handling strategies using the Resilient
Distributed Dataset (RDD) memory abstraction. In addition to allowing parallel
recovery, Spark Streaming is one of the first systems which can
incorporate batch and interactive query models all within the same system.&lt;/p&gt;

&lt;h2 id=&quot;what-are-rdds&quot;&gt;What are RDDs?&lt;/h2&gt;

&lt;p&gt;RDDs are a memory abstraction model described in the original
&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;Spark&lt;/a&gt;
paper.  They are immutable and only allow a specified set of
functional-like transformations to be operated on them. However, these
seemingly constraining properties allow the computation done on RDDs
to be completely deterministic, and RDDs can be computed in parallel
without having to worry about synchronization. Another really
interesting aspect of RDDs are their resilience to faults. By tracking
the lineage of transformations done on RDDs, we can reconstruct any
lost RDDs by simply retracing the lineage and recomputing from the
source RDDs.&lt;/p&gt;

&lt;h2 id=&quot;stream-discretization&quot;&gt;Stream Discretization&lt;/h2&gt;

&lt;p&gt;Spark Streaming is different from other streaming systems in that it
discretizes streaming input using a sliding window. The discretized input are
turned into RDDs and Spark Streaming can then perform small batch operations on
them as it would have in general batched mode. By converting the input streaming
data into batched RDDs, Spark Streaming can easily intermix between the
streaming model and the general batched model because it operates over the same
RDD abstraction.&lt;/p&gt;

&lt;h2 id=&quot;tracking&quot;&gt;Tracking&lt;/h2&gt;

&lt;p&gt;Typically, streaming systems employ a constant operator model in which several
constantly running workers wait for streaming data, operate on them, and output
the data. Spark Streaming differs from this model in that it expresses all the
operations done on the data through the RDDs. The worker nodes maintain no
state of their own. If state is needed to operate on the data, Spark Streaming
employs a special track operation, which provides access to a key-value store
(structured as an RDD). The state for any data can be stored as a value in the
key-value store and can be accessed again using the same key.&lt;/p&gt;

&lt;h2 id=&quot;parallel-recovery&quot;&gt;Parallel Recovery&lt;/h2&gt;

&lt;p&gt;When a fault occurs, the Spark Streaming model simply recalculates the RDDs that
have been destroyed by retracing through the lineage of the graph. However,
because the RDDs are immutable and deterministic, recomputation can be performed
in parallel both in time and partition. This allows Spark Streaming model to
have fault-recovery times on the order of seconds and tens of seconds, while
existing systems take minutes.&lt;/p&gt;

&lt;h2 id=&quot;stragglers&quot;&gt;Stragglers&lt;/h2&gt;

&lt;p&gt;Once again because RDDs are immutable and deterministic, Spark Streaming can
perform speculative replication to handle any stragglers. It is not a problem if
an RDD is computed twice because transformations on an RDD is deterministic, so
no synchronization needs to take place when actually storing the RDD.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;Overall, using RDDs to operate on streaming data provides a nice clean solution
for fault-recovery and stragglers. Also, this idea of discretizing streaming
data could lead to interesting future work in the area.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Sep 2013 16:00:00 -0400</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2013/09/30/spark-streaming.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2013/09/30/spark-streaming.html</guid>
        
        
      </item>
    
      <item>
        <title>SPANStore</title>
        <description>&lt;h2 id=&quot;why-did-we-read-this-paper&quot;&gt;Why did we read this paper?&lt;/h2&gt;

&lt;p&gt;Several cloud providers provide
storage in many data centers globally, and customers can use simple PUTs and
GETs to store and retrieve data without dealing with the complexities of the
storage infrastructure. However, in reality, every storage system leaves
replication across data centers to the application, and although replication
across all data centers provides low latency, it is expensive.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://doi.acm.org/10.1145/2517349.2522730&quot;&gt;SPANStore: Cost-Effective Geo-Replicated Storage Spanning Multiple
 Cloud Services&lt;/a&gt; is the
 first system that tries to solve this automatically, by minimizing the cost incurred by
 latency-sensitive application providers.&lt;/p&gt;

&lt;h2 id=&quot;what-is-spanstore&quot;&gt;What is SPANStore?&lt;/h2&gt;

&lt;p&gt;SPANStore is a key-value store that provides a unified view of storage services
present in several geographically distributed data centers. It spans data
centers across multiple cloud providers and determines where to replicate every
object and how to perform this replication. Finally, it reduces cost by
minimizing the computing resources necessary to offer a global view of storage.&lt;/p&gt;

&lt;h2 id=&quot;multi-cloud&quot;&gt;Multi-Cloud&lt;/h2&gt;

&lt;p&gt;SPANStore uses multiple cloud providers to offer lower GET/PUT latencies.
Also, this allows for lower cost by exploiting price decrepancies across
providers to meet latency SLOs.&lt;/p&gt;

&lt;h2 id=&quot;replication-policy&quot;&gt;Replication Policy&lt;/h2&gt;

&lt;p&gt;PMan determines the replication policies in SPANStore. It requires 1) a
characterization of SPANStore’s deployment, 2) the application’s latency, fault
tolerance, and consistency requirements, and 3) a specification of the
application’s workload as inputs.&lt;/p&gt;

&lt;p&gt;As output, PMan specifies 1) the set of data centers that maintain copies of all
objects with that access set, and 2) at each data center in the access set,
which of these copies SPANStore should read from and write to when an
application VM issues a GET or PUT.&lt;/p&gt;

&lt;h2 id=&quot;eventual-consistency&quot;&gt;Eventual consistency&lt;/h2&gt;

&lt;p&gt;SPANStore can trade-off costs for storage, PUT/GET requests, and network
transfers if the application requires only eventual consistency. SPANStore
replicates objects at fewer data centers to reduce storage costs and PUT request
costs. PMan address this trade-off between storage, networking, and PUT/GET
request costs using a replication policy as a mixed integer program.&lt;/p&gt;

&lt;h2 id=&quot;strong-consistency&quot;&gt;Strong consistency&lt;/h2&gt;

&lt;p&gt;They rely on quorum consistency for strong consistency. They use asymmetric
quorum sets and require an intersection of at least 2f + 1 data centers with the
PUT replica set of every other data center in the access set.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;The main goal of this paper seems to be to minimize costs of deploying an application
by trading off replication, network costs, storage costs, and latency. It seems
that there is still a huge burden on developer to provide the correct inputs to
PMan so that PMan can provide the best replication policy. This doesn’t seem to
reduce the complexity involved. A lot of the paper relies on the objective
function, and there are not many new distributed system concepts.&lt;/p&gt;
</description>
        <pubDate>Mon, 30 Sep 2013 16:00:00 -0400</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2013/09/30/spanstore.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2013/09/30/spanstore.html</guid>
        
        
      </item>
    
      <item>
        <title>Chain Replication</title>
        <description>&lt;h2 id=&quot;why-did-we-read-this-paper&quot;&gt;Why did we read this paper?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://db2.usenix.org/events/osdi04/tech/full_papers/renesse/renesse.pdf&quot;&gt;Chain Replication&lt;/a&gt; is a paper from ‘04 by Renesse and Schneider. The system is interesting, because it is a primary-backup system with an unconventional architecture that aimed to achieve high throughput and availability while maintaining strong consistency.&lt;/p&gt;

&lt;h2 id=&quot;what-is-chain-replication&quot;&gt;What is Chain Replication&lt;/h2&gt;
&lt;p&gt;Chain replication is a special primary backup system, in which the servers are linearly ordered to form a chain with the primary at the end. In “classic” primary backup systems, the topology resembles a star with the primary at the center. 
 The goal of this design is, in the authors’ own words, ‘to achieve high throughput and availability without sacrificing strong consistency’. 
## The Chain replication protocol
The chain replication protocol is quite simple: All read requests are sent to the tail (primary) of the chain as in normal primary-backup systems, all write requests are sent to the head (a backup), which then passes the update along the chain. To avoid unneccessary work, only the result of the write is passed down the chain. Strong consistency naturally follows, because all requests are ordered by the primary at the tail of the chain.&lt;/p&gt;

&lt;p&gt;The protocol considers only three failure cases, all of which are fail-stop failures:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Fail-stop of the head&lt;/li&gt;
  &lt;li&gt;Fail-stop of the tail (primary)&lt;/li&gt;
  &lt;li&gt;Fail-stop of a middle server&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Failures are detected by a Paxos-service. The simplest failure is fail-stop of the head, where the next server in the chain takes over as head. Failure of the tail is equally simple.&lt;/p&gt;

&lt;p&gt;To cope with a middle server failure, servers need to keep a history of requests that they have processed. If a middle server fails, its two neighbors bypass it and connect. The later server sends the other one the list of requests it has seen, so that the other server can forward any requests that were dropped when the middle server failed. To keep the size of this history short, the tail of the chain acks requests and the ack is passed up the chain.&lt;/p&gt;

&lt;p&gt;The protocol also allows for chains to be extended. This is done by copying the state of the tail to a new server and then making the new server the tail.&lt;/p&gt;

&lt;h2 id=&quot;performance-evaluation&quot;&gt;Performance evaluation&lt;/h2&gt;

&lt;p&gt;Renesse and Schneider compare their protocol with ordinary primary backup. While they admit that detecting a failed server will take much longer than fixing the failure once it’s detected, they discuss at length how many message round-trips the recovery will take in both systems. Which system performs better actually depends on the mix of reads vs. writes, because while primary failure is easier to fix in chain replication, backup failure is easier to fix under a classic primary-backup scheme.&lt;/p&gt;

&lt;p&gt;A big part of the paper consists of evaluating the performance of chain replication with different setups under multiple load scenarios. The interesting take-away for systems with a single chain is that weak chain replication (where one can read (possibly stale) data from any server in the chain) may actually perform worse than strong chain replication with a ratio of updates to writes as small as 1/10. They explain this with the fact that processing reads uses resources at the head that could otherwise be used for updates.&lt;/p&gt;

&lt;p&gt;Renesse and Schneider also evaluate the performance of their system when the data is spread in &lt;em&gt;volumes&lt;/em&gt; across multiple chains. The interesting thing to note there is that read throughput is permanently lowered after a failure, even when a new server is added to the system, because that new server will become the tail of all the chains that the failed server was involved in. Also interesting to note is that work throughput increases temporarily after a failure, because the shorter length of chains means that less work has to be done for each write until the failed server is replaced.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;It was interesting to read a paper about a different kind of primary-backup system, and the simplicity of chain-replication was quite surprising. While the system is interesting from a conceputal point of view, it doesn’t seem very suitable for any real-world scenarios. Firstly, the system is designed to operate with equal latency between all nodes and thus doesn’t scale beyond a LAN. Secondly, the authors assume uniform popularity of stored items. The current system would perform very poorly if a few items were the target of the majority of the requests, because repliaction is not used to spread the load.&lt;/p&gt;

&lt;p&gt;The system still has some potential for improvement. As the experiments clearly show, performance decreases with every failure. This could easily be fixed by allowing for servers to be added not only as tail, but also as head or middle server.&lt;/p&gt;
</description>
        <pubDate>Thu, 08 Aug 2013 15:00:00 -0400</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2013/08/08/chain-replication.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2013/08/08/chain-replication.html</guid>
        
        
      </item>
    
      <item>
        <title>MDCC</title>
        <description>&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;h3 id=&quot;why-did-we-read-about-mdcc&quot;&gt;Why did we read about MDCC?&lt;/h3&gt;

&lt;p&gt;There is a write performance trade off when consistently replicating
across multiple data centers due to the high latency when sending
messages between data centers (often &amp;gt; 100ms).  Existing protocols
use forms of two phase commit which incur 3 blocking round trips
between data centers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amplab.cs.berkeley.edu/wp-content/uploads/2013/03/mdcc-eurosys13.pdf&quot;&gt;MDCC&lt;/a&gt; 
introduces a new cross data center transaction protocol for
transactions over key-value stores (no range scans) that uses many
forms of Paxos to achieve one synchronous cross-data center message
in the common case.&lt;/p&gt;

&lt;p&gt;A similar system, Megastore, uses per-shard Paxos groups to replicate
between datacenters.  Unfortunately, heavy conflicts in Megastore
reduce performance to around four transactions per second.&lt;/p&gt;

&lt;h3 id=&quot;mdcc-goals&quot;&gt;MDCC Goals&lt;/h3&gt;

&lt;p&gt;MDCC aims to have the following properties:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Read-commited isolation&lt;/strong&gt;: Transactions never read records
that have not been fully committed.  The paper notes that this is
the form that most commercial and open source databases provide,
so ensuring this level of consistency is sufficient.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;No lost updates&lt;/strong&gt;: This is when transaction T1 reads a record X,
another transaction T2 commits an update to X, then T1 commits a
write to X which overwrites T2’s commit without having ever read
it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Non-stale reads&lt;/strong&gt;: Be able to ensure that reads are always of the
most recent value.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Low latency&lt;/strong&gt;: Minimize number of &lt;em&gt;synchronous&lt;/em&gt; roundtrips – in
this case to one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Costs scale to true transaction overlap&lt;/strong&gt;: There should not be
large fixed costs incurred for each transaction commit.  Rather,
the costs should be proportional to the amount of actual contention
between transactions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Integrity Constraints&lt;/strong&gt;: Support some forms of integrity Constraints.
The paper discusses FIELD &amp;gt;/=/&amp;lt; VALUE type constraints.&lt;/p&gt;

&lt;h3 id=&quot;key-ideas&quot;&gt;Key Ideas&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Paxos per record
    &lt;ul&gt;
      &lt;li&gt;Awesome – could shuffle record ownership depending on access patterns!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimistic concurrency control (keep write sets)&lt;/li&gt;
  &lt;li&gt;During Paxos, rather than propose the new value, propose an &lt;em&gt;option&lt;/em&gt; to
update the new value.  If the option succeeds, then a later write can safely
set the value asynchronously.&lt;/li&gt;
  &lt;li&gt;Move more logic into quorum nodes than in regular Paxos
    &lt;ul&gt;
      &lt;li&gt;Decide write-write conflicts&lt;/li&gt;
      &lt;li&gt;Enforce integrity constraints&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reduce roundtrips via
    &lt;ul&gt;
      &lt;li&gt;multi-Paxos: select master every N commits&lt;/li&gt;
      &lt;li&gt;Fast Paxos: clients directly talk to quorum nodes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reduce conflicts
    &lt;ul&gt;
      &lt;li&gt;Generalized Paxos: commutative operations&lt;/li&gt;
      &lt;li&gt;“Demarcation protocol” conservatively figures out how many conservativy ops are allowed&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Failure
    &lt;ul&gt;
      &lt;li&gt;Send client state to every node on commit (every record’s quorom)
        &lt;ul&gt;
          &lt;li&gt;write set, xact id&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;questions&quot;&gt;Questions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;How does a per-record protocol affect/enable fine-grained tuple placement?&lt;/li&gt;
  &lt;li&gt;What to do to make repeatable reads work?&lt;/li&gt;
  &lt;li&gt;How does Fast Paxos work?&lt;/li&gt;
  &lt;li&gt;How powerful/efficient is the demarcation protocol?&lt;/li&gt;
  &lt;li&gt;How do you run range-queries or queries that need indexes?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-mdcc-is-built-up&quot;&gt;How MDCC is built up.&lt;/h2&gt;

&lt;p&gt;The MDCC paper flows by identifying issues with the best solution
proposed so far, and introducing a new technique to resolve the
issues.  This post will flow in a similar manner.&lt;/p&gt;

&lt;h3 id=&quot;per-record-paxos&quot;&gt;Per-record Paxos&lt;/h3&gt;

&lt;p&gt;Megastore is slow because it runs Paxos at a per shard granularity,
causing false conflicts.  What if we ran Paxos per record?&lt;/p&gt;

&lt;p&gt;Paxos on a record runs as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;up to 1 roundtrip for client to tell master “I want to commit my transaction”&lt;/li&gt;
  &lt;li&gt;1 roundtrip to elect master for record&lt;/li&gt;
  &lt;li&gt;1 roundtrip to set the value (learn the value)&lt;/li&gt;
  &lt;li&gt;1 roundtrip to make value visible (can be async)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multi-record-transactions&quot;&gt;Multi-record Transactions&lt;/h3&gt;

&lt;p&gt;The key idea is to add logic into the acceptor nodes to perform
write-write detection.&lt;/p&gt;

&lt;p&gt;If I want to commit records A and B, I run Paxos for each record.&lt;br /&gt;
When running Paxos, instead of setting the values in the second round, 
send an &lt;em&gt;option&lt;/em&gt; containing the record’s read and write versions. 
If there’s no write conflict, the node returns OK, otherwise NO&lt;/p&gt;

&lt;p&gt;If I hear OK for every record, I tell each record’s quorums to write
“commit” into their option log.  Otherwise I write “abort”.  &lt;strong&gt;This
is different than SQL transaction aborts!&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Also, quorum nodes are only allowed to accept options if all previous
options have been decided (written “commit” or “abort”).  Otherwise
write-write conflicts can’t be correctly detected.&lt;/p&gt;

&lt;h3 id=&quot;supporting-failures&quot;&gt;Supporting Failures&lt;/h3&gt;

&lt;p&gt;The following sequence of actions could block all nodes participating
in a transaction.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I send options to acceptors for records A and B.&lt;/li&gt;
  &lt;li&gt;They accept and respond to me.  The options are undecided, so they can’t accept
anything else until I write a “commit” or “abort” in the options.&lt;/li&gt;
  &lt;li&gt;I go on vacation before writing anything.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I was the only one that knew what other records were updated in my
transaction!  So the participating nodes can’t make anymore progress
until I’m back from vacation.&lt;/p&gt;

&lt;p&gt;The solution is to send my transaction state (transaction id and
write set) along with the options.  That way if I disappear, each
record’s quorum can make a clone of me using the transaction state.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;At this point, multi-record transactions using per-record Paxos
“works”.  Everything after this point is to make Paxos consensus run
faster&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;multi-paxos&quot;&gt;Multi-Paxos&lt;/h3&gt;

&lt;p&gt;We still require 3 roundtrips to commit (send to master, elect master,
pick option).  We can use Paxos to elect a master for a block of
rounds which amortizes the cost of electing a master to 0 (assuming
the masters are stable).&lt;/p&gt;

&lt;p&gt;The communication is now&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;up to 1 roundtrip for client to tell master “I want to commit my transaction”&lt;/li&gt;
  &lt;li&gt;0 roundtrips (amortized) to elect master for record&lt;/li&gt;
  &lt;li&gt;1 roundtrip to send option to acceptors&lt;/li&gt;
  &lt;li&gt;1 asynchronous roundtrip to write “commit”/”abort” on options&lt;/li&gt;
  &lt;li&gt;1 asynchronous roundtrip (can be bundled with other messages) to
make commit visible&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;fast-paxos&quot;&gt;Fast-Paxos&lt;/h3&gt;

&lt;p&gt;We still need two synchronous roundtrips – one to contact the master
if it is in another datacenter, and one to send the options.  Can we
avoid the former message?&lt;/p&gt;

&lt;p&gt;Yes! Fast Paxos is lets clients bypass the master to directly send
options to the quorum nodes.  Recall that the master’s job is to
serialize the log entries.  If we want to de away with the master,
Fast Paxos needs a larger quorum size (a fast quorum).  If a fast
quorum responds OK, then the client can consider the option committed.
Otherwise it falls back to normal Paxos with some details.&lt;/p&gt;

&lt;p&gt;None of us had read the 
&lt;a href=&quot;http://research.microsoft.com/pubs/64624/tr-2005-112.pdf&quot;&gt;fast-Paxos paper&lt;/a&gt; 
so we were ill-equipped
to work through why this section of the paper works. Read below for
how we &lt;em&gt;think&lt;/em&gt; Fast Paxos works based on first principles.  Read
the fast-Paxos paper to actually understand how it works.&lt;/p&gt;

&lt;h3 id=&quot;generalized-paxos&quot;&gt;Generalized-Paxos&lt;/h3&gt;

&lt;p&gt;If my operations are all commutative, I want to avoid the cost of
serialization.&lt;/p&gt;

&lt;p&gt;Normal Paxos only allows 1 value update per round.  Generalized Paxos
allows M as long as those values are commutative.  For example, if in
the same round David wants to increment record A by 1 and Jane wants
to increment record A by 1, then the value of A is 2 regardless of the
log order on any of the quorum nodes.&lt;/p&gt;

&lt;p&gt;What this means is the acceptors aren’t simply writing values
anymore, they also understand “delta operations”, how they commute,
and how to apply them.&lt;/p&gt;

&lt;h3 id=&quot;integrity-constrains&quot;&gt;Integrity constrains&lt;/h3&gt;

&lt;p&gt;If I want to support integrity constraints (e.g., A &amp;gt;= 0) the
acceptors need to understand and enforce them.&lt;/p&gt;

&lt;p&gt;The paper restricts the domains of the delta operations (e.g., can
only add/subtract 1 or 2) and conservatively computes the maximum
number of operations that are allowed before it is at all possible
(on a single node and any combinations of transactions in a quorum)
that the constraint is violated.&lt;/p&gt;

&lt;p&gt;This is useful because many transactions are simply updating counters.&lt;/p&gt;

&lt;h3 id=&quot;thats-a-lot-of-steps-are-there-any-more-tricks&quot;&gt;That’s a lot of steps, are there any more tricks?&lt;/h3&gt;

&lt;p&gt;We usually want reads to go fast, but if I’m reading record A and
the A’s replica in my datacenter was not in the quorum, then it
would give me stale data.  One way to get around this is to make
sure the replica in my data center is &lt;em&gt;always&lt;/em&gt; in the quorum.  This
works if my data center is the only one reading this record.  For
example, if each data center is collecting the local weather data
and replicating the data, it can be pretty sure that only people
living around the data center will want the local data.&lt;/p&gt;

&lt;p&gt;Fast-Paxos is really really bad (in terms of cross datacenter
roundtrips) if a collision happens.  For those cases it makes more
sense to stick with a master and use Multi-Paxos.  Figuring out
when to use each is important.  The paper provides a heuristic that
sticks with multi-Paxos and periodically tries to “upgrade” to
fast-Paxos.&lt;/p&gt;

&lt;p&gt;That’s it!  MDCC collects a number of Paxos variants and puts them
together into a nice cross data-center commit protocol.&lt;/p&gt;

&lt;h2 id=&quot;a-note-on-fast-paxos&quot;&gt;A Note on Fast Paxos&lt;/h2&gt;

&lt;p&gt;The following was derived from first principles.&lt;/p&gt;

&lt;p&gt;The fast-Paxos protocol was described (briefly) in the paper as&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Any app-server can propose an option directly to the storage
(quorum) nodes, which in turn
promise only to accept the first proposed option.  Simple majority
quorums, however, are
no longer sufficient to leran a value and ensure safeness…
If a proposer receives an acknowledgment from a fast quorum, the
value is safe and
guaranteed to be committed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A classical Paxos quorum requires the property that for any two
quorums, &lt;code&gt;Qc1&lt;/code&gt; and &lt;code&gt;Qc2&lt;/code&gt;,&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Qc1 INTERSECT Qc2&lt;/code&gt; != Empty set&lt;/p&gt;

&lt;p&gt;For Fast Paxos it has the requirement that for a fast quorums,
&lt;code&gt;Qf1&lt;/code&gt; and &lt;code&gt;Qf2&lt;/code&gt; and regular quorum &lt;qc&gt;,&lt;/qc&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Qf1 INTERSECT Qf2 INTERSECT Qc&lt;/code&gt; != empty set&lt;/p&gt;

&lt;p&gt;One of our questions was why a simple majority, &lt;code&gt;Qc&lt;/code&gt;, is
insufficient and we need a &lt;code&gt;Qf &amp;gt; Qc&lt;/code&gt;. The following is an
example of when it fails.&lt;/p&gt;

&lt;p&gt;Safety, or correctness, in Paxos means that “At most one value can be
learned”.  One example is that even with &lt;em&gt;f&lt;/em&gt; failures, a committed log
position stays committed.&lt;/p&gt;

&lt;p&gt;Lets say &lt;code&gt;Qf = Qc = majority&lt;/code&gt;.  Consider the following logical
proposals and their quorums where each column is the state of an
acceptor (1-5), and each row is the state at one Paxos round.  C1, C2,
C3 are clients (proposers).  C1 commits A and manages to communicate
with acceptors 1-3, C2 commits B and so on.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    1 2 3 4 5
C1  A A A
C2      B B B
C3  C C   C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Say C1 and C2 happen concurrently, and C3 happens later (as a normal
Paxos round, or Fast Paxos).  Then the actual acceptances and what
round each node thinks its in may be&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1 2 3 4 5 Round (as understood by local node)
 A A A B B 1
     B     2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now if C3 tries to set C, its quorum could be 1,2,4 which don’t
know that A won in round 1 and B “won” in round 2, because they
have only seen values for round 1, so the nodes happily accept C
in (what they think is) round 2.  This effectively overwrites B,
violating safety.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; 1 2 3 4 5 Round (as understood by local node)
 A A A B B 1
 C C B C   2
&lt;/code&gt;&lt;/pre&gt;

</description>
        <pubDate>Thu, 25 Jul 2013 12:00:00 -0400</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2013/07/25/mdcc.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2013/07/25/mdcc.html</guid>
        
        
      </item>
    
      <item>
        <title>CoralCDN</title>
        <description>&lt;h2 id=&quot;why-did-we-read-this-paper&quot;&gt;Why did we read this paper?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.coralcdn.org/docs/coral-nsdi04.pdf&quot;&gt;CoralCDN&lt;/a&gt;
is a system that allows small websites or those with limited resources a method
for remaining available in the face of flash-crowds. The system is interesting
for a number of reasons: it is a live running system that the public can use,
it’s peer to peer, self organizing, and also has a &lt;a href=&quot;http://www.coralcdn.org/docs/coral-nsdi10.pdf&quot;&gt;follow-up paper&lt;/a&gt;
that analyzes the system with five years of hindsight.&lt;/p&gt;

&lt;h2 id=&quot;what-is-coralcdn&quot;&gt;What is CoralCDN&lt;/h2&gt;

&lt;p&gt;CoralCDN is a content distribution network (CDN) built on top of the Coral
indexing service.  The system can be described in its component parts: Coral
DNS services, Coral HTTP proxy, and the Coral indexing service.&lt;/p&gt;

&lt;h2 id=&quot;dns-functions&quot;&gt;DNS Functions&lt;/h2&gt;

&lt;p&gt;Many of CoralCDN’s processes attempt to preserve locality in order to keep
latency low. This starts with the DNS lookup by the end user’s browser.
CoralCDN assumes that the resolver for the user’s browser is close to the
end user on the network. It then picks a Coral HTTP proxy close to the source
address of the DNS request.&lt;/p&gt;

&lt;p&gt;Specifically, CoralCDN keeps track of the latencies between the proxies and the
user and creates levels.  The Coral DNS server can get a round trip time to the
end user and only provide listings to proxies that fall within a given level.&lt;/p&gt;

&lt;p&gt;The DNS servers are also responsible for ensuring that the HTTP proxies that
are returned to the end user are available, since a bad HTTP proxy will fail
the user’s request. The DNS servers can do this by synchronously checking a
proxy’s status with an RPC prior to releasing the DNS response.&lt;/p&gt;

&lt;h2 id=&quot;the-http-proxy&quot;&gt;The HTTP Proxy&lt;/h2&gt;

&lt;p&gt;The driving goal of the system is to shield origin servers from request
stampedes. The Coral proxies therefore attempt to find the content within
CoralCDN before going to the origin server. Additionally, when a proxy begins
retrieval from the origin server, it inserts a short lived record into the
Coral index so any additional servers that want the object will not
simultaneously contact the origin.&lt;/p&gt;

&lt;h2 id=&quot;the-indexing-service&quot;&gt;The Indexing Service&lt;/h2&gt;

&lt;p&gt;One of the authors’ stated contributions of this work is the indexing system
including a &lt;em&gt;distributed sloppy hash table&lt;/em&gt; (DSHT).  Each node is given a 160 bit ID
and holds key/value pairs with keys that are ‘close’ to the node’s ID.
Additionally, each node maintains a routing table to other nodes. On a put or
get, the request can be routed to successively closer nodes to toward the
closest node.&lt;/p&gt;

&lt;p&gt;The service is considered &lt;em&gt;sloppy&lt;/em&gt; because a key/value is not always stored at
the closest node. The put process is completed in two phases. The first is a
collection of hops to get to the closest node and forms a stack from the list
of nodes. This phase ends when the storing node reaches the closest node or
when it reaches one that is &lt;em&gt;full&lt;/em&gt; and &lt;em&gt;loaded&lt;/em&gt;. A full node is one that
already stores &lt;em&gt;l&lt;/em&gt; values for that key. A loaded node is one that is receiving
more than &lt;em&gt;b&lt;/em&gt; requests per minute.  In the second phase, the storing node
attempts to place the value at the node on top of the stack. If that store
fails for some reason, the storer pops the stack and tries again.&lt;/p&gt;

&lt;p&gt;Nodes are arranged in clusters based on their average pairwise latency. If that 
latency is below a certain threshold, the cluster is said to be part of a
specific level. A node can be part of multiple clusters but uses
the same ID in each. This clustering allows a sequential search for a target
value, beginning with low latency ‘close’ nodes and only proceeding to slower
levels if needed.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;The idea of levels was interesting mostly because the latency thresholds allow
the clusters to be self-organizing. Clearly, locality is important to a CDN.
It does seem however, that simple static organization of nodes into clusters
would be sufficient to allow latency locality.&lt;/p&gt;

&lt;p&gt;Though we didn’t technically read the follow-up paper, there is an interesting
section on lessons learned. In the section on usage, the authors found
that a small number of popular URLs account for a large percentage of the
requests but that these requests take up relatively little room on the cache.
This means that more than 70% of requests are served from local cache, with only
7.1% retrieved from other Coral proxies.&lt;/p&gt;

</description>
        <pubDate>Thu, 18 Jul 2013 15:00:00 -0400</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2013/07/18/coralcdn.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2013/07/18/coralcdn.html</guid>
        
        
      </item>
    
      <item>
        <title>Zookeeper</title>
        <description>&lt;h2 id=&quot;why-did-we-read-this-paper&quot;&gt;Why did we read this paper?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://static.usenix.org/event/usenix10/tech/full_papers/Hunt.pdf&quot;&gt;Zookeeper&lt;/a&gt; 
is a practical system with replicated storage used by Yahoo!. 
We are interested in understanding what replication protocol Zookeeper uses, why they 
needed a &lt;em&gt;new&lt;/em&gt; replication protocol, what applications/services people build upon it,
and what features are required to make such a replicated storage system
practical.&lt;/p&gt;

&lt;p&gt;Also, Zookeeper has pretty good performance for a replication
protocol. It is interesting to see how it works.&lt;/p&gt;

&lt;h2 id=&quot;what-is-zookeeper&quot;&gt;What is Zookeeper&lt;/h2&gt;

&lt;p&gt;The Zookeeper design consists of three components - replicated storage,
relaxed consistent caching at clients, and detection of client failures.&lt;/p&gt;

&lt;h2 id=&quot;replication&quot;&gt;Replication&lt;/h2&gt;

&lt;p&gt;Zookeeper provides a key/value data model, where keys are named like the paths of
a file system and values are arbitrary blobs. They call each key/value pair a
&lt;em&gt;Znode&lt;/em&gt;. Each Znode must be accessed with a full path so that Zookeeper doesn’t
have to implement open/close. Each value has a version number and an internal
sequence number, which they use a lot when building applications/services presented
in the paper. Each Znode can have its own value, and a collection of children.&lt;/p&gt;

&lt;p&gt;Data can be accessed with get/set/create/delete methods. Zookeeper supports
conditional versions of these operations too. For example, a client can say
set the value of “/ds-reading/schedule” to “3pm,thursday” only if the Znode’s
current version is 100. This feature is used widely in the the presented
applications/services.&lt;/p&gt;

&lt;p&gt;Znodes are replicated via what they called Zab, an atomic broadcast protocol.
Zab is their own replication protocol. It seems quite similar to viewstamped 
replication (VR). The only difference I can tell is that Zab is special case
of viewstamped based replication. Zab requires clients to send requests
in order (thus they use TCP), but VR does not. Zab also requires the requests
to be idempotent, so that a new leader can re-propose the most recent request
without detecting duplicated requests.&lt;/p&gt;

&lt;p&gt;Despite Zab’s restriction for replication of only idempotent operations, Zookeeper does
support non-idempotent operations. The trick is that for each potentially
non-idempotent request, the leader converts it to idempotent requests by
executing it locally.&lt;/p&gt;

&lt;h2 id=&quot;relaxed-consisent-caching&quot;&gt;Relaxed consisent caching&lt;/h2&gt;

&lt;p&gt;Another component of Zookeeper is relaxed consistent caching between clients
and Zookeeper. When clients get data from Zookeeper, clients can cache it, and
optionally register at Zookeeper to receive notifications if the accessed Znode
changes.  Zookeeper will send notifications to caching clients ASYNCHRONOUSLY
once the data is changed. The benefit of this is that updates don’t have to wait for
invalidations to complete, thus they don’t suffer from the impact of client failure; 
the downside is that now the client’s cache is not consistent with Zookeeper.&lt;/p&gt;

&lt;p&gt;The notification doesn’t contain the actual update, and each registration is
triggered only once (the server deletes it once the notification is delivered).&lt;/p&gt;

&lt;h2 id=&quot;detection-of-client-failures&quot;&gt;Detection of client failures&lt;/h2&gt;

&lt;p&gt;Zookeeper supports a special Znode type called “ephemeral Znode” to detect the
failure of clients.  If a session terminates, all ephemeral ZNodes created
within that session are deleted. Services/applications can use it to detect
failure. For example, Katta uses it to detect master failure.&lt;/p&gt;

&lt;h2 id=&quot;other-design-choices&quot;&gt;Other design choices&lt;/h2&gt;

&lt;p&gt;Zookeeper allows applications and services to choose their own level of consistency.  Zookeeper
linearizes all writes. Reads are served from a local Zookeeper server, but a
client can linearize reads using the sync() API.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;In all, we feel that ZooKeeper is cool. It provides building blocks that people
can use to construct their own services with different consistency/performance
requirements. It also simplifies the building of other services, as demonstrated
in the paper.&lt;/p&gt;

&lt;p&gt;One thing we didn’t understand is why the paper makes the claim that
ZooKeeper is not intended for general storage.  It seems like that
would work.&lt;/p&gt;

</description>
        <pubDate>Thu, 11 Jul 2013 15:00:00 -0400</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2013/07/11/zookeeper.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2013/07/11/zookeeper.html</guid>
        
        
      </item>
    
      <item>
        <title>Thialfi</title>
        <description>&lt;h2 id=&quot;why-did-we-read-this-paper&quot;&gt;Why did we read this paper?&lt;/h2&gt;

&lt;p&gt;Several of the papers we’ve read recently have focused on
sophisticated, generic fault tolerance abstractions based on complex
protocols.  &lt;a href=&quot;http://www.cs.columbia.edu/~lierranli/coms6998-11Fall2012/papers/thia_sosp2011.pdf&quot;&gt;Thialfi&lt;/a&gt;
 offers a contrast: its approach to fault tolerance
is intentionally simple, while at the same time being resilient to
arbitrary (halting) failure, including entire data centers.  Thialfi’s
approach to fault tolerance permeates the design of its abstraction,
unlike Raft and VR, which provide general-purpose state machine
replication.&lt;/p&gt;

&lt;p&gt;Thialfi is also a real, massively deployed system, but is simple
enough to explain in more depth within the constraints of a conference
paper than most production systems.&lt;/p&gt;

&lt;h2 id=&quot;what-is-thialfi&quot;&gt;What is Thialfi?&lt;/h2&gt;

&lt;p&gt;The paper calls Thialfi a “notification service”, but really it’s an
object update signaling service.  When an application server updates
an object, it notifies Thialfi, and Thialfi notifies end-user clients
on the Internet that have registered for the object.  Critically,
these “notifications” contain no information about how the object
changed—the client has to query the application server to get its
updated state—which means Thialfi is free to combine notifications
and to generate spurious notifications, as long as clients interested
in an object eventually get at least one notification that the object
has changed if the client’s last seen version is not the object’s
current version.&lt;/p&gt;

&lt;p&gt;Under normal operation, Thialfi delivers timely update notifications
to connected clients, without any polling (clients must send periodic
heartbeats to maintain the connection, but these are infrequent,
small, and efficient to process).  When a client goes offline and
later returns, Thialfi remembers its previous object registrations and
sends only notifications for objects that changed while the client was
offline.  Similarly, the client only needs to resend its registrations
if they have changed since it was last online or if it was offline for
over two weeks (after which Thialfi garbage collects its state).&lt;/p&gt;

&lt;h2 id=&quot;designing-for-fault-tolerance&quot;&gt;Designing for fault tolerance&lt;/h2&gt;

&lt;p&gt;There are a few core ideas in Thialfi’s design that help it achieve
fault tolerance.  These are not necessarily unique to Thialfi, but
they work well in concert.&lt;/p&gt;

&lt;p&gt;Thialfi’s abstraction is carefully chosen to enable simple fault
tolerance.  Thialfi is &lt;em&gt;always&lt;/em&gt; allowed to respond to clients with “I
don’t know”; in the worst case, the client will fall back to polling
the application server.  This is a key choice because it means Thialfi
is free to drop any and all state, as long as it never incorrectly
claims to know the version of an object.  In fact, the initial design
presented by the paper (4.1) is entirely in-memory, yet can survive
data center failures.  At first, this may seem like an undesirable
abstraction to build a client application atop, but, in fact, clients
already have to deal with this when they first run.&lt;/p&gt;

&lt;p&gt;Since the only thing Thialfi can tell a client is “object X might have
changed”, it’s free to coalesce, repeat, and generate spurious
notifications.  The only thing it’s not allowed to do is drop a
notification entirely.  Since, faced with arbitrary faults, there’s no
way to know whether or not a notification was dropped, Thialfi
conservatively generates spurious notifications whenever a failure
&lt;em&gt;might&lt;/em&gt; have caused a notification to be dropped.&lt;/p&gt;

&lt;p&gt;Responsibility for hard state is colocated with the nodes that care
about that hard state.  A client is responsible for its object
registrations, because if the client dies, its registrations don’t
matter.  Likewise, an application server is responsible for
application data, because it has to persist that anyway (and if it
dies, there’s nothing to send notifications about).&lt;/p&gt;

&lt;p&gt;Regular paths and error handling paths are the same wherever possible.
This means they don’t have to distinguish errors from regular
operation and that the code is more likely to be correct (error
handling code is notoriously buggy largely because it doesn’t get
exercised).  For example, initial registration, modifications to
registrations, handling server loss of registration state, and
re-registering after migration are all handled in the same way: the
client and server exchange digests of what they think the registration
state is in every heartbeat; if these are out of sync, the client
simply resends its entire set of registrations.  This extends to the
user of the client API as well: as mentioned above, client cold-start
and the loss of version state in Thialfi are handled identically at an
API level.&lt;/p&gt;

&lt;p&gt;Ultimately, Thialfi makes recovering from failure the responsibility
of application servers and clients, keeping itself off the critical
path for anything.  This seems simple, but achieving this without
burdening application servers and clients requires careful and
conscious design.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;We felt there was one dark corner of Thialfi’s design that makes it
difficult to completely understand its fault tolerance properties.
Application servers post version changes to Thialfi via Google’s
reliable pub-sub system, about which the paper is devoid of details.
It’s difficult to tell how the pub-sub system could fail and how this
would affect Thialfi.  Furthermore, if Thialfi bootstraps off a
reliable pub-sub system, what would have happened if Google had simply
exposed a client API to subscribe to the pub-sub system?  Our best
guess is that it wouldn’t have scaled to millions of clients like
Thialfi does, but we can only guess.&lt;/p&gt;

&lt;p&gt;Thialfi’s approach of signaling rather than notification reprises the
long debate between “level-triggered” and “edge-triggered” interfaces
in OS and hardware design.  Level-triggered interfaces like &lt;code&gt;poll()&lt;/code&gt;
and PCI interrupts have very similar properties to Thialfi’s API
(e.g., like a Thialfi notification, &lt;code&gt;poll()&lt;/code&gt; only tells the caller
that data is available on an FD, not what the data is, or how many
times the FD was written to).  On the other hand, edge-triggered
interfaces more closely resemble the reliable pub-sub interface that
Thialfi explicitly rejected.  Historically, level-triggered interfaces
have generally scaled better, and we found it interesting to see this
revisited and reinforced from a very different perspective.&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Jul 2013 16:00:00 -0400</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2013/07/03/thialfi.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2013/07/03/thialfi.html</guid>
        
        
      </item>
    
      <item>
        <title>VR Revisited</title>
        <description>&lt;h2 id=&quot;why-are-we-reading-viewstamped-replication-revisited&quot;&gt;Why are we reading Viewstamped Replication Revisited?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://pmg.csail.mit.edu/papers/vr-revisited.pdf&quot;&gt;Viewstamped Replication&lt;/a&gt;
 is a mechanism for providing replication through a
Primary / Backup scheme. This paper provides a distilled view of this
technique along with several optimizations that can be applied. In particular,
this paper focuses solely on the Viewstamped Replication protocol
without looking at any specific implementation or uses.&lt;/p&gt;

&lt;p&gt;While a general Primary / Backup replication scheme may seem easy to
get right, considering how to handle view changes and the
many optimizations that others have come up with over the years, this
paper provides a go-to source for building such a system.&lt;/p&gt;

&lt;p&gt;Previously we have looked at the Paxos protocol for 
consensus as well as Spanner which is an externally consistent distributed storage system.
This paper sits in-between these two extremes in that it is a technique
used for replication, thus being more complete than one-time consensus, while
eliding the details of a full storage system like Spanner.&lt;/p&gt;

&lt;h2 id=&quot;what-is-viewstamped-replication&quot;&gt;What is Viewstamped Replication?&lt;/h2&gt;

&lt;p&gt;Viewstamped Replication (referred to as VR in the remainder of this post) is a
replication protocol that uses consensus to support a replicated state machine.
The replication state machine allows clients using this service to run operations
that either view or modify the state, upon which other services can be built
such as a distributed key-value store.&lt;/p&gt;

&lt;p&gt;One goal of VR is to support &lt;em&gt;f&lt;/em&gt; failures using 2&lt;em&gt;f&lt;/em&gt; + 1 nodes, so it
should be used in a distributed system where failures may
occur. Beyond normal operation, the system handles two scenarios:
changing the primary between the current list of members in a &lt;em&gt;view
change&lt;/em&gt; as well as changing the set of participating members in a
&lt;em&gt;reconfiguration&lt;/em&gt;.  This paper assumes that state replicated on many
machiens can be used for durability, thus avoiding a potential latency
of writing to persistent storage.  Unfortunately if VR is run in one
datacenter all the machines may be on the same power source and thus
in the same failure domain, so this might not be practical without a UPS.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;Some number of clients will be interacting directly with a service such as a
key-value store. The clients use a VR library or proxy that will abstract away
the details of the replication so that client code will use the abstraction of
“read” and “write” like operations on client defined state. Clients will use a
monotonically increasing request number which will allow the system to detect
duplicate requests.&lt;/p&gt;

&lt;p&gt;Some number of servers 2&lt;em&gt;f&lt;/em&gt; + 1 when trying to support
&lt;em&gt;f&lt;/em&gt; failures) will run the VR code as well as the service code. The
VR code on the server will determine when to apply operations on the service
data and push these operations up to the service code. Note that these servers
can return after a failure (or network partition) and will only result in a
view change which would require getting the failed / partitioned node(s) up to
date.&lt;/p&gt;

&lt;p&gt;Note that in VR as presented in this paper, the operations are performed on
several different replicas (instead of shipping the data around after the
operation has been performed). As a result, the operations  must be
deterministic.  It is mentioned in the
paper that particular techniques can be used to ensure this property.&lt;/p&gt;

&lt;p&gt;As this is a Primary / Backup based system, the ordering is decided by the
primary, however &lt;em&gt;f&lt;/em&gt; + 1 replicas must know about a request before
executing it to ensure durability despite failures and that ordering is guaranteed.&lt;/p&gt;

&lt;h2 id=&quot;system-operation&quot;&gt;System Operation&lt;/h2&gt;

&lt;p&gt;The system is in one of three states. These states are normal
operation, view changes when the system needs a new primary, and reconfiguration
when membership is changing. The primary node is deterministically chosen based
on the configuration (list of servers) and the view numbers. As a result
this system does not need to rely on voting or consensus (e.g. longest
log) to determine the next leader to take over.&lt;/p&gt;

&lt;h3 id=&quot;normal-operation&quot;&gt;Normal Operation&lt;/h3&gt;

&lt;p&gt;Replicas use a view number to determine if they are in the correct state. If
the sender is behind the receiver will drop the message, on the other hand if a
sender is ahead the receiver must update itself first and then process the
message.&lt;/p&gt;

&lt;p&gt;A client sends a request to perform an operation at the server. The server then
sends &lt;code&gt;Prepare&lt;/code&gt; messages to each of the backups and waits for 
&lt;em&gt;f&lt;/em&gt; &lt;code&gt;PrepareOk&lt;/code&gt; responses. Once it has received these responses it can
assume that the message will persist and it applies the operation by making an
upcall to the service code and finally replies to the client. A backup will
perform the same operation but does not reply to the client.&lt;/p&gt;

&lt;h3 id=&quot;view-changes&quot;&gt;View Changes&lt;/h3&gt;

&lt;p&gt;View changes occcur when the system needs to elect a new leader. A key correctness
requirement for the protocol is that every operation executed by an up-call to
the service code must make it into the new view in the same order as the original
execution. To achieve this requirement, &lt;em&gt;f&lt;/em&gt; + 1 logs are obtained and merged
using the view number to break conflicts in op number ordering.&lt;/p&gt;

&lt;p&gt;Protocol:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Replica sends &lt;code&gt;StartViewChange&lt;/code&gt; to all other replicas&lt;/li&gt;
  &lt;li&gt;Receives f responses, sends &lt;code&gt;DoViewChange&lt;/code&gt; to new primary&lt;/li&gt;
  &lt;li&gt;New primary waits for &lt;em&gt;f&lt;/em&gt; + 1 &lt;code&gt;DoViewChange&lt;/code&gt; messages before assuming new view&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that sending a suffix of the log (e.g. 1-2 entries) in the
&lt;code&gt;DoViewChange&lt;/code&gt; message will likely bring the new server up to date
without requiring any additional state transfer from the replicas.&lt;/p&gt;

&lt;h3 id=&quot;recovery&quot;&gt;Recovery&lt;/h3&gt;

&lt;p&gt;Server recovery has the correctness requirement that a replicamust be as up to
date as it was when it crashed, otherwise it may forget about ops that it
prepared. This is achieved by receiving state from other replicas using the
following recovery protocol (note that nodes do not participate in request
processing or view changes during the recovery phase):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Recovering node send “Recovery” message to all&lt;/li&gt;
  &lt;li&gt;All reply with “RecoveryResponse”, view number and nonce (and log, etc if primary)&lt;/li&gt;
  &lt;li&gt;Replica waits for &lt;em&gt;f&lt;/em&gt; + 1 responses (and primary), applies log and begins normal processing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that in the theoretical solution, logs may be prohibitively big, however
optimizations exist to trim the log (e.g. snapshots).&lt;/p&gt;

&lt;p&gt;Client recovery is simply achieved by starting any new request with the old
request number (obtained from replicas) + 2.&lt;/p&gt;

&lt;h3 id=&quot;reconfiguration&quot;&gt;Reconfiguration&lt;/h3&gt;

&lt;p&gt;Though reconfiguration is discussed later in the paper, it fits the flow here
in that it is essentially the last mode of operation. Beyond that several
optimizations are considered to speed up various parts of the system.&lt;/p&gt;

&lt;p&gt;Reconfiguration is used to add/remove nodes to the system (thus changing the
&lt;em&gt;f&lt;/em&gt; failures that the system can handle) or to upgrade or relocate
machines (for long running systems). A reconfiguration is instantiated by an
administrator of the system. In this paper, the term “epoch” refers to a configuration
number and the “transitioning” state refers to a node that is currently changing
configurations. The reconfiguration is started similarly to other operations by
sending the operation to the leader, however included in this operation is the
new configuration (list of participating machines). The primary will then 
send the &lt;code&gt;StartEpoch&lt;/code&gt; message and wait for &lt;em&gt;f&lt;/em&gt; responses.&lt;/p&gt;

&lt;p&gt;Any new replicas will be brough up to date before the epoch change (by sending
them a list of operations or a snapshot + diff). Once a new replica is up to date
it will send an &lt;code&gt;EpochStarted&lt;/code&gt; message to old replicas. Thus, once
an old replica (that is not in the new configuration) has received &lt;em&gt;f&lt;/em&gt; + 1
&lt;code&gt;EpochStarted&lt;/code&gt; messages, it is free to shut down. Note that one
particular optimization is to bring new machines up to date (e.g. warm-up)
before performing the reconfiguration to minimize the down time during
transition as nodes will not respond to messages for earlier epochs or while
transitioning.&lt;/p&gt;

&lt;p&gt;The administrator can determine status of old replicas by sending out
&lt;code&gt;CheckEpoch&lt;/code&gt; messages and then know when it is safe to shut down old
machine(s).&lt;/p&gt;

&lt;p&gt;One issue for this system is rendesvous, however the solution provided in the
paper is to simply publish it somewhere out-of-band.&lt;/p&gt;

&lt;h4 id=&quot;efficient-recovery&quot;&gt;Efficient Recovery&lt;/h4&gt;

&lt;p&gt;One concern is achieving efficient recovery of failed server machines. As
presented previously, sending the missing log could result in the transfer of a
substantial amount of data. One way to solve this is to store application state
as a “checkpoint” that represents a log prefix, thus allowing the transfer of a
potentially much more compressed state + some log diff.&lt;/p&gt;

&lt;p&gt;After a server creates a checkpoint, it can mark any modification as “dirty”
and provide those as the diff over the last created checkpoint. A perhaps
generic way of accomplishing this diff is to use merkle trees to efficiently
determine which pages are dirty to avoid sending the entire checkpoint.
Finally, if the state is too large to transfer then the paper suggests an
out-of-band mechanism (e.g. sneakernet).&lt;/p&gt;

&lt;p&gt;Note that checkpoints allow garbage collection of the log, but the log may
still be required to bring back a recovering node, so it is beneficial to keep
some of it, else the system will suffer the cost of state transfer between
servers.&lt;/p&gt;

&lt;h3 id=&quot;state-transfer&quot;&gt;State Transfer&lt;/h3&gt;

&lt;p&gt;State transfer between two replicas can exist in essentially two cases. One
being that there are missing operations in the current view. To solve this the
replica will simply obtain said operations from another replica. The second and
more difficult situation is that there was a view change in which case the
replica will set it’s op-number to the latest commit-number and obtain updates
from another replica. If there is a gap in this replica’s log, it will need to
fast-forward using application state (such as a checkpoint).&lt;/p&gt;

&lt;h3 id=&quot;other-optimizations&quot;&gt;Other Optimizations&lt;/h3&gt;

&lt;p&gt;There are a handful of optimizations that others have come up with since the
original VR procool was presented. One example is using Witnesses that aren’t
performing the operations. This is a simple extension to the system by having
&lt;em&gt;f&lt;/em&gt; replicas act as log keepers that are only used for view changes and
recovery.&lt;/p&gt;

&lt;p&gt;Another optimization is to batch operations which simply implies that if the
system is busy then piggy-back several operations in a single message.&lt;/p&gt;

&lt;p&gt;Finally, &lt;em&gt;fast reads&lt;/em&gt; is a technique used in several replication systems.
This essentially allows the primary to respond to a read request without going
through the full protocol. These can be performed at the Primary, though the
use of leases and loosely synchronized clocks are required to maintain
consistency. Additionally, if the user of the system is ok with stale data,
then reads can be served at backups where a backup will reply to a client if it
has seens commits up to that request.&lt;/p&gt;
</description>
        <pubDate>Thu, 20 Jun 2013 18:00:00 -0400</pubDate>
        <link>http://mit-pdos.github.io/dsrg/2013/06/20/vr-revisited.html</link>
        <guid isPermaLink="true">http://mit-pdos.github.io/dsrg/2013/06/20/vr-revisited.html</guid>
        
        
      </item>
    
  </channel>
</rss>
